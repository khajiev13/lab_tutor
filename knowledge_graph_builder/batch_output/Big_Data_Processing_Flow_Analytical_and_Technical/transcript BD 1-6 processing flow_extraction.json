{
  "topic": "Big Data Processing Flow: Analytical and Technical Perspectives",
  "summary": "This session discusses the big data processing flow from both analytical and technical perspectives, emphasizing the goal of extracting value from data. The analytical perspective involves finding suitable data sources (structured, semi-structured, or unstructured), collecting data using ETL or data crawlers, transforming data for storage and processing, cleaning and performing feature engineering, applying statistical models or machine learning algorithms, visualizing results, and generating reports. The technical perspective focuses on integrating data assets like transactions (OLTP/OLAP), documents, social media content, and IoT data into a cluster or big data storage. This includes offloading and ETL processing transaction data to Hadoop, using document data and social media data for analysis, and collecting IoT data. Data scientists discover, profile, govern, and enrich the data, masking sensitive information. They may build an Enterprise Data Warehouse for business intelligence reporting. An example of JD's big data analysis is provided, illustrating the integration of various data sources into a Hadoop cluster to generate reports while protecting customer privacy. The session concludes by summarizing the general steps of big data analysis and the use of data processing, algorithms, and models to generate insights and support decision-making.",
  "keywords": [
    "big data processing",
    "analytical perspective",
    "technical perspective",
    "ETL",
    "Hadoop",
    "data cleaning",
    "feature engineering",
    "data visualization",
    "business intelligence",
    "Enterprise Data Warehouse"
  ],
  "concepts": [
    {
      "name": "ETL",
      "definition": "Extract Transform load, used to extract data from different database or data warehouse or use data crawlers to crawl the data.",
      "text_evidence": "which could use the ETL(Extract Transform load) to extract data from different database or data warehouse or use data crawlers to crawl the data."
    },
    {
      "name": "OLTP",
      "definition": "Online transaction processing.",
      "text_evidence": "At the most left side, are the data assets, including transactions, OLTP, online transaction processing and OLAP online analyzing processing"
    },
    {
      "name": "OLAP",
      "definition": "Online analyzing processing.",
      "text_evidence": "At the most left side, are the data assets, including transactions, OLTP, online transaction processing and OLAP online analyzing processing"
    },
    {
      "name": "Hadoop",
      "definition": "A big data storage to support the further analysis.",
      "text_evidence": "transaction data in database or data warehouse need to be offload & ETL processed to Hadoop, which is mostly the first step of journey."
    },
    {
      "name": "Enterprise Data Warehouse",
      "definition": "Store the relevant data for the business intelligence.",
      "text_evidence": "Sometimes, in order to do the effective analysis, data scientist would like to build an Enterprise Data Warehouse and move the curated data to the Enterprise Data warehouse, which just store the relevant data for the business intelligence."
    }
  ],
  "original_text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering，\nSchool of Computer Science, Beijing Institute of Technology,\nin this session, we will discuss about Big Data Processing flow both from the analytical perspective and technical perspective.\nAll the big data analysis and processing is about to get value from data.\nThat is the ultimate goal of big data analysis. In order to achieve it, we need to do several steps, this diagram shows the bigdata process flow in analytical perspective.\n1According to the analysis requirement to find the suitable data sources, which could be structured data, semi structured data or unstructured data.\n2 Then collect data from the selected data sources, which could use the ETL(Extract Transform load) to extract data from different database or data warehouse or use data crawlers to crawl the data.\n3 After get data from different data sources, sometime the data format is different, it need transform to facilitate the further data store or processing.\nwe need plan the suitable way to store the data, due to the nature of big data, data volume is big, which may need distributed data storage solution.\n4 Then the data need to be processed to support analysis. In data Processing,\ndata scientist maybe first clean the data, delete the wrong data, redundant data, and other dirty data.\nAfter that, data scientist need to do the feature engineering, to extract or build the representative key features which can enable the further model or algorithm.\nThen data scientist could apply statistic, mathematic models or machine learning algorithms to find the pattern, correlation, classification and so on.\n5 the pattern, correlations, classification and so on will be visualized, which can make the results easy to understand. For example, the pandemic data of the different provinces could explain the distribution better in the form of map than table.\n6 also some reports could be generated to do the further analysis. Or the results could be used to monitor the business.\nThese are the Big data process flow in analytical perspective.\nNow let’ s look at Big data process flow-technical perspective.\nAt the most left side, are the data assets, including transactions, OLTP, online transaction processing and OLAP online analyzing processing,\ndifferent kinds of documents, social media content and the IoT data generated by machine devices.\nAll these data need to be integrated into cluster or big data storage to support the further analysis.\n1 transaction data in database or data warehouse need to be offload & ETL processed to Hadoop, which is mostly the first step of journey.\nThe transaction data is mostly structured data.\n2 Some document data can also be used for data analysis, these data also need to be offloaded to Hadoop cluster for the further processing and analysis.\n3 Social media data can help to understand customer’s opinion and preference, which enable better customized service .\n4 With the help of the IoT device , we can collect IoT data from different kind of sensors, video cameras and so on , those data can help us\nlearn the status of the devices and the its content.\nAll these raw data can be integrated into cluster, and the metadata changed also need to update to cluster,\nbesides the real-time data need to be collected timely to support the further real-time analysis.\nAfter the data collected in cluster, the data scientist will try to discover and profile the data according to the analysis purpose,\nand use the defined meta data to govern and enrich the data set. Then parse and prepare data for next analysis.\nWhen data scientist analysis the data, they should mask the sensitive data like name , id etc. to protect the privacy.\nBased on the data in the cluster, data scientist can generate the business intelligence report or other visualization graphs.\nSometimes, in order to do the effective analysis, data scientist would like to build an Enterprise Data Warehouse and move the curated data to the\nEnterprise Data warehouse, which just store the relevant data for the business intelligence. And on the basis of Enterprise Data Warehouse,\nanalytics can generate the business intelligence report to support decision making.\nLet’s make an example, JD big data analysis.\nJD has lots of transactions data from the online shopping history.\nAnd they also have lots of document data, like customer email, industry reports, user agreements and so on.\nAnd the customer’ s review about their feeling after shopping.\nAnd they also have lots of IoT data, like the logistic trucks GPS, product package ‘s RFID etc.\nWith all these data integrated into the Hadoop cluster, JD data scientist can discover and profile about the customer, product, express and\nwith all the predefined model, metadata, they can parse the original raw data, and generate the JD sales trend BI report, express efficiency BI report, product category BI report and etc.\nBut of cause, customer sensitive data should be masked to protect customer privacy.\nIn this session, we discussed the Big Data Processing flow both from the analytical perspective and technical perspective. We understand the general steps of big data analysis, including selecting the data sources, data collecting, cleaning the data before storing the data.\nBased on the data stored in the big data distributed storage, data scientist can process the data for further analysis, after all the data has been processed, data scientist can use algorithm or designed model to analyze the data, generate the business intelligence report or do the visualization analysis to dig the insight of the data and support decision making.\nThank you for your attention, if you have any question, feel free to connect me."
}