{
  "topic": "Data Processing System Architecture",
  "summary": "This session introduces data processing systems and their architecture. A data processing system provides big data computing and an application development platform. From a computing architecture perspective, it includes the data algorithm layer, computing model layer, computing platform layer, and computing engine layer. Big data computing algorithms include machine learning and data mining. Computing models encompass batch processing, stream computing, massively parallel processing (MPP), in-memory computing, and data flow graph models. Typical big data processing platforms are Hadoop, Spark, and Storm. Data mining algorithms include classification, clustering, correlation analysis, and anomaly detection. Machine learning algorithms include supervised, unsupervised, semi-supervised, and reinforcement learning. Basic big data processing starts with batch processing, represented by MapReduce. Stream processing analyzes data almost instantaneously. Massively parallel processing (MPP) handles coordinated processing by multiple processors. In-memory computation runs calculations entirely in computer memory. Massively Parallel Graph Computation is used for processing graphs containing hundreds of billions of edges. Computing platforms provide the tools and libraries to implement complex algorithms and computing models.",
  "keywords": [
    "data processing system",
    "big data",
    "computing architecture",
    "batch processing",
    "stream computing",
    "MPP",
    "in-memory computing",
    "Hadoop",
    "Spark",
    "data mining"
  ],
  "concepts": [
    {
      "name": "Data processing system",
      "definition": "The data processing system provides big data computing and processing capabilities and an application development platform.",
      "text_evidence": "The data processing system provides big data computing and processing capabilities and an application development platform."
    },
    {
      "name": "Batch processing",
      "definition": "Batch processing is the processing of a large volume of data all at once.",
      "text_evidence": "Batch processing is the processing of a large volume of data all at once."
    },
    {
      "name": "Stream processing",
      "definition": "Stream processing is the process of being able to almost instantaneously analyze data that is streaming from one device to another.",
      "text_evidence": "Stream processing is the process of being able to almost instantaneously analyze data that is streaming from one device to another."
    },
    {
      "name": "Massively parallel processing (MPP)",
      "definition": "Massively parallel processing (MPP) is a storage structure designed to handle the coordinated processing of program operations by multiple processors.",
      "text_evidence": "Massively parallel processing (MPP) is a storage structure designed to handle the coordinated processing of program operations by multiple processors."
    },
    {
      "name": "In-memory computation",
      "definition": "In-memory computation (or in-memory computing) is the technique of running computer calculations entirely in computer memory (e.g., in RAM).",
      "text_evidence": "In-memory computation (or in-memory computing) is the technique of running computer calculations entirely in computer memory (e.g., in RAM)."
    },
    {
      "name": "Computing platform and engine",
      "definition": "Computing platform and engine refer to a development integrated environment that provides Technical Standards, Computing Architecture, and a series of Development Technologies And Tools for big data computing and analysis.",
      "text_evidence": "Computing platform and engine refer to a development integrated environment that provides Technical Standards, Computing Architecture, and a series of Development Technologies And Tools for big data computing and analysis."
    },
    {
      "name": "Data mining algorithms",
      "definition": "Data mining algorithms can be categorized into 4 groups, classification, clustering, correlation Analysis and anomaly detection.",
      "text_evidence": "Data mining algorithms can be categorized into 4 groups, classification, clustering, correlation Analysis and anomaly detection."
    },
    {
      "name": "Machine learning algorithms",
      "definition": "Machine learning algorithms include supervised learning, unsupervised learning semi-supervised learning and reinforcement learning.",
      "text_evidence": "Machine learning algorithms include supervised learning, unsupervised learning semi-supervised learning and reinforcement learning."
    }
  ],
  "original_text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, from this session,\nwe start to learn Data processing system, and in this session, we discuss Data processing Architecture.\nThe data processing system provides big data computing and processing capabilities and an application development platform.\nFrom the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing platform layer, computing engine layer, etc.\nBig data related computing algorithms include machine learning algorithms and data mining algorithms.\nComputing models are the way that different kinds of big data is processed in different scenarios, which include batch processing, stream computing, Large-scale concurrent processing (MPP) model for structured data, In-memory Computing model, and Data Flow Graph models.\nAs far as the computing platform and engine, normally the typical representative big data processing platforms are Hadoop, Spark , storm, Pregel, etc,\nBig data processing algorithm basically include data mining algorithms and Machine learning algorithms.\nData mining algorithms can be categorized into 4 groups, classification, clustering, correlation Analysis and anomaly detection.\nMachine learning algorithms include supervised learning, unsupervised learning semi-supervised learning and reinforcement learning.\nAnd in the scope of supervised learning there are regression, classification, and deep learning.\nAnd in the scope of unsupervised learning there are clustering, dimensionality reduction ,and also anomaly detection.\nFor semi-supervised learning , it include self-training and low density separation models.\nand reinforcement learning include dynamic programming and Monte Carlo methods.\nBasic big data processing start from Batch processing, Represented by MapReduce.\nBatch processing is the processing of a large volume of data all at once.\nThe data easily consists of millions of records for a day and can be stored in a variety of ways (file, record, etc).\nThe jobs are typically completed simultaneously in non-stop, sequential order.\nA go-to example of a batch processing job is all of the transactions a financial firm might submit over the course of a week. Batching can also be used in:\nPayroll processes\nLine item invoices\nSupply chain and fulfillment\nBatch data processing is an extremely efficient way to process large amounts of data that is collected over a period. It also helps to reduce the operational costs that businesses might spend on labor as it doesn’t require specialized data entry clerks to support its functioning. It can be used offline and gives managers complete control as to when to start the processing, whether it be overnight or at the end of a week or pay period.\nStream processing is the process of being able to almost instantaneously analyze data that is streaming from one device to another.\nThis method of continuous computation happens as data flows through the system with no compulsory time limitations on the output.\nWith the almost instant flow, systems do not require large amounts of data to be stored.\nStream processing is highly beneficial if the events you wish to track are happening frequently and close together in time.\nIt is also best to utilize if the event needs to be detected right away and responded to quickly Stream processing, then, is useful for tasks like fraud detection and cybersecurity.\nIf transaction data is stream-processed, fraudulent transactions can be identified and stopped before they are even complete.\nMassively parallel processing (MPP) is a storage structure designed to handle the coordinated processing of program operations by multiple processors. This coordinated processing can work on different parts of a program, with each processor using its own operating system and memory. This allows MPP databases to handle massive amounts of data and provide much faster analytics based on large datasets.\nMPP processors can have up to 200 or more processors working on application and most commonly communicate using a messaging interface. MPP works by allowing messages to be sent between processes through an “interconnect” arrangement of data paths.\nIn-memory computation (or in-memory computing) is the technique of running computer calculations entirely in computer memory (e.g., in RAM). This term typically implies large-scale, complex calculations which require specialized systems software to run the calculations on computers working together in a cluster. As a cluster, the computers pool together their RAM so the calculation is essentially run across computers and leverages the collective RAM space of all the computers together.\nIn-memory computation works by eliminating all slow data accesses and relying exclusively on data stored in RAM.\nOverall computation performance is greatly improved by removing the latency commonly seen when accessing hard disk drives or SSDs.\nSoftware running on one or more computers manages the computation as well as the data in memory, and in the case of multiple computers, the software divides the computation into smaller tasks which are distributed out to each computer to run in parallel.\nMassively Parallel Graph Computation\nGraphs are useful theoretical representations of the connections between groups of entities, and have been used for a variety of purposes in data science, from ranking web pages by popularity and mapping out social networks, to assisting with navigation.\nIn many cases, such applications require the processing of graphs containing hundreds of billions of edges, which is far too large to be processed on a single consumer-grade machine.\nA typical approach to scaling graph algorithms is to run in a distributed setting, i.e., to partition the data (and the algorithm) among multiple computers to perform the computation in parallel.\nWhile this approach allows one to process graphs with trillions of edges, it also introduces new challenges. Namely, because each computer only sees a small piece of the input graph at a time, one needs to handle inter-machine communication and design algorithms that can be split across multiple computers.\nApplying the Data Algorithm Layer algorithms and combined with the Batch, stream , Massive Parallel Processing, In Memory Computing\nOr Graph Computing model, we can process big data problems, but it is hard for us to implement all the algorithm and computing model by ourselves,\nThe Computing Platform & Computing Engine Layer can provide us the platform and engine which includes the needed tools, libraries to facilitate the implementing the complex Algorithms and Computing models.\nComputing platform and engine refer to a development integrated environment that provides Technical Standards, Computing Architecture, and a series of Development Technologies And Tools for big data computing and analysis.\nThe current representative computing platforms are: Hadoop, Cloudera, Spark, Storm, and Google's commercial platforms based on a series of big data computing technologies.\nIn this session we learned the general architecture of the data processing system,\nthank you for your attention, if you have any question, feel free to contact me."
}