{
  "topic": "Data Reduction Techniques",
  "summary": "This session discusses data reduction techniques, which aim to obtain a condensed dataset from a huge original dataset while maintaining its integrity. Data reduction includes dimensionality reduction and numerosity reduction. Dimensionality reduction involves attribute subset selection (step forward, remove gradually backward, and combination), decision tree induction, and statistical analysis. Data compression, both lossless and lossy, is also discussed. Numerosity reduction includes data cube aggregation, discretization, and concept hierarchical generation. Data cube aggregation gathers n-dimensional data cubes into n-1 dimensional data cubes. Discretization reduces the number of values of a continuous attribute. Concept hierarchy uses higher-level concepts to replace lower-level ones to reduce the number of values.",
  "keywords": [
    "data reduction",
    "dimensionality reduction",
    "numerosity reduction",
    "attribute subset selection",
    "data cube aggregation",
    "discretization",
    "concept hierarchy",
    "lossless compression",
    "lossy compression",
    "principal component analysis"
  ],
  "concepts": [
    {
      "name": "Data reduction",
      "definition": "Technology is used to help obtain a condensed data set from the original huge data set, and make this condensed data set maintain the integrity of the original data set, so that data analysis on the condensed data set is obviously efficient higher, and the results of analysis are basically the same as those obtained by using the original data set.",
      "text_evidence": "Data reduction (subtraction) technology is used to help obtain a condensed data set from the original huge data set, and make this condensed data set maintain the integrity of the original data set, so that data analysis on the condensed data set is obviously efficient higher, and the results of analysis are basically the same as those obtained by using the original data set."
    },
    {
      "name": "Dimensionality reduction",
      "definition": "Remove irrelevant attributes and reduce the amount of data processed by data analysis.",
      "text_evidence": "Remove irrelevant attributes and reduce the amount of data processed by data analysis."
    },
    {
      "name": "Attributes subset selection",
      "definition": "Goal is to Find the smallest attribute subset and ensure that the probability distribution of the new data subset is as close as possible to the probability distribution of the original data set",
      "text_evidence": "Attributes subset selection Goal is to Find the smallest attribute subset and ensure that the probability distribution of the new data subset is as close as possible to the probability distribution of the original data set"
    },
    {
      "name": "Lossless compression",
      "definition": "Compressed data can be restored without losing any information.",
      "text_evidence": "Lossless compression: Compressed data can be restored without losing any information."
    },
    {
      "name": "Lossy compression",
      "definition": "Only an approximate representation of the original data can be reconstructed.",
      "text_evidence": "Lossy compression: Only an approximate representation of the original data can be reconstructed."
    },
    {
      "name": "Principal component analysis (PCA)",
      "definition": "Assumes that the data to be compressed consists of N tuples or data vectors taken from k dimensions. Principal component analysis and search to obtain c “k-dimensional” orthogonal vectors that best represent the data, where c≤k. In this way, the original data can be projected into a smaller space to achieve data compression",
      "text_evidence": "Principal component analysis (PCA) assumes that the data to be compressed consists of N tuples or data vectors taken from k dimensions. Principal component analysis and search to obtain c “k-dimensional” orthogonal vectors that best represent the data, where c≤k. In this way, the original data can be projected into a smaller space to achieve data compression"
    },
    {
      "name": "Numerosity reduction",
      "definition": "Use smaller data to represent data, or use shorter data units, or use data models to represent data to reduce the amount of data.",
      "text_evidence": "Numerosity reduction -use smaller data to represent data, or use shorter data units, or use data models to represent data to reduce the amount of data."
    },
    {
      "name": "Data cube aggregation",
      "definition": "Gather n-dimensional data cubes into n-1 dimensional data cubes.",
      "text_evidence": "Data cube aggregation definition-gather n-dimensional data cubes into n-1 dimensional data cubes."
    },
    {
      "name": "Discretization",
      "definition": "Reduce the number of values of a continuous (value) attribute by dividing the range of the attribute (continuous value) domain value into several intervals.",
      "text_evidence": "Reduce the number of values of a continuous (value) attribute by dividing the range of the attribute (continuous value) domain value into several intervals."
    },
    {
      "name": "Concept hierarchy",
      "definition": "Defines a set of mappings from low-level concept sets to high-level concept sets.",
      "text_evidence": "Concept hierarchy defines a set of mappings from low-level concept sets to high-level concept sets."
    }
  ],
  "original_text": "Bigdata analysis 3-5 data reduction\nHello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology , in this session , we will discuss Data reduction .\nComplex data analysis of large-scale database content usually takes a lot of time.\nData reduction (subtraction) technology is used to help obtain a condensed data set from the original huge data set, and make this condensed data set maintain the integrity of the original data set, so that data analysis on the condensed data set is obviously efficient higher, and the results of analysis are basically the same as those obtained by using the original data set.\nData reduction work should satisfy some standard.\nThe time spent on data reduction should not exceed or \"offset\" the time saved by analysis on the reduced data\nThe data obtained by the reduction is much smaller than the original data, but can produce the same or almost the same analysis results\nData reduction technology includes dimensionality reduction and numerosity reduction.\nAnd dimensionality reduction includes attribute subset selection, principal Component analysis and wavelet transform\nNumerosity reduction could be divided into parametric and non-parametric methods.\nnon-parametric methods include data cube aggregation , clustering, sampling histogram etc.\nFirst let’ s look at Dimension reduction\nRemove irrelevant attributes and reduce the amount of data processed by data analysis.\nAttributes subset selection Goal is to Find the smallest attribute subset and ensure that the probability distribution of the new data subset is as close as possible to the probability distribution of the original data set\nFor example, when digging into the classification rules of whether a customer will buy a player in a shopping mall, the customer's phone number is likely to be irrelevant to the mining task and should be removed.\nStep forward Attributes subset selection\nStarting from an empty attribute set (as the initial value of the attribute subset), each time a current optimal attribute is selected from the original attribute set and added to the current attribute subset. Until the optimal attribute cannot be selected or a certain threshold constraint is met.\nRemove gradually backward Attributes subset selection\nStarting from a full attribute set (as the initial value of the attribute subset), each time a current worst attribute is selected from the current attribute subset and eliminated from the current attribute subset. Until the worst attribute cannot be selected or a certain threshold constraint is met.\nor Combine forward selection and backward deletion\nDecision tree (decision tree) induction\nUse the decision tree induction method to classify and induct the initial data to obtain an initial decision tree.\nAll attributes that do not appear on the decision tree are considered irrelevant attributes.\nTherefore, delete these attributes from the initial attribute set to obtain an initial decision tree. A better subset of attributes.\nThere is also Reduction based on statistical analysis\nNow let’s look at Data reduction by Data compression\nCompression algorithm can be classified into Lossless compression and Lossy compression .\nLossless compression: Compressed data can be restored without losing any information.\nFor example: string compression\nHave a broad theoretical foundation and sophisticated algorithms\nLossy compression: Only an approximate representation of the original data can be reconstructed.\nFor example: audio/video compression\nSometimes it is possible to reconstruct a fragment without decompressing the overall data\nData compression-use data encoding or transformation to obtain a compressed representation of the original data.\nThe two data compression methods commonly used in the field of data mining are both lossy:\nPrincipal component analysis (PCA) assumes that the data to be compressed consists of N tuples or data vectors taken from k dimensions. Principal component analysis and search to obtain c “k-dimensional” orthogonal vectors that best represent the data, where c≤k. In this way, the original data can be projected into a smaller space to achieve data compression\nWe talked some technologies of dimensionality reduction and now let’s look at some technologies of numerosity reduction.\nNumerosity reduction -use smaller data to represent data, or use shorter data units, or use data models to represent data to reduce the amount of data.\nNumerosity reduction includes Data cube aggregation , clustering, sampling, histogram etc.\nLet’s look at the data cube aggregation\nA data cube is a multi-dimensional modeling and representation of data, composed of dimensions and facts.\nDimension: attribute\nFacts: data\nData cube aggregation definition-gather n-dimensional data cubes into n-1 dimensional data cubes.\nUsing Data cube Aggregates for approximate query\nIn the diagram, 3 dimension data cube aggregate into 2 dimension, the men and women ’ sale amount aggregated together.\nNow let’s look at Data reduction method Discretization.\nAttribute values could be Name type-e.g. value in an unordered set, Ordinal-e.g. value in an ordered set and Continuous value-e.g. real number.\nDiscretization technology\nReduce the number of values of a continuous (value) attribute by dividing the range of the attribute (continuous value) domain value into several intervals.\nUsing Concept hierarchical generation to do the Data reduction\nConcept hierarchy defines a set of mappings from low-level concept sets to high-level concept sets.\nIt allows data to be processed at various levels of abstraction, thereby discovering knowledge at multiple levels of abstraction.\nUse higher-level concepts to replace lower-level concepts (such as the value of age in this example) to reduce the number of values.\nAlthough some details disappeared in the process of data generalization,\nBut the generalized data obtained in this way may be easier to understand and more meaningful.\nData analysis on the reduced data set is obviously more efficient.\nThe concept hierarchy can be represented by a tree, and each node of the tree represents a concept of certain level.\nIn this session we learned Data reduction and related technologies.\nThank you for your attention, if you have any question, feel free to contact me."
}