{
  "topic": "Spark MLlib Concepts and Mechanisms",
  "summary": "This session introduces Spark MLlib, Apache Spark's Machine Learning Library, emphasizing its simplicity, scalability, and integration with tools like Spark SQL and DataFrames API. MLlib supports various data types and is usable in Java, Scala, and Python. It facilitates preprocessing, model training, and predictions at scale. Big data analytics using Spark MLlib includes descriptive analysis (statistical descriptions, clustering) and predictive analysis (feature modeling, prediction algorithms). MLlib's key features are ease of use, performance (faster than MapReduce), and versatility in deployment. The library comprises algorithms, featurization, pipelines, and utilities. ML Pipelines use Transformers (feature transformers, learned models) and Estimators (learning algorithms) for tasks like data normalization and model training. Evaluators assess model performance. The session also covers building parameter grids for model selection, using CrossValidator for data splitting, and ML persistence for saving and loading pipelines. Finally, it compares MLlib with scikit-learn, recommending scikit-learn for smaller datasets.",
  "keywords": [
    "Spark MLlib",
    "machine learning",
    "big data analytics",
    "ML Pipelines",
    "Transformers",
    "Estimators",
    "Evaluators",
    "CrossValidator",
    "parameter tuning",
    "scikit-learn"
  ],
  "concepts": [
    {
      "name": "Spark MLlib",
      "definition": "Apache Spark’s Machine Learning Library (MLlib) is designed for simplicity, scalability, and easy integration with other tools.",
      "text_evidence": "Apache Spark’s Machine Learning Library (MLlib) is designed for simplicity, scalability, and easy integration with other tools."
    },
    {
      "name": "Transformers",
      "definition": "An abstraction that includes feature transformers and learned models, which Transforming data into consumable format, Take input column, transform it to an output column.",
      "text_evidence": "A Transformer is an abstraction that includes feature transformers and learned models. which Transforming data into consumable format, Take input column, transform it to an output column."
    },
    {
      "name": "Estimator",
      "definition": "An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data.",
      "text_evidence": "An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data."
    },
    {
      "name": "Evaluator",
      "definition": "Evaluator is designed to Evaluate the model performance based certain metrics, like ROC, RMSE.",
      "text_evidence": "Evaluator is designed to Evaluate the model performance based certain metrics, like ROC, RMSE."
    },
    {
      "name": "ML Pipelines",
      "definition": "ML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines.",
      "text_evidence": "ML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines."
    },
    {
      "name": "Param",
      "definition": "A Param is a named parameter with self-contained documentation.",
      "text_evidence": "A Param is a named parameter with self-contained documentation."
    },
    {
      "name": "ParamMap",
      "definition": "A ParamMap is a set of (parameter, value) pairs.",
      "text_evidence": "A ParamMap is a set of (parameter, value) pairs."
    },
    {
      "name": "CrossValidator",
      "definition": "CrossValidator divides the data set into several folds, which can be used for independent training and test sets.",
      "text_evidence": "CrossValidator divides the data set into several folds, which can be used for independent training and test sets."
    },
    {
      "name": "Scikit-learn (Sklearn)",
      "definition": "The most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python.",
      "text_evidence": "Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python."
    }
  ],
  "original_text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, from this session on, we will discuss something about real application,\nIncluding some platform and big data application and in this session, we will talk about the concepts and mechanism of Spark MLlib.\nFrom this chapter ,we will introduce some useful platform， Spark MLlib and TensorFlow.\nWe will show some case experiments applying the corresponding platform;\nand we will also explain 2 kinds of typical big data analysis scenarios, Recommendation System and Social Networking.\nApache Spark’s Machine Learning Library (MLlib) is designed for simplicity, scalability, and easy integration with other tools.\nWith the scalability, language compatibility, and speed of Spark, data scientists can focus on their data problems and models instead of solving the complexities surrounding distributed data (such as infrastructure, configurations, and so on).\nSpark MLlib seamlessly integrates with other Spark components such as Spark SQL, Spark Streaming, and DataFrames API.\nMachine learning can be applied to a wide variety of data types, such as vectors, text, images, and structured data.\nThis API adopts the DataFrame from Spark SQL to support a variety of data types.\nThe library is usable in Java, Scala, and Python as part of Spark applications, so that you can include it in complete workflows.\nMLlib allows for preprocessing, munging咀嚼, training of models, and making predictions at scale on data.\nYou can even use models trained in MLlib to make predictions in Structured Streaming.\nBig data analytics using spark MLlib include descriptive analysis and predictive analytics based on descriptive analytical data and predictive analytical data.\nIn the scope of descriptive analysis, it supports statistical descriptions analysis and clustering.\nIn the scope of predictive analysis, it supports Feature modeling, which includes feature extractor like TF-IDF, Feature transformer like Vector Slicer etc. and feature selector like Chi-square selector. It also supports prediction algorithms like binary classification, multiclass classification and regression.\nMLlib is Apache Spark's scalable machine learning library.\n1Ease of use\nUsable in Java, Scala, Python, and R.\nMLlib fits into Spark's APIs and interoperates with NumPy in Python (as of Spark 0.9) and R libraries (as of Spark 1.5).\nYou can use any Hadoop data source (e.g. HDFS, HBase, or local files), making it easy to plug into Hadoop workflows.\n2 Performance\nHigh-quality algorithms, 100x faster than MapReduce.\nSpark excels at iterative computation, enabling MLlib to run fast.\nAt the same time, we care about algorithmic performance: MLlib contains high-quality algorithms that leverage iteration, and can yield better results than the one yield on MapReduce.\n3 Runs everywhere\nSpark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud, against diverse data sources.\nYou can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, on Mesos, or on Kubernetes. Access data in HDFS, Apache Cassandra, Apache HBase, Apache Hive, and hundreds of other data sources.\nTo support Python with Spark, the Apache Spark community released a tool, PySpark. Using PySpark, one can work with RDDs in Python programming language.\nBuilt on top of Spark, MLlib is a scalable machine learning library consisting of 4 main components, Algorithms, Featurization, Pipeline and Utilities.\ncommon learning algorithms including classification, regression, clustering, collaborative filtering, dimensionality reduction, and underlying optimization primitives.\nand utilities, which include Linear algebra, statistics etc.\nAnd the Featurization include extracting the features and transforming the features.\nML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines.\nthe mechanism to finish the ML tasks is to construct the pipeline to finish all the needed steps. Like constructing model, train the model, evaluating the model, tuning the parameters and persistence the model.\nML algorithms include:\nClassification: logistic regression, naive Bayes, ...\nRegression: generalized linear regression, survival regression, ...\nDecision trees, random forests, and gradient-boosted trees\nRecommendation: alternating least squares (ALS)\nClustering: K-means, Gaussian mixtures (GMMs), ...\nTopic modeling: latent Dirichlet allocation (LDA)\nFrequent item sets, association rules, and sequential pattern mining\nML workflow utilities include:\nFeature transformations: standardization, normalization, hashing, ...\nML Pipeline construction\nModel evaluation and hyper-parameter tuning\nML persistence: saving and loading models and Pipelines\nOther utilities include:\nDistributed linear algebra: SVD, PCA, ...\nStatistics: summary statistics, hypothesis testing, ...\nOn the left, it is the normal machine learning pipeline, 1Load/clean data, 2 feature extraction, 3 model training and 4model evaluation and parameter tuning, then repeat the workflow process.\nAnd on the right, it is the Mllib Pipeline Concepts, from 1 load/clean data, 2 transformers, which is corresponding to feature engineering, 3 Estimator, which is corresponding to model training, and 4 Evaluator, which is responsible for model evaluation. Let me explain them one by one\nA Transformer is an abstraction that includes feature transformers and learned models.\nwhich Transforming data into consumable format,\nTake input column, transform it to an output column.\nTechnically, a Transformer implements a method transform (), which converts one DataFrame into another, generally by appending one or more columns.\nFor example:\nA feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.\nA learning model might take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and output a new DataFrame with predicted labels appended as a column.\nExamples such as : 1 Normalize the data, 2 Tokenization (which means dividing the sentences into words) and 3 Converting categorical values into numbers.\nLike showed in the diagram, transform the data frame 1 into data frame 2.\nAn Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data.\nTechnically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer.\nFor example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.\nEvaluator is designed to Evaluate the model performance based certain metrics, like ROC, RMSE.\nAnd Evaluator can Help with automating the model tuning process through Comparing model performance\nAnd Selecting the best model for generating predictions.\nThe Example here is BinaryClassificationEvaluator with CrossValidator.\nInput is the data and output is the best model selected from all the options.\nIn machine learning, it is common to run a sequence of algorithms to process and learn from data.\nE.g., a simple text document processing workflow might include several stages:\nSplit each document’s text into words.\nConvert each document’s words into a numerical feature vector.\nLearn a prediction model using the feature vectors and labels.\nMLlib represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order.\nThe pipeline Leverages the uniform API of Transformer & Estimator. It Can be persisted.\nThese stages are run in order, and the input DataFrame is transformed as it passes through each stage.\nFor Transformer stages, the transform () method is called on the DataFrame.\nFor Estimator stages, the fit () method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline), and that Transformer’s transform () method is called on the DataFrame.\nWe illustrate this for the simple text document workflow.\nThe figure in the middle, the top row represents a Pipeline with three stages.\nThe first two (Tokenizer and HashingTF) are Transformers (blue), and the third (LogisticRegression) is an Estimator (red).\nThe bottom row represents data flowing through the pipeline, where cylinders indicate DataFrames.\nThe Pipeline.fit() method is called on the original DataFrame, which has raw text documents and labels.\nThe Tokenizer.transform() method splits the raw text documents into words, adding a new column with words to the DataFrame.\nThe HashingTF.transform() method converts the words column into feature vectors, adding a new column with those vectors to the DataFrame.\nNow, since LogisticRegression is an Estimator, the Pipeline first calls LogisticRegression.fit() to produce a LogisticRegressionModel.\nIf the Pipeline had more Estimators, it would call the LogisticRegressionModel’s transform() method on the DataFrame before passing the DataFrame to the next stage.\nA Pipeline is an Estimator.\nThus, after a Pipeline’s fit() method runs, it produces a PipelineModel, which is a Transformer (which means the model is transformer).\nThis PipelineModel is used at test time; the figure at the bottom illustrates this usage.\nthe PipelineModel has the same number of stages as the original Pipeline, but all Estimators in the original Pipeline have become Transformers.\nWhen the PipelineModel’s transform () method is called on a test dataset, the data are passed through the fitted pipeline in order.\nEach stage’s transform () method updates the dataset and passes it to the next stage.\nPipelines and PipelineModels help to ensure that training and test data go through identical feature processing steps.\nMLlib Estimators and Transformers use a uniform API for specifying parameters.\nA Param is a named parameter with self-contained documentation.\nA ParamMap is a set of (parameter, value) pairs.\nThere are two main ways to pass parameters to an algorithm:\nSet parameters for an instance. E.g., if lr is an instance of LogisticRegression, one could call lr.setMaxIter(10) (maximum iteration)to make lr.fit() use at most 10 iterations. This API resembles the API used in spark.mllib package.\nPass a ParamMap to fit() or transform(). Any parameters in the ParamMap will override parameters previously specified via setter methods.\nParameters belong to specific instances of Estimators and Transformers. For example, if we have two LogisticRegression instances lr1 and lr2, then we can build a ParamMap with both maxIter parameters specified: ParamMap(lr1.maxIter -> 10, lr2.maxIter -> 20). This is useful if there are two algorithms with the maxIter parameter in a Pipeline.\nAfter you build a pipeline in MLlib, it can Automate the model tuning process.\nA very important task in ML is model selection, or to say, the use of data to find the best model or parameters for a given task.\nThis is called parameter tuning. Tuning can be performed on a single Estimators (such as LogisticRegression) or\non the entire Pipeline (which can include multiple algorithms, characterization and other steps).\nWe need to Build a param grid for grid search-based model selection,\nTo build parameter grid, we can use ParamGridBuilder tool Class.\nParamGridBuilder() allows to specify different values for a single parameters,\nand then compare the entire set of parameters to choose the best options, which define the best model.\nCrossValidator divides the data set into several folds, which can be used for independent training and test sets.\nFor example: when k=5 folds, CrossValidator will generate 5 (training, test) pairs, each of which uses 4/5 data as the training set and 1/5 as the test set.\nTo evaluate a ParamMap, use Estimator to fit 5 models on 5 different data pairs, and CrossValidator will calculate the average of 5 evaluation metrics.\nAfter selecting the best ParamMap, CrossValidator will finally use the corresponding Estimator and the best ParamMap to refit the entire data set.\nML persistence means Saving and Loading Pipelines\nOften it is worth it to save a model or a pipeline to disk for later use.\nThe data scientist creates a model or a pipeline and the data engineer can deploy model at scale and monitor its application.\nIn Spark 1.6, a model import/export functionality was added to the Pipeline API.\nAs of Spark 2.3, the DataFrame-based API in spark.ml and pyspark.ml has complete coverage.\nML persistence works across Scala, Java and Python.\nHowever, R currently uses a modified format, so models saved in R can only be loaded back in R; this should be fixed in the future and is tracked in SPARK-15572.\nBackwards compatibility for ML persistence\nIn general, MLlib maintains backwards compatibility for ML persistence.\nI.e., if you save an ML model or Pipeline in one version of Spark, then you should be able to load it back and use it in a future version of Spark.\nConsider that Spark’s ML Lib is suitable when you’re doing relatively simple ML on a large data set.\nML Lib is not computationally efficient for small data sets, and you’re better off using scikit-learn for small and medium sized data sets (megabytes, up to a few gigabytes).\nWe have the hands on carried out on sklearn, which include the algorithm and high-level tools.\nScikit-learn (Sklearn) is the most useful and robust library for machine learning in Python.\nIt provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python.\nIn this session, we learned concepts and mechanism of spark MLlib.\nthank you for your attention, if you have any question, feel free to contact me."
}