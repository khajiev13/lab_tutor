{
  "topic": "Data Preprocessing Techniques",
  "summary": "This session introduces data preprocessing, which includes data cleaning, transformation, and reduction. Data cleaning addresses dirty data arising from diverse sources, using methods like handling missing values (ignoring tuples, manual filling, mean/probable value imputation) and noisy data (binning, regression, clustering). Data transformation involves normalization, attribute selection, discretization, and concept hierarchy generation. Data reduction encompasses data cube aggregation, attribute subset selection, numerosity reduction, and dimensionality reduction. Data integration stores all data in a unified repository. Data specification eliminates non-descriptive attributes. Data cleaning technologies involve anomaly detection using statistical methods, data deduplication, and missing data cleaning using approximate values from Bayesian networks, neural networks, KNN, and rough set theory. Data reduction focuses on dimensionality reduction (stepwise selection, decision trees), reducing data volume (histogram, clustering), and data discretization to convert continuous attributes to discrete values.",
  "keywords": [
    "data preprocessing",
    "data cleaning",
    "data transformation",
    "data reduction",
    "missing data",
    "noisy data",
    "dimensionality reduction",
    "data discretization",
    "data integration",
    "data specification"
  ],
  "concepts": [
    {
      "name": "Data Cleaning",
      "definition": "Includes missing data processing and noisy data processing.",
      "text_evidence": "data cleaning includes missing data processing and noisy data processing."
    },
    {
      "name": "Data Transformation",
      "definition": "Includes Normalization, attributes selection, discretization and concept hierarchy generation.",
      "text_evidence": "As for data transformation, it includes Normalization, attributes selection, discretization and concept hierarchy generation."
    },
    {
      "name": "Data Reduction",
      "definition": "Includes data cube aggregation, attribute subset selection, numerosity reduction which is Reduce the amount of data and dimensionality reduction.",
      "text_evidence": "Data reduction includes data cube aggregation, attribute subset selection, numerosity reduction which is Reduce the amount of data and dimensionality reduction."
    },
    {
      "name": "Data Integration",
      "definition": "Store all data in a database, data warehouse or file to form a complete data set.",
      "text_evidence": "Store all data in a database, data warehouse or file to form a complete data set."
    },
    {
      "name": "Data Specification",
      "definition": "Eliminate the data attributes that cannot describe the key characteristics of the system, and only retain part of the data attribute set that can describe the key characteristics.",
      "text_evidence": "Eliminate the data attributes that cannot describe the key characteristics of the system, and only retain part of the data attribute set that can describe the key characteristics."
    },
    {
      "name": "Dimensionality reduction",
      "definition": "The process mainly adopts the method of market redundant data attributes, and deleting redundant data attributes requires business knowledge in a certain field.",
      "text_evidence": "The process mainly adopts the method of market redundant data attributes, and deleting redundant data attributes requires business knowledge in a certain field."
    },
    {
      "name": "Data Discretization Technology",
      "definition": "This technology can convert continuous attributes into discrete attribute values, reducing the number of attribute values, thereby reducing the computing time for processing data",
      "text_evidence": "This technology can convert continuous attributes into discrete attribute values, reducing the number of attribute values, thereby reducing the computing time for processing data"
    }
  ],
  "original_text": "Big data 3-1\n1 Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session, we will discuss Data Preprocessing.\n2 Data Preprocessing mainly includes data cleaning Data transformation and data reduction\nWhy do we need to do data cleaning? Because when we collect data from different data resources, we inevitably have some dirty data in the results.\nLet’s look at an example, in this table, in the second row , for the record id=2, the age attributes is missing.\nIf we calculate the average age, this missing age value will Serious impact the average result.\nand the third and fourth records are repeated. And the sixth record is conflict record, according to the age and the birth data.\nthe seventh record is abnormal age 101 according to the birth data. All these dirty data will damage the analysis results.\nAccording to Murphy's Law: As long as everything can go wrong, it will go wrong.\nHow to prevent dirty data from appearing, we can try 2 different ways.\n1) Develop data standards:\nUnify attribute value encoding of multiple data sources\nGive the attribute name and attribute value as clear as possible\n2) Optimize system design:\nUse options as much as possible for key attributes instead of manually filling in the entry.\nImportant attributes appear in a prominent position, use required options\nOutliers should be modified\nData preprocessing could include data cleaning; data transformation and data reduction\n1) data cleaning includes missing data processing and noisy data processing.\n1.1 For missing data we can directly ignore the tuple, or record. Or we can fill the missing values manually, by mean or by most probable value.\n1.2 eliminating the noisy data methods include binning methods, regression and clustering.\n2) As for data transformation, it includes Normalization, attributes selection, discretization and concept hierarchy generation.\n3) The third aspect of data preprocessing is data reduction.\nData reduction includes data cube aggregation, attribute subset selection, numerosity reduction which is Reduce the amount of data and dimensionality reduction.\n4) Data integration:\nStore all data in a database, data warehouse or file to form a complete data set.\n5) Data specification:\nEliminate the data attributes that cannot describe the key characteristics of the system, and only retain part of the data attribute set that can describe the key characteristics.\nAnd others…\nThe research on data cleaning technology first began with the correction of the US Social Security number. Later, with the rapid development of information and commerce,\nthe research in this area was accelerated, including\n1) Perform anomaly detection on the data set,\nusually using statistical methods to detect the numerical attributes of the data. By calculating the mean and standard deviation of the attribute values ​​and other indicators, identify the anomalous attributes and records within the confidence interval of each attribute.\n2) Deduplication of data:\nThe process of data deduplication is to repeat the process of cleaning data records.\nThis process is particularly important in data warehouse applications, because when integrating data from different data sources, many duplicate data records may be generated.\n3) Cleaning of missing data: Most of them use approximate values ​​to replace missing values ​​to clean the data.\nMethods to obtain approximate values ​​include Bayesian networks, neural networks, KNN classification, rough set theory, etc.\nThe core of these methods is to judge missing records and Similarity between other complete records.\nDomestic research in this area is still in its initial stage.\nData cleaning is mainly concentrated in industries that require high accuracy of customer data, such as banking, insurance, and securities. These industries only do data cleaning for their customers, and only develop software for specific applications, without general products\nIt takes a long time to analyze and mine massive data. To make data mining more effective, data needs to be regulated. The main research content of the data reduction:\n1) Dimensionality reduction processing of high-dimension data\nThe process mainly adopts the method of market redundant data attributes, and deleting redundant data attributes requires business knowledge in a certain field. Commonly used dimensionality reduction methods include stepwise forward selection method, stepwise backward deletion method, and decision tree induction method Wait\n2) Reduce the amount of data\nWhen it takes a long time to process a large amount of data, it cannot meet the requirements of some applications with high real-time requirements. At this time, the amount of data needs to be reduced. The methods of comrades in this process, including histogram, clustering, etc., and then select smaller-scale data from the data set\n3) Data Discretization Technology\nThis technology can convert continuous attributes into discrete attribute values, reducing the number of attribute values, thereby reducing the computing time for processing data\nIn this session we general introduced Data Preprocessing.\nthank you for your attention, if you have any question, feel free to contact me."
}