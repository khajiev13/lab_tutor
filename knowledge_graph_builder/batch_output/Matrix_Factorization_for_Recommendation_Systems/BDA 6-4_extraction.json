{
  "topic": "Matrix Factorization for Recommendation Systems",
  "summary": "This session focuses on Matrix Factorization, a Latent Factor model used in recommendation systems. It begins by decomposing the user-item rating matrix into user-latent factor and item-latent factor matrices. Singular Value Decomposition (SVD) is introduced but replaced by Matrix Decomposition due to missing values in user-item rating matrices. The process involves defining a cost function with MSE and regularization terms, then minimizing it using Alternating Least Squares (ALS) or Gradient Descent. Experiments are conducted using User-based filtering and Matrix Decomposition, including preprocessing steps like data loading and similarity computation, and utilizing the Surprise library for SVD model training and parameter tuning via Grid Search. The session provides hands-on materials for experimentation.",
  "keywords": [
    "Matrix Factorization",
    "Latent Factor Model",
    "Recommendation Systems",
    "SVD",
    "ALS",
    "Gradient Descent",
    "Cost Function",
    "User-based filtering",
    "Surprise library"
  ],
  "concepts": [
    {
      "name": "Matrix Factorization",
      "definition": "a typical Latent Factor model",
      "text_evidence": "in this session we will learn a typical Latent Factor model, Matrix Factorization."
    },
    {
      "name": "Singular Value Decomposition (SVD)",
      "definition": "a factorization of a real or complex matrix. It generalizes the eigen decomposition of a square normal matrix with an orthonormal eigen basis to any m*n matrix.",
      "text_evidence": "In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigen decomposition of a square normal matrix with an orthonormal eigen basis to any m*n matrix."
    },
    {
      "name": "Alternating Least Square (ALS)",
      "definition": "fix P, compute Q to make c min; then, fix Q, compute P to make c min",
      "text_evidence": "using ALS (Alternating Least Square) to minimize cost function :which means fix P, compute Q to make c min; then, fix Q, compute P to make c min;"
    },
    {
      "name": "Gradient Descent",
      "definition": "Compute partial derivative of C with respect to Pu and partial derivative of C with respect to Qi. then Do Iteration using the formula in the slide (where is step size)",
      "text_evidence": "using Gradient Descent to minimize cost function, which means Compute partial derivative of C with respect to Pu and partial derivative of C with respect to Qi. then Do Iteration using the formula in the slide (where is step size)"
    },
    {
      "name": "Cost function",
      "definition": "evaluate the better choice of P and Q. We only calculate cost function with the already given rating values by the users.",
      "text_evidence": "Define cost function as showed in the formula, and use the cost function to evaluate the better choice of P and Q. We only calculate cost function with the already given rating values by the users."
    }
  ],
  "original_text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology\nin last session we learned 3 main Recommendation System Algorithms,\nUser-based filtering, Item-based filtering and Content-Based Filtering, and\nin this session we will learn a typical Latent Factor model, Matrix Factorization.\nLet’s watch a video “How Recommender Systems Work (NetflixAmazon)\nFrom the video we know there is hidden patterns in the Rating Matrix,\nWe want to Find some character the items may have.\nAnd we Decompose the rating matrix into item-character rating & user-character rating.\nMoreover, we don’t want to know the meaning of characters. Which is Abstract model, we just suppose the number of characters.\nIn linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix.\nIt generalizes the eigen decomposition of a square normal matrix with an orthonormal eigen basis to any m*n matrix.\nIt is related to the polar decomposition.\nSpecifically, the singular value decomposition of an m*n complex matrix M is a factorization of the form U,\nwhere UΣV*, where U is an m*m complex unitary matrix,\nΣ is an m*n rectangular diagonal matrix with non-negative real numbers on the diagonal,\nand V is an n*n complex unitary matrix.\nIf M is real, U and V can also be guaranteed to be real orthogonal matrices.\nIn such contexts, the SVD is often denoted  UΣ(V transpose)\nSVD requires dense matrix, that is the matrix doesn’t have missing values.\nEvidently, user-item rating matrix has lots of missing values.\nSo, use Matrix Decomposition to replace SVD.\nDecompose the matrix into two matrix, that is , where is user-item rating matrix, is user-LF(Latent factor ) matrix, and is item-LF(Latent factor ) matrix.\nFor u-user and i-item, their rating is:\nIf get two dense matrix , from we can predict the missing value in .\nSo how to calculate ?\nDefine cost function as showed in the formula, and use the cost function to evaluate the better choice of P and Q. We only calculate cost function with the already given rating values by the users.\nThe first part in the cost function is the MSE of the predict rating value Rui hat and true value Rui.\nAnd the second part in the cost function is regular value, which prevent overfitting.\nWe do the iteration to minimize the cost function.\nTwo ways to minimize cost function:\n1) using ALS (Alternating Least Square) to minimize cost function :which means fix P, compute Q to make c min; then, fix Q, compute P to make c min;\nEnd until reach max iteration or c satisfies threshold condition.\nCompute partial derivative of C with respect to Pu, and make the formula equals to 0, get .\nSimilarly get .\n2) using Gradient Descent to minimize cost function, which means Compute partial derivative of C with respect to Pu\nand partial derivative of C with respect to Qi.\nthen Do Iteration using the formula in the slide (where is step size)\nAfter each iteration, the Pu and Qi are updated until the end, until reach max iteration or c satisfies threshold condition.\nTo understand the recommendation system, A series of experiments were designed,\nit includes User-based filtering recommendation and Matrix Decomposition.\nIn User-based filtering recommendation, it consist preprocessing and Collaborative Filtering.\nIn 1.1 preprocessing, it includes load the data and relate the two original tables, and create a new data.csv file, and make a dictionary by deleting the duplicate records.\nIn 1.2 Collaborative Filtering, first compute the user similarity, and then list top 10 similar users to the current user, and make the recommendation.\nFor matrix decomposition experiment, we need import the library surprise, the used data set includes 100,000 user’s ratings on movies.\nThe related models include Funk or Bias SVD, Grid Search for training.\nThe goal is to Train and test on the best model and Get the best parameters for SVD,\nThe process will be\n1）Import library\n2）import data\n3）Grid search SVD training\n4）Use the best parameters obtained by grid search for raining and prediction\nAnd finally visualize the Result.\nAll the experiments material including the manual and codes are provided on the platform, which can help you to do the hands-on.\nIn this session, we learned a typical Latent Factor model, Matrix Factorization.\nthank you for your attention, if you have any question, feel free to contact me."
}