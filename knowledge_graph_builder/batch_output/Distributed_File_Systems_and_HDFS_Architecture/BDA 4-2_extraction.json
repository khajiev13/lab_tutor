{
  "topic": "Distributed File Systems and HDFS Architecture",
  "summary": "This session discusses distributed file systems, focusing on HDFS as a key component of big data storage architecture. The big data computing system is categorized into data storing, data processing, and data application systems, with the data storage architecture being fundamental. Distributed file systems, like HDFS and GFS (Colossus), provide physical storage, enabling data access and sharing across a network. HDFS, a master-slave architecture, includes a Name node (master) for managing the file system namespace and Data Nodes (slaves) for storing data blocks. Data is replicated for high availability. The Name node manages metadata, while Data nodes store file data blocks. Data writing involves the client creating a file, the Name node performing checks, and data being split into packets and streamed to Data nodes. Data reading involves the client opening a file, the Name node providing Data node locations, and data being streamed back to the client. HDFS supports batch reading and writing but not updating, and data is immutable once written.",
  "keywords": [
    "distributed file system",
    "HDFS",
    "Name node",
    "Data node",
    "data blocks",
    "data storage",
    "big data",
    "GFS",
    "Colossus",
    "master-slave architecture"
  ],
  "concepts": [
    {
      "name": "Distributed file system",
      "definition": "In distributed file system the data is accessed and processed as if it was stored on the local client machine.",
      "text_evidence": "In distributed file system the data is accessed and processed as if it was stored on the local client machine."
    },
    {
      "name": "HDFS",
      "definition": "HDFS adopts a master-slave structure. And HDFS save 3 copies of each data block, which means use redundancy to achieve high availability.",
      "text_evidence": "HDFS adopts a master-slave structure. And HDFS save 3 copies of each data block, which means use redundancy to achieve high availability."
    },
    {
      "name": "Name node",
      "definition": "As the central service node, the Name node is responsible for managing the file system namespace, the mapping relationship from data files to data blocks to Data nodes, and client scheduling of file access. And the metadata is stored in memory for quick access.",
      "text_evidence": "As the central service node, the Name node is responsible for managing the file system namespace, the mapping relationship from data files to data blocks to Data nodes, and client scheduling of file access. And the metadata is stored in memory for quick access."
    },
    {
      "name": "Data node",
      "definition": "Store file data block, Realize the mapping of data blocks to the local file system of the data node and Data blocks are stored on the local disk",
      "text_evidence": "Data node Store file data block, Realize the mapping of data blocks to the local file system of the data node and Data blocks are stored on the local disk"
    },
    {
      "name": "GFS (Google file system)",
      "definition": "Google’s GFS (Google file system) which has evolved into Colossus.",
      "text_evidence": "Google’s GFS (Google file system) which has evolved into Colossus."
    },
    {
      "name": "Colossus",
      "definition": "Google’s GFS (Google file system) which has evolved into Colossus.",
      "text_evidence": "Google’s GFS (Google file system) which has evolved into Colossus."
    }
  ],
  "original_text": "BDA 4-2 DFS\n1 Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology , in this session we discuss about distributed file system .\n2 The big data computing system can be summarized into three categories:\nData storing system, Data processing system, Data application system\nThe data storage architecture is the foundation of big data computing.\nIn data storing system , there are 4 parts to accomplish different tasks,\nwhich are Data collection and modeling, Distributed file system, Distributed database/data warehouse and Unified Data Access Interface.\nwe learned data collection and modeling, now we focus on distributed file system.\nActually File system could be Centralized or distributed file system, but in the big data scenario, mostly we use distributed file system to achieve the scale and the efficiency.\nThe distributed file system provides a physical storage architecture for data.\nFile system with data stored on servers. In distributed file system the data is accessed and processed as if it was stored on the local client machine.\nIt is Convenient to share information and files among users on a network in a controlled and authorized way, which is transparent to the users.\nAnd it reaches multiplied storage of a single server.\nAt present, there are the two main file systems in the big data computing architecture, which are\nthe open source community’s architecture HDFS and Google’s GFS (Google file system) which has evolved into Colossus.\nLet ‘s look at the HDFS\nHDFS adopts a master-slave structure. And HDFS save 3 copies of each data block, which means use redundancy to achieve high availability.\nAn HDFS cluster includes a Name node, which is the master node, and several Data Nodes, which are slave nodes.\nAs the central service node, the Name node is responsible for managing the file system namespace, the mapping relationship from data files to data blocks to Data nodes,\nand client scheduling of file access. And the metadata is stored in memory for quick access.\nHDFS also has a secondary name node, which is regularly connected to the primary name node, and the instant image of the system directory is stored on the local disk.\nWhen the primary name node is fails or crashes, the secondary name node can provide the name node rollback recovery and restart functions.\nData node Store file data block, Realize the mapping of data blocks to the local file system of the data node and Data blocks are stored on the local disk\nIn HDFS, each storage file is first divided into multiple data blocks with a fixed length of 64MB or 128MB,\nthese data blocks are replicated to 3 copies and stored on different Data nodes according to a certain rule.\nWhen one data node crashes, we can still retrieve the same data blocks from other 2 copies of another 2 data nodes.\nOnce the data is written, it can’t be changed , so the data in HDFS is immutable. So HDFS just support batch reading and writing operation, but doesn’t support updating operation.\nThis means that a Data Node can store data blocks from different files.\nEach data node runs a node program or process, which is responsible for processing read and write requests from the file system client.\nThe creation, deletion, and replication of data blocks are performed under the unified scheduling of the Name node.\nThe master node Name node and the slave node Data node perform their respective tasks\nNow let’s understand the Process of writing data in HDFS , The diagram summarizes file write operation in Hadoop.\nThe client creates the file by calling create() method on DistributedFileSystem.\nDistributedFileSystem makes an RPC call to the namenode to create a new file in the filesystem’s namespace, with no blocks associated with it.\nThe namenode performs various checks to make sure the file doesn’t already exist and the client has the right permissions to create the file. If all these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an IOException. TheDistributedFileSystem returns an FSDataOutputStream for the client to start writing data to datanode. FSDataOutputStream wraps a DFSOutputStream which handles communication with the datanodes and namenode.\nAs the client writes data, DFSOutputStream splits it into packets, which it writes to an internal queue, called the data queue. The data queue is consumed by the DataStreamer, which is responsible for asking the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas. The list of datanodes forms a pipeline, and default replication level is three, so there are three nodes in the pipeline. The DataStreamer streams the packets to the first datanode in the pipeline, which stores the packet and forwards it to the second datanode in the pipeline.\nSimilarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipeline.\nDFSOutputStream also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the ack queue. A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline.\nWhen the client has finished writing data, it calls close() on the stream.It flushes all the remaining packets to the datanode pipeline and waits for acknowledgments before contacting the namenode to signal that the file is complete The namenode already knows which blocks the file is made up of , so it only has to wait for blocks to be minimally replicated before returning successfully.\nThe Process of reading data in HDFS 2.0 is like this,\nThe client opens the file by calling open() method on DistributedFileSystem.\nDistributedFileSystem makes an RPC call to the namenode to determine location of datanodes where files is stored in form of blocks.For each blocks,the namenode returns address of datanodes(metadata of blocks and datanodes) that have a copy of block. Datanodes are sorted according to proximity(depending of network topology information).\nThe DistributedFileSystem returns an FSDataInputStream (an input stream that supports file seeks) to the client for it to read data from. FSDataInputStream in turn wraps a DFSInputStream, which manages the datanode and namenode I/O.\nThe client then calls read() on the stream. DFSInputStream, which has stored the datanode addresses for the first few blocks in the file, then connects to the first (closest) datanode for the first block in the file.\nData is streamed from the datanode back to the client (in the form of packets) and read () is repeatedly called on the stream by client.\nWhen the end of the block is reached, DFSInputStream will close the connection to the datanode, then find the best datanode for the next block (Step 5)\nWhen the client has finished reading, it calls close() on the FSDataInputStream (step 6).\nIn addition, During reading, if the DFSInputStream encounters an error while communicating with a datanode, it will try the next closest one for that block.It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks.\nThe DFSInputStream also verifies checksums for the data transferred to it from the datanode. If a corrupted block is found, the DFSInputStream attempts to read a replica of the block from another datanode; it also reports the corrupted block to the namenode.\nIn this session we learned the big data distributed file system mechanism using HDFS as an example, which is physical store of big data .\nWe learned the architecture of HDFS, name node, data node, and their responsibilities,\nWe also learned the data writing and data reading process of HDFS.\nthank you for your attention, if you have any question, feel free to contact me."
}