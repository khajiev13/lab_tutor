{
  "topic": "External Data Acquisition using Web Crawlers",
  "summary": "This session discusses external data acquisition, focusing on network big data collected via web crawlers. Network big data, generated by the interaction of humans, machines, and materials in cyberspace, exhibits characteristics like multi-source heterogeneity, timeliness, sociality, interactivity, suddenness, and high noise. Web crawlers, starting with seed URLs, browse the internet, identify links, and download web pages, following a crawling strategy. Common crawling strategies include depth-first, breadth-first, Partial PageRank, and OPIC. For distributed crawling, architectures like master-slave, peer-to-peer, and mixed structures are used to handle the large scale of the task. The session covers the web crawler process, crawling strategies, and distributed web crawler structures.",
  "keywords": [
    "external data",
    "network big data",
    "web crawler",
    "crawling strategy",
    "depth-first",
    "breadth-first",
    "PageRank",
    "OPIC",
    "distributed architecture"
  ],
  "concepts": [
    {
      "name": "Network big data",
      "definition": "Big data generated by the interaction and integration of the ternary world of \"human, machine, and material\" in cyberspace and available on the Internet.",
      "text_evidence": "Big data generated by the interaction and integration of the ternary world of \"human, machine, and material\" in cyberspace and available on the Internet."
    },
    {
      "name": "Web crawler",
      "definition": "A program or web robot that automatically browses the Internet and get data.",
      "text_evidence": "A web crawler is a program or web robot that automatically browses the Internet and get data."
    },
    {
      "name": "Depth first",
      "definition": "A crawling strategy where the crawler goes as deep as possible into each branch before exploring other branches.",
      "text_evidence": "Here is a example of Fan out URLs structure, and if use depth first strategy, the sequence should be M1-M2-M5-M8-M6-M3-S7-S4"
    },
    {
      "name": "Breadth first",
      "definition": "A crawling strategy where the crawler explores all the neighbors of the current node before moving to the next level.",
      "text_evidence": "And if use Breadth first strategy, the sequence should be M1-M2-M3-S4-M5-M6-S7-M8"
    },
    {
      "name": "PageRank",
      "definition": "A technology applied by search engines based on mutual hyperlinks between web pages，Invented by Google founder Larry Page (Larry Page).",
      "text_evidence": "PageRank, also known as Page Ranking and Google Ranking is a technology applied by search engines based on mutual hyperlinks between web pages，Invented by Google founder Larry Page (Larry Page)."
    },
    {
      "name": "OPIC (online Page Importance Computation)",
      "definition": "A crawling strategy that assigns the same \"gold coins\" to each web page. Whenever a page P is downloaded, the \"gold coins\" owned by P are equally distributed to the linked pages contained in the web page.",
      "text_evidence": "The OPIC strategy assigns the same \"gold coins\" to each web page. Whenever a page P is downloaded, the \"gold coins\" owned by P are equally distributed to the linked pages contained in the web page."
    },
    {
      "name": "Distributed Architecture model",
      "definition": "The effective collaboration and cooperation of multiple stand-alone crawler systems to achieve Internet big data capture.",
      "text_evidence": "Through the effective collaboration and cooperation of multiple stand-alone crawler systems to achieve Internet big data capture. which is Distributed Architecture model."
    }
  ],
  "original_text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session, we will discuss\nSome idea about External Data Acquisition.\nExternal data is mainly Network big data, which is the data available in the internet,\nand the way to collect network data is using web crawlers.\nIn one URL, there may be many links and the sub-links of the upper level links, so which links should be crawled first and the crawling priority and sequences decided by the Web crawler crawling strategy.\nAnd in this session, we will discuss the network big data, web crawler and its strategy.3\nBig data generated by the interaction and integration of the ternary world of \"human, machine, and material\" in cyberspace and available on the Internet.\nNetwork big data characteristics include:\n1)Multi-source and heterogeneous, which means the internet data comes from Multi-sources, and the different source has different data.\n2) Timeliness which means when something happens, it can be published immediately.\n3) Sociality, the network big data directly reflects the social status\n4) Interactivity: WeChat Weibo, Facebook, twitter, etc. Internet users can not only publish information according to their needs, but also reply and forward information according to their personal preferences.\n5) Sudden: Some news dissemination will cause a large amount of new network data to be generated in a short time,\nreflecting the suddenness of network big data and network groups\n6) High noise characteristic is easy to understand, the internet data cannot be 100% true and useful, no one is responsible for the quality of internet data, so value density is low, full of dirty data, when you want use the internet data, you must clean it.\nLike showed in the diagram, A web crawler is a program or web robot that automatically browses the Internet and get data.\n1)Web crawler crawling process Starts with a list of uniform resource addresses called seed URL and use it as the link entry for crawling. When the crawler visits these seed URL s, it identifies all the needed links on the page and adds them to the queue to be crawled.\n2) After that, the webpage links are taken out from the queue to be crawled, then Read URL, do the DNS resolution, and web pages were download into the Downloaded web library.\n3) Put the already downloaded URL into the crawled URL list\n4) Extract the new URL into the URL queue to be crawled and put them in the to be crawled URL queue according to strategy\n5) all the process will end until the queue for crawling is empty.\nHow to deal with fan-out URLs in seed URLs, which is the links of the link, which involves web crawler crawling strategies\nThe most often used Crawling strategies include\nDepth first\nBreadth first\nPartial PageRank Strategy\nOPIC (online Page Importance Computation)\nLet’ s explains them one by one\nHere is a example of Fan out URLs structure, and if use depth first strategy,\nthe sequence should be M1-M2-M5-M8-M6-M3-S7-S4;\nAnd if use Breadth first strategy, the sequence should be M1-M2-M3-S4-M5-M6-S7-M8;\nPageRank, also known as Page Ranking and Google Ranking is a technology applied by search engines based on mutual hyperlinks between web pages，\nInvented by Google founder Larry Page (Larry Page).\nThe PageRank link analysis algorithm expresses each page importance by counting the number and the importance of links pointed to by other webpages, thereby realizing the ranking of the importance of each webpage.\nThis algorithm takes the Quantity and quality into consideration.\nFor example, Links number of webpage E are far more than links to webpage C, but webpage C is much more important than webpage E.\nbecause page C is linked by page B, and page B is of high importance.\nUse the principle of PageRank to calculate the importance value in the URL list and sort the crawled web pages in order, and then traverse each URL in this order.\nThe OPIC strategy assigns the same \"gold coins\" to each web page. Whenever a page P is downloaded, the \"gold coins\" owned by P are equally distributed to the linked pages contained in the web page.\nThe links in the queue to be crawled are sorted by \"gold coins\"\nOPIC calculation speed is faster than partial PageRank strategy\nNow we know the crawling task is huge, it cant easily be done by one stand-alone crawler ,\nThrough the effective collaboration and cooperation of multiple stand-alone crawler systems to achieve Internet big data capture.\nwhich is Distributed Architecture model.\nThere are 3 basic Distributed Architecture model, master- slave, peer to peer and mixed structure.\nFor master- slave, the master needs to be updated when expanding,\nthe master node is under heavy pressure and easily becomes a bottleneck, and the number of slave nodes is limited.\nFor peer to peer, all nodes need to update all other nodes when communication is expanded.\nNo master, no single machine hot spots, and the number of slave nodes is limited\nFor mixed structure, the master communicates with the master, and the number of slave nodes could be small.\nLet’s summarize the external data and acquisition.\nWe studied the Web crawler crawling process, Web crawler crawling strategy,\nincluding Depth first, Breadth first, Page Rank, OPIC (online Page Importance Computation).\nAnd we learned the Distributed web crawler structure, master- slave, peer to peer and mixed structure.\nIn this session we learned and external data acquisition. thank you for your attention, if you have any question, feel free to contact me."
}