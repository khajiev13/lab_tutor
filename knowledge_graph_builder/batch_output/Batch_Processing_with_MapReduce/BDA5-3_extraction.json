{
  "topic": "Batch Processing with MapReduce",
  "summary": "This session discusses the batch processing computing model represented by MapReduce. The data processing system provides big data computing and an application development platform, divided into data algorithm, computing model, computing platform, and computing engine layers. Computing models include batch processing, stream computing, MPP, in-memory computing, and data flow graph models. MapReduce parallelizes computing tasks in a distributed environment. Data is retrieved from HDFS data blocks, organized as data splits, and fed into map tasks. The output undergoes a shuffle phase (sort, copy, merge) before becoming input for reduce tasks. The MapReduce architecture includes the Client, JobTracker, TaskTracker, and Task. The JobTracker monitors resources and schedules jobs, while TaskTrackers report resource usage and execute commands. Tasks are divided into Map and Reduce tasks, utilizing slots (CPU, memory) allocated by the Hadoop scheduler. HDFS and MapReduce are built on the same cluster, with the master node acting as both name node and job tracker, and slave nodes as datanodes and tasktrackers. Input data should be as close as possible to the corresponding map task to reduce transmission overhead.",
  "keywords": [
    "batch processing",
    "MapReduce",
    "HDFS",
    "JobTracker",
    "TaskTracker",
    "data processing",
    "shuffle phase",
    "Hadoop",
    "data blocks",
    "data split"
  ],
  "concepts": [
    {
      "name": "MapReduce",
      "definition": "batch processing model represented by MapReduce",
      "text_evidence": "in this session we discuss batch processing computing model, represented by MapReduce"
    },
    {
      "name": "data processing system",
      "definition": "provides big data computing and processing capabilities and an application development platform",
      "text_evidence": "The data processing system provides big data computing and processing capabilities and an application development platform."
    },
    {
      "name": "JobTracker",
      "definition": "responsible for resource monitoring and job scheduling",
      "text_evidence": "JobTracker is responsible for resource monitoring and job scheduling"
    },
    {
      "name": "TaskTracker",
      "definition": "will periodically report the usage of resources on the node and the progress of the task to the JobTracker through the \"heartbeat\", and at the same time receive the commands sent by the JobTracker and perform the corresponding operations (such as starting new tasks, killing tasks, etc.)",
      "text_evidence": "TaskTracker will periodically report the usage of resources on the node and the progress of the task to the JobTracker through the \"heartbeat\", and at the same time receive the commands sent by the JobTracker and perform the corresponding operations (such as starting new tasks, killing tasks, etc.)"
    },
    {
      "name": "Task",
      "definition": "is divided into Map Task and Reduce Task, which are started by TaskTracker",
      "text_evidence": "Task is divided into Map Task and Reduce Task, which are started by TaskTracker"
    },
    {
      "name": "Slot",
      "definition": "amount of resources (CPU, memory, etc.)",
      "text_evidence": "Slot is amount of resources (CPU, memory, etc.)"
    },
    {
      "name": "Hadoop scheduler",
      "definition": "is to allocate idle slots on each TaskTracker to the Task.",
      "text_evidence": "Hadoop scheduler is to allocate idle slots on each TaskTracker to the Task."
    },
    {
      "name": "HDFS",
      "definition": "storing unit in HDFS is data blocks",
      "text_evidence": "the storing unit in HDFS is data blocks"
    }
  ],
  "original_text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology , in this session we discuss batch processing computing model, represented by MapReduce.\nThe data processing system provides big data computing and processing capabilities and an application development platform.\nFrom the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing platform layer, computing engine layer, etc.\nComputing models are the way that different kinds of big data is processed in different scenarios,\nwhich include batch processing, stream computing, Large-scale concurrent processing (MPP) model for structured data, In-memory Computing model, and Data Flow Graph models.\nWe first look at the batch processing model represented by MapReduce\nLet’s watch a video about “Learn MapReduce with Playing Cards “ to easily understand the mechanism of map reduce process Intuitively\nFrom the video we understand the basic working mechanism of Map Reduce.\nMR tries to parallelize the big computing tasks in distributed environment to improve efficiency.\nthe storing unit in HDFS is data blocks, Retrieve the data from HDFS data blocks , then organize input data as data split feed into the map tasks.\nThe output of map task are going through the shuffle phase through sort , copy and merge operations showed in the diagram.\nAfter shuffle the reorganized middle results become the input of the reduce tasks, and after reduce phase , the final result is calculated.\nMapReduce architecture mainly four parts: Client, JobTracker,TaskTracker，Task\nMapReduce program written by the user is submitted to the JobTracker through the client\nUsers can view job running status through some interfaces provided by Client\n2) JobTracker\nJobTracker is responsible for resource monitoring and job scheduling\nJobTracker monitors the health status of all TaskTrackers and Jobs, and if it finds a failure, it will transfer the corresponding tasks to other nodes\nJobTracker will track the task execution progress, resource usage, and other information, and inform the task scheduler (TaskScheduler), and the scheduler will select the appropriate task to use these resources when resources become free\n3) TaskTracker\nTaskTracker will periodically report the usage of resources on the node and the progress of the task to the JobTracker through the \"heartbeat\",\nand at the same time receive the commands sent by the JobTracker and perform the corresponding operations (such as starting new tasks, killing tasks, etc.)\nTaskTracker uses \"slot\" to divide the amount of resources (CPU, memory, etc.) on this node. A Task has a chance to run after it gets a slot, and the role of the Hadoop scheduler is to allocate idle slots on each TaskTracker to the Task. Slots are divided into Map slot and Reduce slot, which are used by MapTask and Reduce Task respectively.\nTask is divided into Map Task and Reduce Task, which are started by TaskTracker\nTask scheduler is responsible for Selecting the appropriate task to use these resources when resources become free\nSlot is amount of resources (CPU, memory, etc.)., which include Map slot and Reduce slot.\nHadoop scheduler is to allocate idle slots on each TaskTracker to the Task.\nLet’s combine the HDFS and MapReduce together. HDFS and MapReduce should be built on the same cluster. The master node should be both name node in HDFS and Job tracker in MapReduce,\nand the slave node should be both the datanode in HDFS and tasktracker in MapReduce.\nFor example we build a one master node and three slave nodes HDFS and MapReduce cluster.\nIn order to reduce the data transmission overhead, we should try to make the input data of corresponding map task to be as close as possible, it is better on the same machine.\nIn this session, we learned batch processing model MapReduce.\nthank you for your attention, if you have any question, feel free to contact me."
}