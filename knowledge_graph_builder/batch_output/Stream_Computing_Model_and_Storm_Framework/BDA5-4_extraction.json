{
  "topic": "Stream Computing Model and Storm Framework",
  "summary": "This session discusses the stream computing model, contrasting it with traditional batch processing. Stream computing processes real-time dynamic data, crucial for applications needing fast responses and low latency. The data's immediate processing is emphasized over storage, focusing on overall results rather than individual data points. The stream computing framework includes real-time data collection, calculation, and query services. Storm, a Native Stream Processing System, uses a directed topology graph with Spouts (data sources) and Bolts (processing units). Storm's architecture is master-slave, with Nimbus on the master node managing worker nodes via Zookeeper and Supervisors. Micro-batch processing, represented by Spark, offers near real-time processing by collecting data in small batches, saving computing costs. Batch and stream computing can be combined for comprehensive analysis, with Kafka delivering data to Storm and Hadoop clusters, and outputs stored in NoSQL databases like Cassandra and Hbase for further analytics.",
  "keywords": [
    "stream computing",
    "Storm",
    "real-time data",
    "batch processing",
    "data flow",
    "topology",
    "Nimbus",
    "Zookeeper",
    "Micro-batch processing",
    "Spark"
  ],
  "concepts": [
    {
      "name": "Stream computing",
      "definition": "A computing model that processes real-time dynamic data.",
      "text_evidence": "Stream computing is a computing model that processes real-time dynamic data."
    },
    {
      "name": "Native Stream Processing System",
      "definition": "The processing of stream data is based on each piece of data, and its parallel calculation is implemented based on a directed topology graph.",
      "text_evidence": "Storm is a Native Stream Processing System, that is, the processing of stream data is based on each piece of data, and its parallel calculation is implemented based on a directed topology graph."
    },
    {
      "name": "Topology",
      "definition": "Defines the logical model (or abstract model) of parallel computing, that is, designs the calculation steps and processes from the perspective of function and architecture.",
      "text_evidence": "Topology Defines the logical model (or abstract model) of parallel computing, that is, designs the calculation steps and processes from the perspective of function and architecture."
    },
    {
      "name": "Spout",
      "definition": "Data source in Storm's topology.",
      "text_evidence": "Topology composed of Spout (data source) and Bolt (processing unit)."
    },
    {
      "name": "Bolt",
      "definition": "Processing unit in Storm's topology.",
      "text_evidence": "Topology composed of Spout (data source) and Bolt (processing unit)."
    },
    {
      "name": "Nimbus",
      "definition": "A daemon runs on the master node, like Hadoop's JobTracker, responsible for task distribution and fault monitoring of the cluster.",
      "text_evidence": "A Nimbus daemon runs on the master node, like Hadoop's JobTracker, responsible for task distribution and fault monitoring of the cluster."
    },
    {
      "name": "Supervisor",
      "definition": "Each worker node runs a Supervisor daemon, monitors the status of the local node, and starts and shuts down the worker process of the node when necessary according to Nimbus instructions.",
      "text_evidence": "Each worker node runs a Supervisor daemon, monitors the status of the local node, and starts and shuts down the worker process of the node when necessary according to Nimbus instructions."
    },
    {
      "name": "Micro-batch processing",
      "definition": "The practice of collecting data in small groups (“batches”) for the purposes of taking action on (processing) that data.",
      "text_evidence": "Micro-batch processing is the practice of collecting data in small groups (“batches”) for the purposes of taking action on (processing) that data."
    },
    {
      "name": "Directed Acyclic Graph (DAG)",
      "definition": "Commonly used in distributed systems to characterize the calculation process or calculation model.",
      "text_evidence": "Directed Acyclic Graph (DAG, Directed Acyclic Graph) is commonly used in distributed systems to characterize the calculation process or calculation model."
    }
  ],
  "original_text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology,\nin this session, we discuss Stream computing model.\nThe data processing system provides big data computing and processing capabilities and an application development platform.\nFrom the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing platform layer, computing engine layer, etc.\nComputing models are the way that different kinds of big data is processed in different scenarios,\nwhich include batch processing, stream computing, Large-scale concurrent processing (MPP) model for structured data, In-memory Computing model, and Data Flow Graph models.\nNow let’s look at the stream computing model represented by Storm\nStream computing is a computing model that processes real-time dynamic data.\nThe traditional enterprise database stores historical data, that is, static data, that is, all data must be entered into the database before calculation and processing.\nTechnicians can query and update the database, and use data mining and OLAP analysis tools to extract static data from the database. Find valuable information to support business decision analysis.\nHowever, in Internet applications (user web click tracking, online real-time recommendation systems, etc.), intelligent transportation systems, wireless sensor network monitoring and other fields, its data generation methods and data characteristics have the following characteristics and calculation requirements\n1）Data is no longer arriving in batches but continuously arriving dynamically\n2）Computational analysis requires real-time, fast response, and low latency\n3）The amount of data is large, but the storage of the data is not valued, but the immediate processing and analysis of the data are emphasized\n4）Pay attention to the calculation and analysis results of the data as a whole, but not to pay attention to the individual data\n5）The order and timing of the arrival of data elements cannot be predicted or controlled, and the calculation program must be able to respond\nMapReduce performs offline batch calculations for static data that has entered the database, and the calculation results are also stored in the static database;\nwhile stream computing is real-time analysis and calculation for dynamic continuous data streams.\nAfter the calculation results are obtained, the data is either imported into the static database, either discard, that is, one-time use.\nTo support this data flow calculation mode, the flow calculation framework generally includes three steps: real-time data collection, real-time data calculation, real-time data query service\nDirected Acyclic Graph (DAG, Directed Acyclic Graph) is commonly used in distributed systems to characterize the calculation process or calculation model.\nThe figure shows the combination of chained tasks in a distributed system.\nThe nodes in different colors in the figure represent computing tasks (or computing objects) at different stages.\nThe one-way arrow indicates the order of the calculation steps and the dependencies.\nStorm is a Native Stream Processing System, that is, the processing of stream data is based on each piece of data, and its parallel calculation is implemented based on a directed topology graph.\nTopology composed of Spout (data source) and Bolt (processing unit).\nTopology Defines the logical model (or abstract model) of parallel computing, that is, designs the calculation steps and processes from the perspective of function and architecture.\nThe out put of a spout is a series of tuples stream, and the input and output of Bolt is also is a series of tuples stream.\nStorm's computing system also adopts a master-slave (Master/Slave) architecture. There are mainly two types of nodes: master node and slave node.\nA Nimbus daemon runs on the master node, like Hadoop's JobTracker, responsible for task distribution and fault monitoring of the cluster.\nNimbus manages many worker nodes through a group of Zookeeper\nEach worker node runs a Supervisor daemon, monitors the status of the local node, and starts and shuts down the worker process of the node when necessary according to Nimbus instructions.\nThere are two ways of stream computing, Native Stream Processing System represented by storm and Micro-batch Stream Processing System represented by spark\nMicro-batch processing is the practice of collecting data in small groups (“batches”) for the purposes of taking action on (processing) that data.\nCompared with Native Stream Processing System, Micro-batch processing is not really Realtime processing , but nearly Realtime processing , it can save the computing cost because it wait until the micro batch composed.\nContrast this to traditional “batch processing,” which often implies taking action on a large group of data.\nMicro-batch processing is a variant of traditional batch processing in that the data processing occurs more frequently so that smaller groups of new data are processed. In both micro-batch processing and traditional batch processing, data is collected based on a predetermined threshold or frequency before any processing occurs.\nDuring the real application, sometimes the Batch computing and Stream computing combined to finish the history records analysis and Realtime analysis like showed in the diagram.\nKafka broker take the data form the data producer, and deliver them into Stream computing storm cluster and the Batch computing Hadoop cluster, the processing output of storm can be stored in NoSQL Cassandra DB The output of Batch processing can be stored in Hbase.\nAfter the data has been processed, the data analytics like virtualization, decision making, Prediction, OLAP and recommendation can be done based on the result of Batch processing and Stream processing.\nIn this session we learned stream computing represented by Storm.\nThank you for your attention, if you have any question, feel free to contact me."
}