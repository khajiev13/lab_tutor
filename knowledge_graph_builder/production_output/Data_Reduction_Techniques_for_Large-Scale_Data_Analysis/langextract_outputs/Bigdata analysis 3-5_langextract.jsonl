{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "Data Reduction Techniques for Large-Scale Data Analysis", "char_interval": {"start_pos": 21, "end_pos": 35}, "alignment_status": "match_lesser", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "Data reduction techniques are used to create a condensed, representative dataset from a large original dataset to improve the efficiency of data analysis while maintaining the integrity of the results. The time spent on reduction should not outweigh the time saved during analysis. Data reduction is broadly categorized into dimensionality reduction and numerosity reduction. Dimensionality reduction aims to remove irrelevant attributes through methods like attribute subset selection (which includes forward selection, backward deletion, and combined approaches), decision tree induction, and statistical analysis. Data compression, another dimensionality reduction technique, can be lossless (fully reversible) or lossy (approximate reconstruction), with Principal Component Analysis (PCA) being a common lossy method. Numerosity reduction uses smaller representations of data, employing parametric or non-parametric methods such as data cube aggregation, clustering, sampling, and histograms. Other techniques include discretization, which reduces continuous attribute values into intervals, and concept hierarchy generation, which replaces low-level concepts with higher-level ones to generalize the data, making it more meaningful and easier to analyze.", "char_interval": {"start_pos": 328, "end_pos": 342}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "Data reduction, dimensionality reduction, numerosity reduction, attribute subset selection, Principal Component Analysis (PCA), data compression, data cube aggregation, discretization, concept hierarchy", "char_interval": {"start_pos": 1321, "end_pos": 5518}, "alignment_status": "match_fuzzy", "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Data Reduction", "char_interval": {"start_pos": 4650, "end_pos": 4654}, "alignment_status": "match_lesser", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "A technology used to obtain a condensed data set from a huge original data set, which maintains the integrity of the original data, allowing for more efficient analysis with basically the same results."}}, {"extraction_class": "CONCEPT", "extraction_text": "Dimensionality Reduction", "char_interval": {"start_pos": 1039, "end_pos": 1063}, "alignment_status": "match_fuzzy", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "A data reduction technique that removes irrelevant attributes to reduce the amount of data processed by data analysis."}}, {"extraction_class": "CONCEPT", "extraction_text": "Numerosity Reduction", "char_interval": {"start_pos": 1068, "end_pos": 1088}, "alignment_status": "match_fuzzy", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "A data reduction technique that uses smaller data representations, shorter data units, or data models to reduce the amount of data."}}, {"extraction_class": "CONCEPT", "extraction_text": "Attribute Subset Selection", "char_interval": {"start_pos": 1128, "end_pos": 1154}, "alignment_status": "match_fuzzy", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "The goal is to find the smallest attribute subset that ensures the probability distribution of the new data subset is as close as possible to that of the original data set."}}, {"extraction_class": "CONCEPT", "extraction_text": "Step forward Attributes subset selection", "char_interval": {"start_pos": 1945, "end_pos": 1985}, "alignment_status": "match_fuzzy", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "An approach starting with an empty attribute set, where the current optimal attribute is progressively added from the original set until a stopping condition is met."}}, {"extraction_class": "CONCEPT", "extraction_text": "Remove gradually backward Attributes subset selection", "char_interval": {"start_pos": 2285, "end_pos": 2338}, "alignment_status": "match_fuzzy", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "An approach starting with a full attribute set, where the current worst attribute is progressively eliminated until a stopping condition is met."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Compression", "char_interval": {"start_pos": 4714, "end_pos": 4718}, "alignment_status": "match_lesser", "extraction_index": 10, "group_index": 9, "description": null, "attributes": {"definition": "The use of data encoding or transformation to obtain a compressed representation of the original data."}}, {"extraction_class": "CONCEPT", "extraction_text": "Lossless Compression", "char_interval": {"start_pos": 3227, "end_pos": 3247}, "alignment_status": "match_fuzzy", "extraction_index": 11, "group_index": 10, "description": null, "attributes": {"definition": "A compression method where the compressed data can be restored without losing any information."}}, {"extraction_class": "CONCEPT", "extraction_text": "Lossy Compression", "char_interval": {"start_pos": 3252, "end_pos": 3269}, "alignment_status": "match_fuzzy", "extraction_index": 12, "group_index": 11, "description": null, "attributes": {"definition": "A compression method where only an approximate representation of the original data can be reconstructed."}}, {"extraction_class": "CONCEPT", "extraction_text": "Principal Component Analysis (PCA)", "char_interval": {"start_pos": 3883, "end_pos": 3917}, "alignment_status": "match_fuzzy", "extraction_index": 13, "group_index": 12, "description": null, "attributes": {"definition": "A data compression method that finds a set of orthogonal vectors (principal components) that best represent the data, allowing the original data to be projected into a smaller space."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Cube Aggregation", "char_interval": {"start_pos": 4787, "end_pos": 4808}, "alignment_status": "match_exact", "extraction_index": 14, "group_index": 13, "description": null, "attributes": {"definition": "A numerosity reduction technique that gathers n-dimensional data cubes into (n-1) dimensional data cubes."}}, {"extraction_class": "CONCEPT", "extraction_text": "Discretization", "char_interval": {"start_pos": 5094, "end_pos": 5108}, "alignment_status": "match_exact", "extraction_index": 15, "group_index": 14, "description": null, "attributes": {"definition": "A technique to reduce the number of values of a continuous attribute by dividing its range into several intervals."}}, {"extraction_class": "CONCEPT", "extraction_text": "Concept Hierarchy", "char_interval": {"start_pos": 5501, "end_pos": 5518}, "alignment_status": "match_exact", "extraction_index": 16, "group_index": 15, "description": null, "attributes": {"definition": "A set of mappings from low-level concept sets to high-level concept sets, allowing data to be processed at various levels of abstraction for reduction."}}], "text": "Bigdata analysis 3-5 data reduction\nHello everyone, I am Haiying Che, from  Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology , in this session , we will discuss  Data reduction .\nComplex data analysis of large-scale database content usually takes a lot of time.\nData reduction (subtraction) technology is used to help obtain a condensed data set from the original huge data set, and make this condensed data set maintain the integrity of the original data set, so that data analysis on the condensed data set is obviously efficient higher, and the results of analysis are basically the same as those obtained by using the original data set.\nData reduction work should satisfy some standard. \nThe time spent on data reduction should not exceed or \"offset\" the time saved by analysis on the reduced data\nThe data obtained by the reduction is much smaller than the original data, but can produce the same or almost the same analysis results\nData reduction technology includes dimensionality reduction and numerosity reduction.\nAnd dimensionality reduction includes attribute subset selection, principal Component analysis and wavelet transform\nNumerosity reduction could be divided into parametric and non-parametric methods. \nnon-parametric methods include data cube aggregation , clustering, sampling histogram etc.\nFirst let’ s look at Dimension reduction\nRemove irrelevant attributes and reduce the amount of data processed by data analysis.\nAttributes subset selection Goal is to Find the smallest attribute subset and ensure that the probability distribution of the new data subset is as close as possible to the probability distribution of the original data set\nFor example, when digging into the classification rules of whether a customer will buy a player in a shopping mall, the customer's phone number is likely to be irrelevant to the mining task and should be removed.\nStep forward Attributes subset selection\nStarting from an empty attribute set (as the initial value of the attribute subset), each time a current optimal attribute is selected from the original attribute set and added to the current attribute subset. Until the optimal attribute cannot be selected or a certain threshold constraint is met.\nRemove gradually backward Attributes subset selection\nStarting from a full attribute set (as the initial value of the attribute subset), each time a current worst attribute is selected from the current attribute subset and eliminated from the current attribute subset. Until the worst attribute cannot be selected or a certain threshold constraint is met.\nor Combine forward selection and backward deletion\nDecision tree (decision tree) induction\nUse the decision tree induction method to classify and induct the initial data to obtain an initial decision tree. \nAll attributes that do not appear on the decision tree are considered irrelevant attributes. \nTherefore, delete these attributes from the initial attribute set to obtain an initial decision tree. A better subset of attributes.\nThere is also Reduction based on statistical analysis\nNow let’s look at Data reduction by Data compression\nCompression algorithm can be classified into Lossless compression and Lossy compression .\nLossless compression: Compressed data can be restored without losing any information.\nFor example: string compression\nHave a broad theoretical foundation and sophisticated algorithms\nLossy compression: Only an approximate representation of the original data can be reconstructed.\nFor example: audio/video compression\nSometimes it is possible to reconstruct a fragment without decompressing the overall data\nData compression-use data encoding or transformation to obtain a compressed representation of the original data.\nThe two data compression methods commonly used in the field of data mining are both lossy:\nPrincipal component analysis (PCA) assumes that the data to be compressed consists of N tuples or data vectors taken from k dimensions. Principal component analysis and search to obtain c “k-dimensional” orthogonal vectors that best represent the data, where c≤k. In this way, the original data can be projected into a smaller space to achieve data compression\nWe talked some technologies of dimensionality reduction and now let’s look at some technologies of numerosity reduction.\nNumerosity reduction -use smaller data to represent data, or use shorter data units, or use data models to represent data to reduce the amount of data.\nNumerosity reduction includes Data cube aggregation , clustering, sampling, histogram etc.\nLet’s look at the data cube aggregation\nA data cube is a multi-dimensional modeling and representation of data, composed of dimensions and facts.\nDimension: attribute\nFacts: data\nData cube aggregation definition-gather n-dimensional data cubes into n-1 dimensional data cubes.\nUsing Data cube Aggregates for approximate query\nIn the diagram, 3 dimension data cube aggregate into 2 dimension, the men and women ’ sale amount aggregated together. \nNow let’s look at Data reduction method Discretization.\nAttribute values could be Name type-e.g. value in an unordered set, Ordinal-e.g. value in an ordered set and Continuous value-e.g. real number.\nDiscretization technology\nReduce the number of values of a continuous (value) attribute by dividing the range of the attribute (continuous value) domain value into several intervals.\nUsing Concept hierarchical generation to do the Data reduction \nConcept hierarchy defines a set of mappings from low-level concept sets to high-level concept sets. \nIt allows data to be processed at various levels of abstraction, thereby discovering knowledge at multiple levels of abstraction.\nUse higher-level concepts to replace lower-level concepts (such as the value of age in this example) to reduce the number of values. \nAlthough some details disappeared in the process of data generalization,\nBut the generalized data obtained in this way may be easier to understand and more meaningful. \nData analysis on the reduced data set is obviously more efficient.\nThe concept hierarchy can be represented by a tree, and each node of the tree represents a concept of certain level.\nIn this session we learned Data reduction and related technologies.\nThank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_14649432"}
