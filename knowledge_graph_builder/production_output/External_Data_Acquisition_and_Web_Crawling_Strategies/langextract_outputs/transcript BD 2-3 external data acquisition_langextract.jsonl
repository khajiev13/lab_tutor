{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "External Data Acquisition and Web Crawling Strategies", "char_interval": {"start_pos": 204, "end_pos": 229}, "alignment_status": "match_lesser", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "External data, primarily network big data from the internet, is collected using web crawlers. This data is characterized by being multi-source, heterogeneous, timely, social, interactive, sudden, and having high noise. A web crawler is a program that automatically browses the internet, starting with a list of seed URLs. It visits these URLs, identifies links, adds them to a queue, downloads the pages, and extracts new URLs to continue the process until the queue is empty. To manage the order of crawling, especially with numerous fan-out links, web crawlers employ various strategies. Common strategies include Depth-first, which explores as deeply as possible along one branch before backtracking, and Breadth-first, which explores all links at the current level before moving deeper. More advanced strategies include PageRank, which ranks pages based on the quantity and quality of incoming links, and OPIC (Online Page Importance Computation), which distributes 'gold coins' from a downloaded page to its linked pages to determine priority. Since crawling the entire internet is a massive task, distributed architectures are used. These include the master-slave model, the peer-to-peer model, and a mixed structure, each with different approaches to collaboration and scalability.", "char_interval": {"start_pos": 231, "end_pos": 244}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "External Data Acquisition, Web Crawler, Crawling Strategy, Network Big Data, PageRank, OPIC, Depth-first, Breadth-first, Distributed Architecture, Seed URL", "char_interval": null, "alignment_status": null, "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "External Data", "char_interval": {"start_pos": 204, "end_pos": 217}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "Mainly Network big data, which is the data available on the internet."}}, {"extraction_class": "CONCEPT", "extraction_text": "Web Crawler", "char_interval": {"start_pos": 364, "end_pos": 376}, "alignment_status": "match_fuzzy", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "A program or web robot that automatically browses the Internet and gets data, starting with a list of seed URLs."}}, {"extraction_class": "CONCEPT", "extraction_text": "Web Crawler Crawling Strategy", "char_interval": {"start_pos": 2553, "end_pos": 2573}, "alignment_status": "match_lesser", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "A method that decides which links should be crawled first and determines the crawling priority and sequence."}}, {"extraction_class": "CONCEPT", "extraction_text": "Depth-first Strategy", "char_interval": {"start_pos": 2634, "end_pos": 2639}, "alignment_status": "match_lesser", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "A crawling strategy that follows a path of links as deep as possible before backtracking to explore other branches."}}, {"extraction_class": "CONCEPT", "extraction_text": "Breadth-first Strategy", "char_interval": {"start_pos": 2646, "end_pos": 2653}, "alignment_status": "match_lesser", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "A crawling strategy that explores all links at the current level before proceeding to the links on the next level."}}, {"extraction_class": "CONCEPT", "extraction_text": "PageRank", "char_interval": {"start_pos": 2668, "end_pos": 2676}, "alignment_status": "match_exact", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "A technology that expresses each web page's importance by counting the number and importance of links pointing to it from other webpages, thereby ranking them."}}, {"extraction_class": "CONCEPT", "extraction_text": "OPIC (Online Page Importance Computation)", "char_interval": {"start_pos": 2686, "end_pos": 2727}, "alignment_status": "match_exact", "extraction_index": 10, "group_index": 9, "description": null, "attributes": {"definition": "A crawling strategy where each page is assigned 'gold coins'. When a page is downloaded, its coins are equally distributed to the pages it links to, and the crawl queue is sorted by the number of coins."}}, {"extraction_class": "CONCEPT", "extraction_text": "Distributed Architecture Model", "char_interval": {"start_pos": 4370, "end_pos": 4400}, "alignment_status": "match_exact", "extraction_index": 11, "group_index": 10, "description": null, "attributes": {"definition": "A model that uses the effective collaboration and cooperation of multiple stand-alone crawler systems to capture internet big data."}}, {"extraction_class": "CONCEPT", "extraction_text": "Master-Slave Architecture", "char_interval": {"start_pos": 4452, "end_pos": 4465}, "alignment_status": "match_lesser", "extraction_index": 12, "group_index": 11, "description": null, "attributes": {"definition": "A distributed crawler model where a master node manages slave nodes, but the master can become a bottleneck and the number of slaves is limited."}}, {"extraction_class": "CONCEPT", "extraction_text": "Peer-to-Peer Architecture", "char_interval": {"start_pos": 4467, "end_pos": 4471}, "alignment_status": "match_lesser", "extraction_index": 13, "group_index": 12, "description": null, "attributes": {"definition": "A distributed crawler model with no master node, which avoids single machine hot spots but requires all nodes to update each other upon expansion."}}, {"extraction_class": "CONCEPT", "extraction_text": "Mixed Structure", "char_interval": {"start_pos": 4484, "end_pos": 4499}, "alignment_status": "match_exact", "extraction_index": 14, "group_index": 13, "description": null, "attributes": {"definition": "A distributed crawler model where masters communicate with other masters, allowing for a smaller number of slave nodes per master."}}], "text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session, we will discuss\nSome idea about External Data Acquisition.\nExternal data is mainly Network big data, which is the data available in the internet, \nand the way to collect network data is using web crawlers.\nIn one URL, there may be many links and the sub-links of the upper level links, so which links should be crawled first and the crawling priority and sequences decided by the Web crawler crawling strategy.\nAnd in this session, we will discuss the network big data, web crawler and its strategy.3\nNetwork big data characteristics include:\n)Multi-source and heterogeneous, which means the internet data comes from Multi-sources, and the different source has different data. \n) Timeliness which means when something happens, it can be published immediately.\n) Sociality, the network big data directly reflects the social status\n) Interactivity: WeChat Weibo, Facebook, twitter, etc. Internet users can not only publish information according to their needs, but also reply and forward information according to their personal preferences. \n) Sudden: Some news dissemination will cause a large amount of new network data to be generated in a short time, \nreflecting the suddenness of network big data and network groups\n) High noise characteristic is easy to understand, the internet data cannot be 100% true and useful, no one is responsible for the quality of internet data, so value density is low, full of dirty data, when you want use the internet data, you must clean it.\nLike showed in the diagram, A web crawler is a program or web robot that automatically browses the Internet and get data.\n)Web crawler crawling process Starts with a list of uniform resource addresses called seed URL and use it as the link entry for crawling. When the crawler visits these seed URL s, it identifies all the needed links on the page and adds them to the queue to be crawled. \n) After that, the webpage links are taken out from the queue to be crawled, then Read URL, do the DNS resolution, and web pages were download into the Downloaded web library.\n) Put the already downloaded URL into the crawled URL list\n) Extract the new URL into the URL queue to be crawled and put them in the to be crawled URL queue according to strategy\n) all the process will end until the queue for crawling is empty.\nHow to deal with fan-out URLs in seed URLs, which is the links of the link, which involves web crawler crawling strategies\nThe most often used Crawling strategies include \nDepth first\nBreadth first\nPartial PageRank Strategy\nOPIC (online Page Importance Computation)\nLet’ s explains them one by one\nHere is a example of Fan out URLs structure, and if use depth first strategy, \nthe sequence should be M1-M2-M5-M8-M6-M3-S7-S4;\nAnd if use Breadth first strategy, the sequence should be M1-M2-M3-S4-M5-M6-S7-M8;\nPageRank, also known as Page Ranking and Google Ranking is a technology applied by search engines based on mutual hyperlinks between web pages，\nInvented by Google founder Larry Page (Larry Page). \nThe PageRank link analysis algorithm expresses each page importance by counting the number and the importance of links pointed to by other webpages, thereby realizing the ranking of the importance of each webpage.\nThis algorithm takes the Quantity and quality into consideration. \nFor example, Links number of webpage E are far more than links to webpage C, but webpage C is much more important than webpage E. \nbecause page C is linked by page B, and page B is of high importance.\nUse the principle of PageRank to calculate the importance value in the URL list and sort the crawled web pages in order, and then traverse each URL in this order.\nThe OPIC strategy assigns the same \"gold coins\" to each web page. Whenever a page P is downloaded, the \"gold coins\" owned by P are equally distributed to the linked pages contained in the web page. \nThe links in the queue to be crawled are sorted by \"gold coins\"\nOPIC calculation speed is faster than partial PageRank strategy\nNow we know the crawling task is huge, it cant easily be done by one stand-alone crawler , \nThrough the effective collaboration and cooperation of multiple stand-alone crawler systems to achieve Internet big data capture.\nwhich is Distributed Architecture model.\nThere are 3 basic Distributed Architecture model, master- slave, peer to peer and mixed structure.\nFor master- slave, the master needs to be updated when expanding, \nthe master node is under heavy pressure and easily becomes a bottleneck, and the number of slave nodes is limited.\nFor peer to peer, all nodes need to update all other nodes when communication is expanded. \nNo master, no single machine hot spots, and the number of slave nodes is limited\nFor mixed structure, the master communicates with the master, and the number of slave nodes could be small.\nLet’s summarize the external data and acquisition.\nWe studied the Web crawler crawling process, Web crawler crawling strategy, \nincluding Depth first, Breadth first, Page Rank, OPIC (online Page Importance Computation).\nAnd we learned the Distributed web crawler structure, master- slave, peer to peer and mixed structure.\nIn this session we learned and external data acquisition. thank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_559ac75b"}
