{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "Stream Computing Models and the Storm Framework", "char_interval": {"start_pos": 2351, "end_pos": 3207}, "alignment_status": "match_fuzzy", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "This session introduces stream computing as a model for processing real-time, dynamic data, contrasting it with traditional batch processing like MapReduce which handles static, historical data. Stream computing is characterized by continuous data arrival, the need for low-latency responses, and a focus on immediate analysis rather than storage. The process typically involves three steps: real-time data collection, computation, and query services. The lecture details the Storm framework, a native stream processing system that uses a Directed Acyclic Graph (DAG) topology composed of Spouts (data sources) and Bolts (processing units) to define its parallel computation logic. Storm's master-slave architecture consists of a Nimbus master node for task distribution and Supervisor daemons on worker nodes. The session also compares native stream processing with micro-batch processing, a near-real-time approach that processes data in small batches. Finally, it describes hybrid architectures where stream computing (Storm) and batch computing (Hadoop) are combined, often using Kafka for data ingestion and NoSQL databases like Cassandra or HBase for storing processed results, which then feed into analytics applications.", "char_interval": {"start_pos": 159, "end_pos": 171}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "Stream Computing, Storm, Batch Processing, MapReduce, Directed Acyclic Graph (DAG), Spout, Bolt, Micro-batch processing, Nimbus, Real-time processing", "char_interval": null, "alignment_status": null, "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Stream Computing", "char_interval": {"start_pos": 184, "end_pos": 200}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "A computing model that processes real-time dynamic data, as opposed to traditional databases that store and process static, historical data."}}, {"extraction_class": "CONCEPT", "extraction_text": "MapReduce", "char_interval": {"start_pos": 2183, "end_pos": 2192}, "alignment_status": "match_fuzzy", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "A model that performs offline batch calculations for static data that has already been stored in a database."}}, {"extraction_class": "CONCEPT", "extraction_text": "Directed Acyclic Graph (DAG)", "char_interval": {"start_pos": 2772, "end_pos": 2824}, "alignment_status": "match_fuzzy", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "A model commonly used in distributed systems to characterize the calculation process, where nodes represent computing tasks and one-way arrows indicate the order and dependencies of calculation steps."}}, {"extraction_class": "CONCEPT", "extraction_text": "Storm", "char_interval": {"start_pos": 3202, "end_pos": 3207}, "alignment_status": "match_exact", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "A Native Stream Processing System where the processing of stream data is based on each individual piece of data, and its parallel calculation is implemented based on a directed topology graph."}}, {"extraction_class": "CONCEPT", "extraction_text": "Topology", "char_interval": {"start_pos": 3381, "end_pos": 3389}, "alignment_status": "match_fuzzy", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "In Storm, it defines the logical model or abstract model of parallel computing, composed of Spouts (data sources) and Bolts (processing units)."}}, {"extraction_class": "CONCEPT", "extraction_text": "Spout", "char_interval": {"start_pos": 3418, "end_pos": 3423}, "alignment_status": "match_fuzzy", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "The data source in a Storm topology, which outputs a series of tuples stream."}}, {"extraction_class": "CONCEPT", "extraction_text": "Bolt", "char_interval": {"start_pos": 3442, "end_pos": 3446}, "alignment_status": "match_fuzzy", "extraction_index": 10, "group_index": 9, "description": null, "attributes": {"definition": "A processing unit in a Storm topology whose input and output are a series of tuples stream."}}, {"extraction_class": "CONCEPT", "extraction_text": "Nimbus", "char_interval": {"start_pos": 3923, "end_pos": 3929}, "alignment_status": "match_fuzzy", "extraction_index": 11, "group_index": 10, "description": null, "attributes": {"definition": "A daemon that runs on the master node in a Storm cluster, responsible for task distribution and fault monitoring, similar to Hadoop's JobTracker."}}, {"extraction_class": "CONCEPT", "extraction_text": "Supervisor", "char_interval": {"start_pos": 4148, "end_pos": 4158}, "alignment_status": "match_fuzzy", "extraction_index": 12, "group_index": 11, "description": null, "attributes": {"definition": "A daemon that runs on each worker node in a Storm cluster, which monitors the status of the local node and starts or shuts down worker processes based on instructions from Nimbus."}}, {"extraction_class": "CONCEPT", "extraction_text": "Native Stream Processing System", "char_interval": {"start_pos": 3213, "end_pos": 3244}, "alignment_status": "match_exact", "extraction_index": 13, "group_index": 12, "description": null, "attributes": {"definition": "A stream computing approach, represented by Storm, where the processing of stream data is based on each individual piece of data."}}, {"extraction_class": "CONCEPT", "extraction_text": "Micro-batch Stream Processing System", "char_interval": {"start_pos": 4409, "end_pos": 4445}, "alignment_status": "match_exact", "extraction_index": 14, "group_index": 13, "description": null, "attributes": {"definition": "The practice of collecting data in small groups ('micro-batches') for processing. It is considered nearly real-time, not truly real-time, and can save computing costs."}}], "text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, \nin this session, we discuss Stream computing model.\nThe data processing system provides big data computing and processing capabilities and an application development platform.\nFrom the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing platform layer, computing engine layer, etc.\nComputing models are the way that different kinds of big data is processed in different scenarios, \nwhich include batch processing, stream computing, Large-scale concurrent processing (MPP) model for structured data, In-memory Computing model, and Data Flow Graph models.\nNow let’s look at the stream computing model represented by Storm\nStream computing is a computing model that processes real-time dynamic data. \nThe traditional enterprise database stores historical data, that is, static data, that is, all data must be entered into the database before calculation and processing. \nTechnicians can query and update the database, and use data mining and OLAP analysis tools to extract static data from the database. Find valuable information to support business decision analysis.\nHowever, in Internet applications (user web click tracking, online real-time recommendation systems, etc.), intelligent transportation systems, wireless sensor network monitoring and other fields, its data generation methods and data characteristics have the following characteristics and calculation requirements\n）Data is no longer arriving in batches but continuously arriving dynamically\n）Computational analysis requires real-time, fast response, and low latency\n）The amount of data is large, but the storage of the data is not valued, but the immediate processing and analysis of the data are emphasized\n）Pay attention to the calculation and analysis results of the data as a whole, but not to pay attention to the individual data\n）The order and timing of the arrival of data elements cannot be predicted or controlled, and the calculation program must be able to respond\nMapReduce performs offline batch calculations for static data that has entered the database, and the calculation results are also stored in the static database; \nwhile stream computing is real-time analysis and calculation for dynamic continuous data streams. \nAfter the calculation results are obtained, the data is either imported into the static database, either discard, that is, one-time use.\nTo support this data flow calculation mode, the flow calculation framework generally includes three steps: real-time data collection, real-time data calculation, real-time data query service\nDirected Acyclic Graph (DAG, Directed Acyclic Graph) is commonly used in distributed systems to characterize the calculation process or calculation model.\nThe figure shows the combination of chained tasks in a distributed system. \nThe nodes in different colors in the figure represent computing tasks (or computing objects) at different stages.\nThe one-way arrow indicates the order of the calculation steps and the dependencies.\nStorm is a Native Stream Processing System, that is, the processing of stream data is based on each piece of data, and its parallel calculation is implemented based on a directed topology graph.\nTopology composed of Spout (data source) and Bolt (processing unit).\nTopology Defines the logical model (or abstract model) of parallel computing, that is, designs the calculation steps and processes from the perspective of function and architecture.\nThe out put of a spout is a series of tuples stream, and the input and output of Bolt is also is a series of tuples stream.\nStorm's computing system also adopts a master-slave (Master/Slave) architecture. There are mainly two types of nodes: master node and slave node.   \nA Nimbus daemon runs on the master node, like Hadoop's JobTracker, responsible for task distribution and fault monitoring of the cluster.\nNimbus manages many worker nodes through a group of Zookeeper   \nEach worker node runs a Supervisor daemon, monitors the status of the local node, and starts and shuts down the worker process of the node when necessary according to Nimbus instructions.\nThere are two ways of stream computing, Native Stream Processing System represented by storm and Micro-batch Stream Processing System represented by spark\nMicro-batch processing is the practice of collecting data in small groups (“batches”) for the purposes of taking action on (processing) that data.\nCompared with Native Stream Processing System, Micro-batch processing is not really Realtime processing , but nearly Realtime processing , it can save the computing cost because it wait until the micro batch composed.\nContrast this to traditional “batch processing,” which often implies taking action on a large group of data. \nMicro-batch processing is a variant of traditional batch processing in that the data processing occurs more frequently so that smaller groups of new data are processed. In both micro-batch processing and traditional batch processing, data is collected based on a predetermined threshold or frequency before any processing occurs.\nDuring the real application, sometimes the Batch computing and Stream computing combined to finish the history records analysis and Realtime analysis like showed in the diagram.\nKafka broker take the data form the data producer, and deliver them into Stream computing storm cluster and the Batch computing Hadoop cluster, the processing output of storm can be stored in NoSQL Cassandra DB The output of Batch processing can be stored in Hbase.\nAfter the data has been processed, the data analytics like virtualization, decision making, Prediction, OLAP and recommendation can be done based on the result of Batch processing and Stream processing. \nIn this session we learned stream computing represented by Storm.\nThank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_fc237100"}
