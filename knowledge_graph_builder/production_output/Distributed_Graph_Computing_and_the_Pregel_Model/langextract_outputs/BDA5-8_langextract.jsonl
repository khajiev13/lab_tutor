{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "Distributed Graph Computing and the Pregel Model", "char_interval": {"start_pos": 182, "end_pos": 209}, "alignment_status": "match_lesser", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "Distributed Graph Computing is a model for processing large-scale graphs that exceed the capacity of a single node. This is achieved through graph parallel computing systems like Google's Pregel, Apache Giraph, and GraphLab. The Pregel model, which inspired Giraph and Spark's GraphX, operates on a vertex-centric approach using the Bulk Synchronous Parallel (BSP) model. Computation proceeds in a series of synchronized 'supersteps'. In each superstep, active vertices receive messages from the previous step, execute a user-defined function to modify their state or graph topology, and send messages to other vertices, which will be delivered in the next superstep. The architecture consists of a master node that coordinates workers. The master partitions the graph and assigns these partitions to worker machines. Workers execute the computation and handle message passing. The system ensures fault tolerance through periodic checkpointing, where workers save their state to persistent storage. Pregel includes optimizations like 'combiners' to reduce network traffic by merging messages and 'aggregators' to compute global values from vertex data. This model is better suited for iterative graph algorithms than MapReduce, which incurs high overhead from passing the entire graph state between stages and lacks a native iterative model. However, Pregel can face limitations from network bottlenecks in dense graphs and synchronization delays caused by the superstep barrier.", "char_interval": null, "alignment_status": null, "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "Distributed Graph Computing, Pregel, Bulk Synchronous Parallel (BSP), Superstep, Message Passing, Master-Worker Architecture, Apache Giraph, Fault Tolerance, MapReduce, Graph Partitioning", "char_interval": null, "alignment_status": null, "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Distributed Graph Computing", "char_interval": {"start_pos": 182, "end_pos": 209}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "A computing model designed to process large-scale graphs that are too big to fit on a single node, utilizing graph parallel computing techniques."}}, {"extraction_class": "CONCEPT", "extraction_text": "Pregel", "char_interval": {"start_pos": 937, "end_pos": 943}, "alignment_status": "match_fuzzy", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "A system developed by Google for large-scale graph processing based on the Bulk Synchronous Parallel model. It serves as a generalized parallel graph transformation framework and has inspired systems like Apache Giraph and Apache Spark’s GraphX."}}, {"extraction_class": "CONCEPT", "extraction_text": "Bulk Synchronous Parallel (BSP) model", "char_interval": {"start_pos": 2045, "end_pos": 3969}, "alignment_status": "match_fuzzy", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "A model of distributed computation where multiple processing units proceed in parallel through a sequence of 'supersteps'. In each superstep, units receive messages, perform local computations, and queue outgoing messages, with a synchronization point occurring before the next superstep begins."}}, {"extraction_class": "CONCEPT", "extraction_text": "Superstep", "char_interval": {"start_pos": 3670, "end_pos": 3679}, "alignment_status": "match_fuzzy", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "A single iteration in the BSP model where each processing unit receives messages from the previous step, manipulates its local data, and sends messages to other units that will be delivered in the next iteration."}}, {"extraction_class": "CONCEPT", "extraction_text": "Apache Giraph", "char_interval": {"start_pos": 966, "end_pos": 979}, "alignment_status": "match_fuzzy", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "An iterative, highly scalable graph processing system inspired by Google's Pregel, used at Facebook to analyze its social graph."}}, {"extraction_class": "CONCEPT", "extraction_text": "GraphLab", "char_interval": {"start_pos": 1324, "end_pos": 1332}, "alignment_status": "match_fuzzy", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "An open-source project originally developed for Machine Learning tasks that has proven effective for a broad range of data-mining tasks on graphs."}}, {"extraction_class": "CONCEPT", "extraction_text": "Master", "char_interval": {"start_pos": 2205, "end_pos": 2211}, "alignment_status": "match_fuzzy", "extraction_index": 10, "group_index": 9, "description": null, "attributes": {"definition": "A central coordinating machine in the Pregel architecture that partitions the graph, assigns partitions to workers, signals the start of each superstep, and manages fault tolerance through checkpointing."}}, {"extraction_class": "CONCEPT", "extraction_text": "Worker", "char_interval": {"start_pos": 3441, "end_pos": 3447}, "alignment_status": "match_fuzzy", "extraction_index": 11, "group_index": 10, "description": null, "attributes": {"definition": "A machine in the Pregel architecture that hosts one or more graph partitions and executes a user-defined compute function on the vertices within those partitions."}}, {"extraction_class": "CONCEPT", "extraction_text": "Partition", "char_interval": {"start_pos": 3293, "end_pos": 3303}, "alignment_status": "match_fuzzy", "extraction_index": 12, "group_index": 11, "description": null, "attributes": {"definition": "A unit of execution in Pregel that contains a large number of graph nodes. A worker machine can host multiple partitions."}}, {"extraction_class": "CONCEPT", "extraction_text": "Combiner", "char_interval": {"start_pos": 11577, "end_pos": 11585}, "alignment_status": "match_fuzzy", "extraction_index": 13, "group_index": 12, "description": null, "attributes": {"definition": "An optional, user-defined associative and commutative function in Pregel used to combine messages destined for the same node, thereby reducing network bandwidth usage."}}, {"extraction_class": "CONCEPT", "extraction_text": "Aggregator", "char_interval": {"start_pos": 2233, "end_pos": 2244}, "alignment_status": "match_fuzzy", "extraction_index": 14, "group_index": 13, "description": null, "attributes": {"definition": "A mechanism in Pregel where each node can emit a value, which is then aggregated across all nodes to produce a final global value. This value is made available to all nodes in the next superstep and can be used for summary statistics or coordination."}}, {"extraction_class": "CONCEPT", "extraction_text": "Computing Platform & Computing Engine", "char_interval": {"start_pos": 15253, "end_pos": 15290}, "alignment_status": "match_exact", "extraction_index": 15, "group_index": 14, "description": null, "attributes": {"definition": "A development integrated environment that provides technical standards, computing architecture, and a series of development technologies and tools for big data computing and analysis, such as Hadoop, Spark, and Storm."}}, {"extraction_class": "TOPIC", "extraction_text": "Overview of Distributed Computing Models and Platforms", "char_interval": null, "alignment_status": null, "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "This session summarizes various computing models and platforms, including batch processing, stream computing, Massively Parallel Processing (MPP) for structured data, in-memory computing, graph computing, and interactive computing. A table details their respective computing patterns, representative products, storage systems, and key technologies. The lecture specifically focused on the mechanism and principles of distributed graph computing and provided a summary of different computing models.", "char_interval": {"start_pos": 15848, "end_pos": 15852}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "Batch processing, stream computing, Massively Parallel Processing, in-memory computing, Graph Computing, interactive computing, distributed graph computing, computing model", "char_interval": {"start_pos": 16033, "end_pos": 16321}, "alignment_status": "match_fuzzy", "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Batch processing", "char_interval": {"start_pos": 16033, "end_pos": 16049}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "A computing model summarized in the session, covering its computing pattern, representative products, storage system, and key technologies."}}, {"extraction_class": "CONCEPT", "extraction_text": "Stream computing", "char_interval": {"start_pos": 16051, "end_pos": 16067}, "alignment_status": "match_fuzzy", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "A computing model summarized in the session, covering its computing pattern, representative products, storage system, and key technologies."}}, {"extraction_class": "CONCEPT", "extraction_text": "Massively Parallel Processing (MPP)", "char_interval": null, "alignment_status": null, "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "A computing model for structured data, summarized in the session, covering its computing pattern, representative products, storage system, and key technologies."}}, {"extraction_class": "CONCEPT", "extraction_text": "In-memory computing", "char_interval": {"start_pos": 16118, "end_pos": 16139}, "alignment_status": "match_fuzzy", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "A computing model summarized in the session, covering its computing pattern, representative products, storage system, and key technologies."}}, {"extraction_class": "CONCEPT", "extraction_text": "Graph Computing", "char_interval": {"start_pos": 16141, "end_pos": 16156}, "alignment_status": "match_fuzzy", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "A computing model summarized in the session, covering its computing pattern, representative products, storage system, and key technologies."}}, {"extraction_class": "CONCEPT", "extraction_text": "Interactive computing", "char_interval": {"start_pos": 16165, "end_pos": 16186}, "alignment_status": "match_fuzzy", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "A computing model summarized in the session, covering its computing pattern, representative products, storage system, and key technologies."}}, {"extraction_class": "CONCEPT", "extraction_text": "Distributed graph computing", "char_interval": {"start_pos": 16260, "end_pos": 16287}, "alignment_status": "match_fuzzy", "extraction_index": 10, "group_index": 9, "description": null, "attributes": {"definition": "A computing model whose mechanism and principles were covered in the session."}}], "text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session we discuss Distributed Graph Computing.\nFrom the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing platform and engine layer, etc.\nComputing models are the way that different kinds of big data is processed in different scenarios, We have learned the batch processing, stream computing, Large-scale concurrent processing model for structured data, and In-memory database, in this session we discuss Distributed Graph Computing.\nWith Hadoop and Spark, we can accomplish data parallel processing and improve the computing performance. How about big graph that beyond the single node processing ability.\nthe graph parallel computing can solve this problem, represented by the Google Pregel, Apache Graph lab and Apache Giraph.\nPregel was first outlined in a paper published by Google in 2010. \nIt is system for large scale graph processing (think billions of nodes), and has served as inspiration for Apache Giraph, which Facebook uses internally to analyze their social network, as well Apache Spark’s GraphX library, which provides an API for Pregel computations.\nThe GraphLab project was started by Prof. Carlos Guestrin of Carnegie Mellon University in 2009. It is an open source project using an Apache License. While GraphLab was originally developed for Machine Learning tasks, it has found great success at a broad range of other data-mining tasks; out-performing other abstractions by orders of magnitude.\nApache Giraph is an iterative graph processing system built for high scalability. \nFor example, it is currently used at Facebook to analyze the social graph formed by users and their connections. \nGiraph originated as the open-source counterpart to Pregel, the graph processing architecture developed at Google and described in a 2010 paper. \nBoth systems are inspired by the Bulk Synchronous Parallel model of distributed computation introduced by Leslie Valiant. \nGiraph adds several features beyond the basic Pregel model, including master computation, sharded aggregators, edge-oriented input, out-of-core computation, and more.\nMost of the graph processing algorithm can be expressed in terms of a combination of \"traversal\" and \"transformation\".\nIn the case of \"traversal\", it can be expressed as a path which contains a sequence of segments. \nEach segment contains a traversal from a node to an arc, followed by a traversal from an arc to a node. \nThe main goal of Graph transformation is to modify the graph. \nThis include modifying the properties of existing nodes and arcs, creating new arcs / nodes and removing existing arcs / nodes. \nThe modification logic is provided by a user-defined function, which will be applied to all active nodes.\nthe most basic (atomic) unit is a \"node\" that contains its properties, outward arcs (and its properties) as well as the node id (just the id) that the outward arc points to. \nThe node also has a logical inbox to receive all messages sent to it.\nPregel can be thought as a generalized parallel graph transformation framework.\nThe whole graph is broken down into multiple \"partitions\", each contains a large number of nodes. \nPartition is a unit of execution and typically has an execution thread associated with it. \nA \"worker\" machine can host multiple \"partitions\".\nLet me briefly describe the process first:\nProcessing model is all active node will be executed whole processing completed when No more active node and No more in transit messages.\nSuperstep execution is, 1) Receive message from inbox, 2)Modify node and arc properties.  3)Halt self until new message received.  4) send messages to other nodes causing them active; 5)remove the existing or creating new arcs.\nThe execution model is based on BSP (Bulk Synchronous Processing) model. \nIn this model, there are multiple processing units proceeding in parallel in a sequence of \"supersteps\". \nWithin each \"superstep\", each processing units first receive all messages delivered to them from the preceding \"superstep\", \nand then manipulate their local data and may queue up the message that it intends to send to other processing units. \nThis happens asynchronously and simultaneously among all processing units. The queued up message will be delivered to the destined processing units but won't be seen until the next \"superstep\". \nWhen all the processing unit finishes the message delivery (hence the synchronization point), the next superstep can be started, and the cycle repeats until the termination condition has been reached.\nNotice that depends on the graph algorithms, the assignment of nodes to a partition may have an overall performance impact. \nPregel provides a default assignment where partition = nodeId % N but user can overwrite this assignment algorithm if they want. \nIn general, it is a good idea to put close-neighbor nodes into the same partition so that message between these nodes doesn't need to flow into the network and hence reduce communication overhead. \nOf course, this also means traversing the neighboring nodes all happen within the same machine and hinder parallelism. \nThis usually is not a problem when the context nodes are very diverse. \nIn my experience of parallel graph processing, coarse-grain parallelism is preferred over fine-grain parallelism as it reduces communication overhead.\nIn each superstep, there are two phases which are compute and communicate.\nIn computing phase, the node finishes their computing task and in communicating phase, the vertices send and receive the message.\nIf vertices receive a message, it will be active in next superstep, if no message receive, the vertices will be inactive in next superstep.\nThe complete picture of execution can be implemented as diagram :\nThe basic processing unit is a \"thread\" associated with each partition, running inside a worker. \nEach worker receive messages from previous \"superstep\" from its \"inQ\" and dispatch the message to the corresponding partition that the destination node is residing. \nAfter that, a user defined \"compute()\" function is invoked on each node of the partition. \nNotice that there is a single thread per partition so nodes within a partition are executed sequentially and the order of execution is undeterministic.\nThe \"master\" is playing a central role to coordinate the execute of supersteps in sequence. \nIt signals the beginning of a new superstep to all workers after knowing all of them has completed the previous one. \nIt also pings each worker to know their processing status and periodically issue \"checkpoint\" command to all workers who will then save its partition to a persistent graph store. \nPregel doesn't define or mandate the graph storage model so any persistent mechanism should work well. \nThere is a \"load\" phase at the beginning where each partition starts empty and read a slice of the graph storage. \nFor each node read from the storage, a \"partition()\" function will be invoked and load the node in the current partition if the function returns the same node, otherwise the node is queue to another partition who the node is assigned to.\nFault resilience is achieved by having the checkpoint mechanism where each worker is instructed to save its in-memory graph partition to the graph storage periodically (at the beginning of a superstep). \nIf the worker is detected to be dead (not responding to the \"ping\" message from the master), the master will instruct the surviving workers to take up the partitions of the failed worker. \nThe whole processing will be reverted to the previous checkpoint and proceed again from there (even the healthy worker needs to redo the previous processing).\nFurther optimization is available in Pregel to reduce the network bandwidth usage. \nMessages destined to the same node can be combined using a user-defined \"combine ()\" function, which is required to be associative and commutative. \nThis is similar to the same combine () method in Map/Reduce model.\nIn addition, each node can also emit an \"aggregate value\" at the end of \"compute ()\". \nWorker will invoke a user-defined \"aggregate ()\" function that aggregate all node's aggregate value into a partition level aggregate value and all the way to the master. \nThe final aggregated value will be made available to all nodes in the next superstep. \nJust aggregate value can be used to calculate summary statistic of each node as well as coordinating the progress of each processing units.\nMany problems by nature form a graph such as the web, transport and social media. However, a common large scale distributed processing pipeline, MapReduce is ill suited for such tasks. MapReduce requires the data chunks to be processed independently. In other words, every job needs to know all the information to compute. This independence is certainly not the case with graphs and jobs might require previous computation or information from neighbouring jobs in order to calculate. To address this issue, Google’s Pregel architecture employs a message passing system creating a “large-scale graph processing” framework.\nAnother problem is the fact that MapReduce moves data around, for example by shuffling, in order to process it. This approach entails graph partitions moving around machines incurring high network overhead. A similar problem arises when MapReduce jobs are chained to implement iterative graph algorithms. Therefore, “MapReduce requires passing the entire state of the graph from one stage to the next - in general requiring much more communication and associated serialization overhead.”\nFinally, “coordinating the steps of a chained MapReduce adds programming complexity that is avoided by Pregel’s iteration over supersteps”. MapReduce is not iterative, it can handle single iteration and requires the user to handle the iteration, i.e. the chaining of multiple MapReduce jobs. This situation can get complicated really quick for real world graph algorithms running at scale not only to implement but also to debug when things go wrong.\nThe user algorithm is distributed to many machines one of which becomes the Master. The Master partitions the graph into multiple graph partitions and assigns them to workers. A worker can have multiple graph partitions which contain a set of vertices and their corresponding outgoing edges which might point to any vertex in the graph, possibly stored on another worker.\nOnce the setup is complete, the master tells each worker to execute a superstep. At the beginning of a superstep, workers save their current state into persistent storage for fault tolerance. Workers then Compute() the user function for active vertices in their graph partitions sending and receiving messages asynchronously. When the worker is done computing, it tells the master it has finished and how many vertices will be active in the next superstep.\nFor message passing, 2 queues are maintained for superstep S and S+1 to simultaneously receive messages while computing the current superstep. Any message sent at superstep S is received at superstep S+1 to avoid locking at message level. There is no guarantee on the ordering of received messages. While computing, a worker can send messages to other workers including itself depending on where the vertex is stored. Messages are sent in a single network message when a certain message buffer size threshold is reached in order to reduce network traffic. If an optional combiner is specified by the user, messages can be combined when either outbound or inbound reducing network usage and storage respectively. For example, several messages can be combined into one if the user is interested in the sum of their value.\nAggregators can be specified by the user to collect results at each superstep. Workers provide values to an aggregator instance to produce a single local value Aggr. \nAt the end of the superstep, workers combine the partially reduced aggregator local values into a global value using a tree-based reduction and deliver it to the master.\nWhen all workers complete a superstep, the master initiates the execution of the next superstep.\nThis process repeats until there are no active vertices. \nThe vertices become inactive after they vote to halt and there are no incoming messages to wake them up again. \nWhen all vertices are inactive, the execution of the algorithm terminates.\nAlternative to Message Passing\nAn alternative to message passing is a state passing model in which the sent messages are stored locally and only exchanged at the end of a superstep. In other words, the model latches onto the synchronisation barrier between supersteps to exchange the state of message queues rather than during computation. \nAs an immediate advantage, workers don’t incur any network traffic during computation. At the end of a superstep, messages can be sent in bulk, larger than the current buffer size, compensating for the handshake overhead of opening and closing multiple connections to different machines. In this case, all messages destined to another worker will be sent in one network connection.\nA disadvantage would be a longer pause time in between supersteps. State passing approach would be worse whenever computation is less than information passing. \nFor example, if nodes only compute the average of neighbors, getting neighbor values outweighs computing an average causing the state to be transferred at the end of the first superstep. Perhaps, the user can specify depending on the algorithm when messages should be exchanged prioritizing computation or communication. In this manner, the message passing of the Pregel execution can be fine-tuned to optimize the performance of the user’s algorithm and dataset.\nScalability Limitation\nA common limitation with large-scale distributed frameworks is the network traffic message passing creates. For dense graphs, the network could pose as a bottleneck when a lot of messages are passed during computation. Similarly, if there is a node with a high degree, whenever it sends or receives messages a single worker will have to deal with all the network traffic causing that worker to slow down and increase wait time on superstep barrier synchronisation.\nAnother limitation is the superstep barrier itself when every worker waits for other workers to complete to proceed to the next superstep. When scaled horizontally, the increased probability of failing machines might mean that the computation can wait a long time until all workers have completed and are ready for the next superstep. \nFor example, while a worker is being recovered another can fail causing another recovery during which the overall computation is stagnating. Unlike MapReduce in which other sub branches of the computation such as reduction continue, Pregel needs to wait for the barrier to be passed.\nApplying the Data Algorithm Layer algorithms and combined with the Batch, stream, Massive Parallel Processing, In Memory Computing\nOr Graph Computing model, we can process big data problems, but it is hard for us to implement all the algorithm and computing model by ourselves manually, \nThe Computing Platform & Computing Engine Layer can provide us the platform and engine which includes the needed tools, libraries to facilitate the implementing the complex Algorithms and Computing models.\nComputing platform and engine refer to a development integrated environment that provides Technical Standards, Computing Architecture, and a series of Development Technologies and Tools for big data computing and analysis.\nThe current representative computing platforms are: Hadoop, Spark, Storm, and Google's commercial platforms based on a series of big data computing technologies.\nHere in this table the computing model and platform are summarized, the computing pattern, representative products, storage system, Computing model, Computing platform and Key technologies \nof Batch processing, stream computing, Massively Parallel Processing for structured data, in memory computing, Graph Computing and the interactive computing are showed in table.\nIn this session, we learned mechanism, principle of distributed graph computing and we summarized computing model. Thank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_bdbe9cba"}
