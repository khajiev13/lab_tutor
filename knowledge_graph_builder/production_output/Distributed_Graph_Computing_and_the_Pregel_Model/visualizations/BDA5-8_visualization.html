<style>
.lx-highlight { position: relative; border-radius:3px; padding:1px 2px;}
.lx-highlight .lx-tooltip {
  visibility: hidden;
  opacity: 0;
  transition: opacity 0.2s ease-in-out;
  background: #333;
  color: #fff;
  text-align: left;
  border-radius: 4px;
  padding: 6px 8px;
  position: absolute;
  z-index: 1000;
  bottom: 125%;
  left: 50%;
  transform: translateX(-50%);
  font-size: 12px;
  max-width: 240px;
  white-space: normal;
  box-shadow: 0 2px 6px rgba(0,0,0,0.3);
}
.lx-highlight:hover .lx-tooltip { visibility: visible; opacity:1; }
.lx-animated-wrapper { max-width: 100%; font-family: Arial, sans-serif; }
.lx-controls {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 8px;
  padding: 12px; margin-bottom: 16px;
}
.lx-button-row {
  display: flex; justify-content: center; gap: 8px; margin-bottom: 12px;
}
.lx-control-btn {
  background: #4285f4; color: white; border: none; border-radius: 4px;
  padding: 8px 16px; cursor: pointer; font-size: 13px; font-weight: 500;
  transition: background-color 0.2s;
}
.lx-control-btn:hover { background: #3367d6; }
.lx-progress-container {
  margin-bottom: 8px;
}
.lx-progress-slider {
  width: 100%; margin: 0; appearance: none; height: 6px;
  background: #ddd; border-radius: 3px; outline: none;
}
.lx-progress-slider::-webkit-slider-thumb {
  appearance: none; width: 18px; height: 18px; background: #4285f4;
  border-radius: 50%; cursor: pointer;
}
.lx-progress-slider::-moz-range-thumb {
  width: 18px; height: 18px; background: #4285f4; border-radius: 50%;
  cursor: pointer; border: none;
}
.lx-status-text {
  text-align: center; font-size: 12px; color: #666; margin-top: 4px;
}
.lx-text-window {
  font-family: monospace; white-space: pre-wrap; border: 1px solid #90caf9;
  padding: 12px; max-height: 260px; overflow-y: auto; margin-bottom: 12px;
  line-height: 1.6;
}
.lx-attributes-panel {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 6px;
  padding: 8px 10px; margin-top: 8px; font-size: 13px;
}
.lx-current-highlight {
  border-bottom: 4px solid #ff4444;
  font-weight: bold;
  animation: lx-pulse 1s ease-in-out;
}
@keyframes lx-pulse {
  0% { text-decoration-color: #ff4444; }
  50% { text-decoration-color: #ff0000; }
  100% { text-decoration-color: #ff4444; }
}
.lx-legend {
  font-size: 12px; margin-bottom: 8px;
  padding-bottom: 8px; border-bottom: 1px solid #e0e0e0;
}
.lx-label {
  display: inline-block;
  padding: 2px 4px;
  border-radius: 3px;
  margin-right: 4px;
  color: #000;
}
.lx-attr-key {
  font-weight: 600;
  color: #1565c0;
  letter-spacing: 0.3px;
}
.lx-attr-value {
  font-weight: 400;
  opacity: 0.85;
  letter-spacing: 0.2px;
}

/* Add optimizations with larger fonts and better readability for GIFs */
.lx-gif-optimized .lx-text-window { font-size: 16px; line-height: 1.8; }
.lx-gif-optimized .lx-attributes-panel { font-size: 15px; }
.lx-gif-optimized .lx-current-highlight { text-decoration-thickness: 4px; }
</style>
    <div class="lx-animated-wrapper lx-gif-optimized">
      <div class="lx-attributes-panel">
        <div class="lx-legend">Highlights Legend: <span class="lx-label" style="background-color:#D2E3FC;">CONCEPT</span> <span class="lx-label" style="background-color:#C8E6C9;">KEYWORDS</span> <span class="lx-label" style="background-color:#FEF0C3;">SUMMARY</span> <span class="lx-label" style="background-color:#F9DEDC;">TOPIC</span></div>
        <div id="attributesContainer"></div>
      </div>
      <div class="lx-text-window" id="textWindow">
        Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering
School of Computer Science, in Beijing Institute of Technology, in this session we discuss <span class="lx-highlight lx-current-highlight" data-idx="0" style="background-color:#F9DEDC;"><span class="lx-highlight" data-idx="1" style="background-color:#D2E3FC;">Distributed Graph Computing</span></span>.
From the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing platform and engine layer, etc.
Computing models are the way that different kinds of big data is processed in different scenarios, We have learned the batch processing, stream computing, Large-scale concurrent processing model for structured data, and In-memory database, in this session we discuss Distributed Graph Computing.
With Hadoop and Spark, we can accomplish data parallel processing and improve the computing performance. How about big graph that beyond the single node processing ability.
the graph parallel computing can solve this problem, represented by the Google <span class="lx-highlight" data-idx="2" style="background-color:#D2E3FC;">Pregel</span>, Apache Graph lab and <span class="lx-highlight" data-idx="3" style="background-color:#D2E3FC;">Apache Giraph</span>.
Pregel was first outlined in a paper published by Google in 2010. 
It is system for large scale graph processing (think billions of nodes), and has served as inspiration for Apache Giraph, which Facebook uses internally to analyze their social network, as well Apache Spark’s GraphX library, which provides an API for Pregel computations.
The <span class="lx-highlight" data-idx="4" style="background-color:#D2E3FC;">GraphLab</span> project was started by Prof. Carlos Guestrin of Carnegie Mellon University in 2009. It is an open source project using an Apache License. While GraphLab was originally developed for Machine Learning tasks, it has found great success at a broad range of other data-mining tasks; out-performing other abstractions by orders of magnitude.
Apache Giraph is an iterative graph processing system built for high scalability. 
For example, it is currently used at Facebook to analyze the social graph formed by users and their connections. 
Giraph originated as the open-source counterpart to Pregel, the graph processing architecture developed at Google and described in a 2010 paper. 
Both systems are inspired by the <span class="lx-highlight" data-idx="5" style="background-color:#D2E3FC;">Bulk Synchronous Parallel model of distributed computation introduced by Leslie Valiant. 
Giraph adds several features beyond the basic Pregel model, including <span class="lx-highlight" data-idx="6" style="background-color:#D2E3FC;">master</span> computation, sharded <span class="lx-highlight" data-idx="7" style="background-color:#D2E3FC;">aggregators</span>, edge-oriented input, out-of-core computation, and more.
Most of the graph processing algorithm can be expressed in terms of a combination of &quot;traversal&quot; and &quot;transformation&quot;.
In the case of &quot;traversal&quot;, it can be expressed as a path which contains a sequence of segments. 
Each segment contains a traversal from a node to an arc, followed by a traversal from an arc to a node. 
The main goal of Graph transformation is to modify the graph. 
This include modifying the properties of existing nodes and arcs, creating new arcs / nodes and removing existing arcs / nodes. 
The modification logic is provided by a user-defined function, which will be applied to all active nodes.
the most basic (atomic) unit is a &quot;node&quot; that contains its properties, outward arcs (and its properties) as well as the node id (just the id) that the outward arc points to. 
The node also has a logical inbox to receive all messages sent to it.
Pregel can be thought as a generalized parallel graph transformation framework.
The whole graph is broken down into multiple &quot;<span class="lx-highlight" data-idx="8" style="background-color:#D2E3FC;">partitions</span>&quot;, each contains a large number of nodes. 
Partition is a unit of execution and typically has an execution thread associated with it. 
A &quot;<span class="lx-highlight" data-idx="9" style="background-color:#D2E3FC;">worker</span>&quot; machine can host multiple &quot;partitions&quot;.
Let me briefly describe the process first:
Processing model is all active node will be executed whole processing completed when No more active node and No more in transit messages.
<span class="lx-highlight" data-idx="10" style="background-color:#D2E3FC;">Superstep</span> execution is, 1) Receive message from inbox, 2)Modify node and arc properties.  3)Halt self until new message received.  4) send messages to other nodes causing them active; 5)remove the existing or creating new arcs.
The execution model is based on BSP (Bulk Synchronous Processing) model</span>. 
In this model, there are multiple processing units proceeding in parallel in a sequence of &quot;supersteps&quot;. 
Within each &quot;superstep&quot;, each processing units first receive all messages delivered to them from the preceding &quot;superstep&quot;, 
and then manipulate their local data and may queue up the message that it intends to send to other processing units. 
This happens asynchronously and simultaneously among all processing units. The queued up message will be delivered to the destined processing units but won&#x27;t be seen until the next &quot;superstep&quot;. 
When all the processing unit finishes the message delivery (hence the synchronization point), the next superstep can be started, and the cycle repeats until the termination condition has been reached.
Notice that depends on the graph algorithms, the assignment of nodes to a partition may have an overall performance impact. 
Pregel provides a default assignment where partition = nodeId % N but user can overwrite this assignment algorithm if they want. 
In general, it is a good idea to put close-neighbor nodes into the same partition so that message between these nodes doesn&#x27;t need to flow into the network and hence reduce communication overhead. 
Of course, this also means traversing the neighboring nodes all happen within the same machine and hinder parallelism. 
This usually is not a problem when the context nodes are very diverse. 
In my experience of parallel graph processing, coarse-grain parallelism is preferred over fine-grain parallelism as it reduces communication overhead.
In each superstep, there are two phases which are compute and communicate.
In computing phase, the node finishes their computing task and in communicating phase, the vertices send and receive the message.
If vertices receive a message, it will be active in next superstep, if no message receive, the vertices will be inactive in next superstep.
The complete picture of execution can be implemented as diagram :
The basic processing unit is a &quot;thread&quot; associated with each partition, running inside a worker. 
Each worker receive messages from previous &quot;superstep&quot; from its &quot;inQ&quot; and dispatch the message to the corresponding partition that the destination node is residing. 
After that, a user defined &quot;compute()&quot; function is invoked on each node of the partition. 
Notice that there is a single thread per partition so nodes within a partition are executed sequentially and the order of execution is undeterministic.
The &quot;master&quot; is playing a central role to coordinate the execute of supersteps in sequence. 
It signals the beginning of a new superstep to all workers after knowing all of them has completed the previous one. 
It also pings each worker to know their processing status and periodically issue &quot;checkpoint&quot; command to all workers who will then save its partition to a persistent graph store. 
Pregel doesn&#x27;t define or mandate the graph storage model so any persistent mechanism should work well. 
There is a &quot;load&quot; phase at the beginning where each partition starts empty and read a slice of the graph storage. 
For each node read from the storage, a &quot;partition()&quot; function will be invoked and load the node in the current partition if the function returns the same node, otherwise the node is queue to another partition who the node is assigned to.
Fault resilience is achieved by having the checkpoint mechanism where each worker is instructed to save its in-memory graph partition to the graph storage periodically (at the beginning of a superstep). 
If the worker is detected to be dead (not responding to the &quot;ping&quot; message from the master), the master will instruct the surviving workers to take up the partitions of the failed worker. 
The whole processing will be reverted to the previous checkpoint and proceed again from there (even the healthy worker needs to redo the previous processing).
Further optimization is available in Pregel to reduce the network bandwidth usage. 
Messages destined to the same node can be combined using a user-defined &quot;combine ()&quot; function, which is required to be associative and commutative. 
This is similar to the same combine () method in Map/Reduce model.
In addition, each node can also emit an &quot;aggregate value&quot; at the end of &quot;compute ()&quot;. 
Worker will invoke a user-defined &quot;aggregate ()&quot; function that aggregate all node&#x27;s aggregate value into a partition level aggregate value and all the way to the master. 
The final aggregated value will be made available to all nodes in the next superstep. 
Just aggregate value can be used to calculate summary statistic of each node as well as coordinating the progress of each processing units.
Many problems by nature form a graph such as the web, transport and social media. However, a common large scale distributed processing pipeline, MapReduce is ill suited for such tasks. MapReduce requires the data chunks to be processed independently. In other words, every job needs to know all the information to compute. This independence is certainly not the case with graphs and jobs might require previous computation or information from neighbouring jobs in order to calculate. To address this issue, Google’s Pregel architecture employs a message passing system creating a “large-scale graph processing” framework.
Another problem is the fact that MapReduce moves data around, for example by shuffling, in order to process it. This approach entails graph partitions moving around machines incurring high network overhead. A similar problem arises when MapReduce jobs are chained to implement iterative graph algorithms. Therefore, “MapReduce requires passing the entire state of the graph from one stage to the next - in general requiring much more communication and associated serialization overhead.”
Finally, “coordinating the steps of a chained MapReduce adds programming complexity that is avoided by Pregel’s iteration over supersteps”. MapReduce is not iterative, it can handle single iteration and requires the user to handle the iteration, i.e. the chaining of multiple MapReduce jobs. This situation can get complicated really quick for real world graph algorithms running at scale not only to implement but also to debug when things go wrong.
The user algorithm is distributed to many machines one of which becomes the Master. The Master partitions the graph into multiple graph partitions and assigns them to workers. A worker can have multiple graph partitions which contain a set of vertices and their corresponding outgoing edges which might point to any vertex in the graph, possibly stored on another worker.
Once the setup is complete, the master tells each worker to execute a superstep. At the beginning of a superstep, workers save their current state into persistent storage for fault tolerance. Workers then Compute() the user function for active vertices in their graph partitions sending and receiving messages asynchronously. When the worker is done computing, it tells the master it has finished and how many vertices will be active in the next superstep.
For message passing, 2 queues are maintained for superstep S and S+1 to simultaneously receive messages while computing the current superstep. Any message sent at superstep S is received at superstep S+1 to avoid locking at message level. There is no guarantee on the ordering of received messages. While computing, a worker can send messages to other workers including itself depending on where the vertex is stored. Messages are sent in a single network message when a certain message buffer size threshold is reached in order to reduce network traffic. If an optional <span class="lx-highlight" data-idx="11" style="background-color:#D2E3FC;">combiner</span> is specified by the user, messages can be combined when either outbound or inbound reducing network usage and storage respectively. For example, several messages can be combined into one if the user is interested in the sum of their value.
Aggregators can be specified by the user to collect results at each superstep. Workers provide values to an aggregator instance to produce a single local value Aggr. 
At the end of the superstep, workers combine the partially reduced aggregator local values into a global value using a tree-based reduction and deliver it to the master.
When all workers complete a superstep, the master initiates the execution of the next superstep.
This process repeats until there are no active vertices. 
The vertices become inactive after they vote to halt and there are no incoming messages to wake them up again. 
When all vertices are inactive, the execution of the algorithm terminates.
Alternative to Message Passing
An alternative to message passing is a state passing model in which the sent messages are stored locally and only exchanged at the end of a superstep. In other words, the model latches onto the synchronisation barrier between supersteps to exchange the state of message queues rather than during computation. 
As an immediate advantage, workers don’t incur any network traffic during computation. At the end of a superstep, messages can be sent in bulk, larger than the current buffer size, compensating for the handshake overhead of opening and closing multiple connections to different machines. In this case, all messages destined to another worker will be sent in one network connection.
A disadvantage would be a longer pause time in between supersteps. State passing approach would be worse whenever computation is less than information passing. 
For example, if nodes only compute the average of neighbors, getting neighbor values outweighs computing an average causing the state to be transferred at the end of the first superstep. Perhaps, the user can specify depending on the algorithm when messages should be exchanged prioritizing computation or communication. In this manner, the message passing of the Pregel execution can be fine-tuned to optimize the performance of the user’s algorithm and dataset.
Scalability Limitation
A common limitation with large-scale distributed frameworks is the network traffic message passing creates. For dense graphs, the network could pose as a bottleneck when a lot of messages are passed during computation. Similarly, if there is a node with a high degree, whenever it sends or receives messages a single worker will have to deal with all the network traffic causing that worker to slow down and increase wait time on superstep barrier synchronisation.
Another limitation is the superstep barrier itself when every worker waits for other workers to complete to proceed to the next superstep. When scaled horizontally, the increased probability of failing machines might mean that the computation can wait a long time until all workers have completed and are ready for the next superstep. 
For example, while a worker is being recovered another can fail causing another recovery during which the overall computation is stagnating. Unlike MapReduce in which other sub branches of the computation such as reduction continue, Pregel needs to wait for the barrier to be passed.
Applying the Data Algorithm Layer algorithms and combined with the Batch, stream, Massive Parallel Processing, In Memory Computing
Or Graph Computing model, we can process big data problems, but it is hard for us to implement all the algorithm and computing model by ourselves manually, 
The <span class="lx-highlight" data-idx="12" style="background-color:#D2E3FC;">Computing Platform &amp; Computing Engine</span> Layer can provide us the platform and engine which includes the needed tools, libraries to facilitate the implementing the complex Algorithms and Computing models.
Computing platform and engine refer to a development integrated environment that provides Technical Standards, Computing Architecture, and a series of Development Technologies and Tools for big data computing and analysis.
The current representative computing platforms are: Hadoop, Spark, Storm, and Google&#x27;s commercial platforms based on a series of big data computing technologies.
Here in <span class="lx-highlight" data-idx="13" style="background-color:#FEF0C3;">this</span> table the computing model and platform are summarized, the computing pattern, representative products, storage system, Computing model, Computing platform and Key technologies 
of <span class="lx-highlight" data-idx="14" style="background-color:#C8E6C9;"><span class="lx-highlight" data-idx="15" style="background-color:#D2E3FC;">Batch processing</span>, <span class="lx-highlight" data-idx="16" style="background-color:#D2E3FC;">stream computing</span>, Massively Parallel Processing for structured data<span class="lx-highlight" data-idx="17" style="background-color:#D2E3FC;">, in memory computing</span>, <span class="lx-highlight" data-idx="18" style="background-color:#D2E3FC;">Graph Computing</span> and the <span class="lx-highlight" data-idx="19" style="background-color:#D2E3FC;">interactive computing</span> are showed in table.
In this session, we learned mechanism, principle of <span class="lx-highlight" data-idx="20" style="background-color:#D2E3FC;">distributed graph computing</span> and we summarized computing model</span>. Thank you for your attention, if you have any question, feel free to contact me.
      </div>
      <div class="lx-controls">
        <div class="lx-button-row">
          <button class="lx-control-btn" onclick="playPause()">▶️ Play</button>
          <button class="lx-control-btn" onclick="prevExtraction()">⏮ Previous</button>
          <button class="lx-control-btn" onclick="nextExtraction()">⏭ Next</button>
        </div>
        <div class="lx-progress-container">
          <input type="range" id="progressSlider" class="lx-progress-slider"
                 min="0" max="20" value="0"
                 onchange="jumpToExtraction(this.value)">
        </div>
        <div class="lx-status-text">
          Entity <span id="entityInfo">1/21</span> |
          Pos <span id="posInfo">[182-209]</span>
        </div>
      </div>
    </div>

    <script>
      (function() {
        const extractions = [{"index": 0, "class": "TOPIC", "text": "Distributed Graph Computing and the Pregel Model", "color": "#F9DEDC", "startPos": 182, "endPos": 209, "beforeText": ", from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session we discuss ", "extractionText": "Distributed Graph Computing", "afterText": ".\nFrom the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing pl", "attributesHtml": "<div><strong>class:</strong> TOPIC</div><div><strong>attributes:</strong> {}</div>"}, {"index": 1, "class": "CONCEPT", "text": "Distributed Graph Computing", "color": "#D2E3FC", "startPos": 182, "endPos": 209, "beforeText": ", from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session we discuss ", "extractionText": "Distributed Graph Computing", "afterText": ".\nFrom the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing pl", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A computing model designed to process large-scale graphs that are too big to fit on a single node, utilizing graph parallel computing techniques.</span>}</div>"}, {"index": 2, "class": "CONCEPT", "text": "Pregel", "color": "#D2E3FC", "startPos": 937, "endPos": 943, "beforeText": "e. How about big graph that beyond the single node processing ability.\nthe graph parallel computing can solve this problem, represented by the Google ", "extractionText": "Pregel", "afterText": ", Apache Graph lab and Apache Giraph.\nPregel was first outlined in a paper published by Google in 2010. \nIt is system for large scale graph processing", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A system developed by Google for large-scale graph processing based on the Bulk Synchronous Parallel model. It serves as a generalized parallel graph transformation framework and has inspired systems like Apache Giraph and Apache Spark\u2019s GraphX.</span>}</div>"}, {"index": 3, "class": "CONCEPT", "text": "Apache Giraph", "color": "#D2E3FC", "startPos": 966, "endPos": 979, "beforeText": "eyond the single node processing ability.\nthe graph parallel computing can solve this problem, represented by the Google Pregel, Apache Graph lab and ", "extractionText": "Apache Giraph", "afterText": ".\nPregel was first outlined in a paper published by Google in 2010. \nIt is system for large scale graph processing (think billions of nodes), and has ", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An iterative, highly scalable graph processing system inspired by Google&#x27;s Pregel, used at Facebook to analyze its social graph.</span>}</div>"}, {"index": 4, "class": "CONCEPT", "text": "GraphLab", "color": "#D2E3FC", "startPos": 1324, "endPos": 1332, "beforeText": "h Facebook uses internally to analyze their social network, as well Apache Spark\u2019s GraphX library, which provides an API for Pregel computations.\nThe ", "extractionText": "GraphLab", "afterText": " project was started by Prof. Carlos Guestrin of Carnegie Mellon University in 2009. It is an open source project using an Apache License. While Graph", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An open-source project originally developed for Machine Learning tasks that has proven effective for a broad range of data-mining tasks on graphs.</span>}</div>"}, {"index": 5, "class": "CONCEPT", "text": "Bulk Synchronous Parallel (BSP) model", "color": "#D2E3FC", "startPos": 2045, "endPos": 3969, "beforeText": "-source counterpart to\u00a0Pregel, the graph processing architecture developed at Google and described in a 2010\u00a0paper. \nBoth systems are inspired by the\u00a0", "extractionText": "Bulk Synchronous Parallel\u00a0model of distributed computation introduced by Leslie Valiant. \nGiraph adds several features beyond the basic Pregel model, including master computation, sharded aggregators, edge-oriented input, out-of-core computation, and more.\nMost of the graph processing algorithm can be expressed in terms of a combination of &quot;traversal&quot; and &quot;transformation&quot;.\nIn the case of &quot;traversal&quot;, it can be expressed as a path which contains a sequence of segments. \nEach segment contains a traversal from a node to an arc, followed by a traversal from an arc to a node. \nThe main goal of Graph transformation is to modify the graph. \nThis include modifying the properties of existing nodes and arcs, creating new arcs / nodes and removing existing arcs / nodes. \nThe modification logic is provided by a user-defined function, which will be applied to all active nodes.\nthe most basic (atomic) unit is a &quot;node&quot; that contains its properties, outward arcs (and its properties) as well as the node id (just the id) that the outward arc points to. \nThe node also has a logical inbox to receive all messages sent to it.\nPregel can be thought as a generalized parallel graph transformation framework.\nThe whole graph is broken down into multiple &quot;partitions&quot;, each contains a large number of nodes. \nPartition is a unit of execution and typically has an execution thread associated with it. \nA &quot;worker&quot; machine can host multiple &quot;partitions&quot;.\nLet me briefly describe the process first:\nProcessing model is all active node will be executed whole processing completed when No more active node and No more in transit messages.\nSuperstep execution is, 1) Receive message from inbox, 2)Modify node and arc properties.  3)Halt self until new message received.  4) send messages to other nodes causing them active; 5)remove the existing or creating new arcs.\nThe execution model is based on\u00a0BSP (Bulk Synchronous Processing) model", "afterText": ". \nIn this model, there are multiple processing units proceeding in parallel in a sequence of &quot;supersteps&quot;. \nWithin each &quot;superstep&quot;, each processing ", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A model of distributed computation where multiple processing units proceed in parallel through a sequence of &#x27;supersteps&#x27;. In each superstep, units receive messages, perform local computations, and queue outgoing messages, with a synchronization point occurring before the next superstep begins.</span>}</div>"}, {"index": 6, "class": "CONCEPT", "text": "Master", "color": "#D2E3FC", "startPos": 2205, "endPos": 2211, "beforeText": "ronous Parallel\u00a0model of distributed computation introduced by Leslie Valiant. \nGiraph adds several features beyond the basic Pregel model, including ", "extractionText": "master", "afterText": " computation, sharded aggregators, edge-oriented input, out-of-core computation, and more.\nMost of the graph processing algorithm can be expressed in ", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A central coordinating machine in the Pregel architecture that partitions the graph, assigns partitions to workers, signals the start of each superstep, and manages fault tolerance through checkpointing.</span>}</div>"}, {"index": 7, "class": "CONCEPT", "text": "Aggregator", "color": "#D2E3FC", "startPos": 2233, "endPos": 2244, "beforeText": "tributed computation introduced by Leslie Valiant. \nGiraph adds several features beyond the basic Pregel model, including master computation, sharded ", "extractionText": "aggregators", "afterText": ", edge-oriented input, out-of-core computation, and more.\nMost of the graph processing algorithm can be expressed in terms of a combination of &quot;traver", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A mechanism in Pregel where each node can emit a value, which is then aggregated across all nodes to produce a final global value. This value is made available to all nodes in the next superstep and can be used for summary statistics or coordination.</span>}</div>"}, {"index": 8, "class": "CONCEPT", "text": "Partition", "color": "#D2E3FC", "startPos": 3293, "endPos": 3303, "beforeText": "ll messages sent to it.\nPregel can be thought as a generalized parallel graph transformation framework.\nThe whole graph is broken down into multiple &quot;", "extractionText": "partitions", "afterText": "&quot;, each contains a large number of nodes. \nPartition is a unit of execution and typically has an execution thread associated with it. \nA &quot;worker&quot; mach", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A unit of execution in Pregel that contains a large number of graph nodes. A worker machine can host multiple partitions.</span>}</div>"}, {"index": 9, "class": "CONCEPT", "text": "Worker", "color": "#D2E3FC", "startPos": 3441, "endPos": 3447, "beforeText": " &quot;partitions&quot;, each contains a large number of nodes. \nPartition is a unit of execution and typically has an execution thread associated with it. \nA &quot;", "extractionText": "worker", "afterText": "&quot; machine can host multiple &quot;partitions&quot;.\nLet me briefly describe the process first:\nProcessing model is all active node will be executed whole proces", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A machine in the Pregel architecture that hosts one or more graph partitions and executes a user-defined compute function on the vertices within those partitions.</span>}</div>"}, {"index": 10, "class": "CONCEPT", "text": "Superstep", "color": "#D2E3FC", "startPos": 3670, "endPos": 3679, "beforeText": "cess first:\nProcessing model is all active node will be executed whole processing completed when No more active node and No more in transit messages.\n", "extractionText": "Superstep", "afterText": " execution is, 1) Receive message from inbox, 2)Modify node and arc properties.  3)Halt self until new message received.  4) send messages to other no", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A single iteration in the BSP model where each processing unit receives messages from the previous step, manipulates its local data, and sends messages to other units that will be delivered in the next iteration.</span>}</div>"}, {"index": 11, "class": "CONCEPT", "text": "Combiner", "color": "#D2E3FC", "startPos": 11577, "endPos": 11585, "beforeText": "sages are sent in a single network message when a certain message buffer size threshold is reached in order to reduce network traffic. If an optional\u00a0", "extractionText": "combiner", "afterText": "\u00a0is specified by the user, messages can be combined when either outbound or inbound reducing network usage and storage respectively. For example, seve", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An optional, user-defined associative and commutative function in Pregel used to combine messages destined for the same node, thereby reducing network bandwidth usage.</span>}</div>"}, {"index": 12, "class": "CONCEPT", "text": "Computing Platform & Computing Engine", "color": "#D2E3FC", "startPos": 15253, "endPos": 15290, "beforeText": "mputing model, we can process big data problems, but it is hard for us to implement all the algorithm and computing model by ourselves manually, \nThe ", "extractionText": "Computing Platform &amp; Computing Engine", "afterText": " Layer can provide us the platform and engine which includes the needed tools, libraries to facilitate the implementing the complex Algorithms and Com", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A development integrated environment that provides technical standards, computing architecture, and a series of development technologies and tools for big data computing and analysis, such as Hadoop, Spark, and Storm.</span>}</div>"}, {"index": 13, "class": "SUMMARY", "text": "This session summarizes various computing models and platforms, including batch processing, stream computing, Massively Parallel Processing (MPP) for structured data, in-memory computing, graph computing, and interactive computing. A table details their respective computing patterns, representative products, storage systems, and key technologies. The lecture specifically focused on the mechanism and principles of distributed graph computing and provided a summary of different computing models.", "color": "#FEF0C3", "startPos": 15848, "endPos": 15852, "beforeText": "tative computing platforms are: Hadoop, Spark, Storm, and Google&#x27;s commercial platforms based on a series of big data computing technologies.\nHere in ", "extractionText": "this", "afterText": " table the computing model and platform are summarized, the computing pattern, representative products, storage system, Computing model, Computing pla", "attributesHtml": "<div><strong>class:</strong> SUMMARY</div><div><strong>attributes:</strong> {}</div>"}, {"index": 14, "class": "KEYWORDS", "text": "Batch processing, stream computing, Massively Parallel Processing, in-memory computing, Graph Computing, interactive computing, distributed graph computing, computing model", "color": "#C8E6C9", "startPos": 16033, "endPos": 16321, "beforeText": "platform are summarized, the computing pattern, representative products, storage system, Computing model, Computing platform and Key technologies \nof ", "extractionText": "Batch processing, stream computing, Massively Parallel Processing for structured data, in memory computing, Graph Computing and the interactive computing are showed in table.\nIn this session, we learned mechanism, principle of distributed graph computing and we summarized computing model", "afterText": ". Thank you for your attention, if you have any question, feel free to contact me.", "attributesHtml": "<div><strong>class:</strong> KEYWORDS</div><div><strong>attributes:</strong> {}</div>"}, {"index": 15, "class": "CONCEPT", "text": "Batch processing", "color": "#D2E3FC", "startPos": 16033, "endPos": 16049, "beforeText": "platform are summarized, the computing pattern, representative products, storage system, Computing model, Computing platform and Key technologies \nof ", "extractionText": "Batch processing", "afterText": ", stream computing, Massively Parallel Processing for structured data, in memory computing, Graph Computing and the interactive computing are showed i", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A computing model summarized in the session, covering its computing pattern, representative products, storage system, and key technologies.</span>}</div>"}, {"index": 16, "class": "CONCEPT", "text": "Stream computing", "color": "#D2E3FC", "startPos": 16051, "endPos": 16067, "beforeText": "rized, the computing pattern, representative products, storage system, Computing model, Computing platform and Key technologies \nof Batch processing, ", "extractionText": "stream computing", "afterText": ", Massively Parallel Processing for structured data, in memory computing, Graph Computing and the interactive computing are showed in table.\nIn this s", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A computing model summarized in the session, covering its computing pattern, representative products, storage system, and key technologies.</span>}</div>"}, {"index": 17, "class": "CONCEPT", "text": "In-memory computing", "color": "#D2E3FC", "startPos": 16118, "endPos": 16139, "beforeText": "em, Computing model, Computing platform and Key technologies \nof Batch processing, stream computing, Massively Parallel Processing for structured data", "extractionText": ", in memory computing", "afterText": ", Graph Computing and the interactive computing are showed in table.\nIn this session, we learned mechanism, principle of distributed graph computing a", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A computing model summarized in the session, covering its computing pattern, representative products, storage system, and key technologies.</span>}</div>"}, {"index": 18, "class": "CONCEPT", "text": "Graph Computing", "color": "#D2E3FC", "startPos": 16141, "endPos": 16156, "beforeText": "mputing platform and Key technologies \nof Batch processing, stream computing, Massively Parallel Processing for structured data, in memory computing, ", "extractionText": "Graph Computing", "afterText": " and the interactive computing are showed in table.\nIn this session, we learned mechanism, principle of distributed graph computing and we summarized ", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A computing model summarized in the session, covering its computing pattern, representative products, storage system, and key technologies.</span>}</div>"}, {"index": 19, "class": "CONCEPT", "text": "Interactive computing", "color": "#D2E3FC", "startPos": 16165, "endPos": 16186, "beforeText": " technologies \nof Batch processing, stream computing, Massively Parallel Processing for structured data, in memory computing, Graph Computing and the ", "extractionText": "interactive computing", "afterText": " are showed in table.\nIn this session, we learned mechanism, principle of distributed graph computing and we summarized computing model. Thank you for", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A computing model summarized in the session, covering its computing pattern, representative products, storage system, and key technologies.</span>}</div>"}, {"index": 20, "class": "CONCEPT", "text": "Distributed graph computing", "color": "#D2E3FC", "startPos": 16260, "endPos": 16287, "beforeText": "red data, in memory computing, Graph Computing and the interactive computing are showed in table.\nIn this session, we learned mechanism, principle of ", "extractionText": "distributed graph computing", "afterText": " and we summarized computing model. Thank you for your attention, if you have any question, feel free to contact me.", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A computing model whose mechanism and principles were covered in the session.</span>}</div>"}];
        let currentIndex = 0;
        let isPlaying = false;
        let animationInterval = null;
        let animationSpeed = 1.0;

        function updateDisplay() {
          const extraction = extractions[currentIndex];
          if (!extraction) return;

          document.getElementById('attributesContainer').innerHTML = extraction.attributesHtml;
          document.getElementById('entityInfo').textContent = (currentIndex + 1) + '/' + extractions.length;
          document.getElementById('posInfo').textContent = '[' + extraction.startPos + '-' + extraction.endPos + ']';
          document.getElementById('progressSlider').value = currentIndex;

          const playBtn = document.querySelector('.lx-control-btn');
          if (playBtn) playBtn.textContent = isPlaying ? '⏸ Pause' : '▶️ Play';

          const prevHighlight = document.querySelector('.lx-text-window .lx-current-highlight');
          if (prevHighlight) prevHighlight.classList.remove('lx-current-highlight');
          const currentSpan = document.querySelector('.lx-text-window span[data-idx="' + currentIndex + '"]');
          if (currentSpan) {
            currentSpan.classList.add('lx-current-highlight');
            currentSpan.scrollIntoView({block: 'center', behavior: 'smooth'});
          }
        }

        function nextExtraction() {
          currentIndex = (currentIndex + 1) % extractions.length;
          updateDisplay();
        }

        function prevExtraction() {
          currentIndex = (currentIndex - 1 + extractions.length) % extractions.length;
          updateDisplay();
        }

        function jumpToExtraction(index) {
          currentIndex = parseInt(index);
          updateDisplay();
        }

        function playPause() {
          if (isPlaying) {
            clearInterval(animationInterval);
            isPlaying = false;
          } else {
            animationInterval = setInterval(nextExtraction, animationSpeed * 1000);
            isPlaying = true;
          }
          updateDisplay();
        }

        window.playPause = playPause;
        window.nextExtraction = nextExtraction;
        window.prevExtraction = prevExtraction;
        window.jumpToExtraction = jumpToExtraction;

        updateDisplay();
      })();
    </script>