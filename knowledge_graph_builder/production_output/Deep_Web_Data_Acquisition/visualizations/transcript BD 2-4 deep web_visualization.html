<style>
.lx-highlight { position: relative; border-radius:3px; padding:1px 2px;}
.lx-highlight .lx-tooltip {
  visibility: hidden;
  opacity: 0;
  transition: opacity 0.2s ease-in-out;
  background: #333;
  color: #fff;
  text-align: left;
  border-radius: 4px;
  padding: 6px 8px;
  position: absolute;
  z-index: 1000;
  bottom: 125%;
  left: 50%;
  transform: translateX(-50%);
  font-size: 12px;
  max-width: 240px;
  white-space: normal;
  box-shadow: 0 2px 6px rgba(0,0,0,0.3);
}
.lx-highlight:hover .lx-tooltip { visibility: visible; opacity:1; }
.lx-animated-wrapper { max-width: 100%; font-family: Arial, sans-serif; }
.lx-controls {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 8px;
  padding: 12px; margin-bottom: 16px;
}
.lx-button-row {
  display: flex; justify-content: center; gap: 8px; margin-bottom: 12px;
}
.lx-control-btn {
  background: #4285f4; color: white; border: none; border-radius: 4px;
  padding: 8px 16px; cursor: pointer; font-size: 13px; font-weight: 500;
  transition: background-color 0.2s;
}
.lx-control-btn:hover { background: #3367d6; }
.lx-progress-container {
  margin-bottom: 8px;
}
.lx-progress-slider {
  width: 100%; margin: 0; appearance: none; height: 6px;
  background: #ddd; border-radius: 3px; outline: none;
}
.lx-progress-slider::-webkit-slider-thumb {
  appearance: none; width: 18px; height: 18px; background: #4285f4;
  border-radius: 50%; cursor: pointer;
}
.lx-progress-slider::-moz-range-thumb {
  width: 18px; height: 18px; background: #4285f4; border-radius: 50%;
  cursor: pointer; border: none;
}
.lx-status-text {
  text-align: center; font-size: 12px; color: #666; margin-top: 4px;
}
.lx-text-window {
  font-family: monospace; white-space: pre-wrap; border: 1px solid #90caf9;
  padding: 12px; max-height: 260px; overflow-y: auto; margin-bottom: 12px;
  line-height: 1.6;
}
.lx-attributes-panel {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 6px;
  padding: 8px 10px; margin-top: 8px; font-size: 13px;
}
.lx-current-highlight {
  border-bottom: 4px solid #ff4444;
  font-weight: bold;
  animation: lx-pulse 1s ease-in-out;
}
@keyframes lx-pulse {
  0% { text-decoration-color: #ff4444; }
  50% { text-decoration-color: #ff0000; }
  100% { text-decoration-color: #ff4444; }
}
.lx-legend {
  font-size: 12px; margin-bottom: 8px;
  padding-bottom: 8px; border-bottom: 1px solid #e0e0e0;
}
.lx-label {
  display: inline-block;
  padding: 2px 4px;
  border-radius: 3px;
  margin-right: 4px;
  color: #000;
}
.lx-attr-key {
  font-weight: 600;
  color: #1565c0;
  letter-spacing: 0.3px;
}
.lx-attr-value {
  font-weight: 400;
  opacity: 0.85;
  letter-spacing: 0.2px;
}

/* Add optimizations with larger fonts and better readability for GIFs */
.lx-gif-optimized .lx-text-window { font-size: 16px; line-height: 1.8; }
.lx-gif-optimized .lx-attributes-panel { font-size: 15px; }
.lx-gif-optimized .lx-current-highlight { text-decoration-thickness: 4px; }
</style>
    <div class="lx-animated-wrapper lx-gif-optimized">
      <div class="lx-attributes-panel">
        <div class="lx-legend">Highlights Legend: <span class="lx-label" style="background-color:#D2E3FC;">CONCEPT</span> <span class="lx-label" style="background-color:#C8E6C9;">KEYWORDS</span> <span class="lx-label" style="background-color:#FEF0C3;">SUMMARY</span> <span class="lx-label" style="background-color:#F9DEDC;">TOPIC</span></div>
        <div id="attributesContainer"></div>
      </div>
      <div class="lx-text-window" id="textWindow">
        Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering
School of Computer Science, in Beijing Institute of Technology, in this session, we will discuss
<span class="lx-highlight lx-current-highlight" data-idx="0" style="background-color:#F9DEDC;">Deep web data acquisition</span>.
As <span class="lx-highlight" data-idx="1" style="background-color:#FEF0C3;">the</span> picture shows, the content of all internet information could be divided into 3 parts, <span class="lx-highlight" data-idx="2" style="background-color:#D2E3FC;">surface web</span>, <span class="lx-highlight" data-idx="3" style="background-color:#C8E6C9;">deep web and <span class="lx-highlight" data-idx="4" style="background-color:#D2E3FC;">dark web</span>.
on top is the surface web, Surface web, its content is basically unstructured HTML information, anyone can access it through the Internet.
In the middle is deep web,
Deep Web concept is created by Bright Planet in 2000, which is Used to express those websites whose information content is stored in the search database and only responds to direct queries; 
the content is mostly structured DB information. The information could be academic records, financial records, legal documents, government records and science reports.
At Bottom is the dark web, which mostly is the illegal information, related to drugs, weapons and others.
In the dark web part, besides the onion is the “<span class="lx-highlight" data-idx="5" style="background-color:#D2E3FC;">TOR</span>”.
Tor is the acronym for &quot;the onion router&quot;, and users can communicate anonymously on the Internet through Tor. 
The project was originally sponsored by the U.S. Naval Research Laboratory to hide the whereabouts of government personnel collecting intelligence online;
Tor is designed and provided for free by Tor Project, a non-profit organization, and has been adopted by freedom advocates and criminals. 
Tor sends chat messages, Google searches, purchase orders or emails through multiple computers in a circuitous manner, disguising the activities of Internet users like an onion wraps its core. 
Information transmission is encrypted at every step, and there is no way to know where the user is , Location and destination of information transfer. 
Volunteers around the world provide about 5,000 computers as nodes on the transmission path to cover up new pages or chat requests. 
The Tor Project calls these points <span class="lx-highlight" data-idx="6" style="background-color:#D2E3FC;">relay nodes</span>.
American Internet experts and librarians Chris Sher-man and Gary Price defined:
&quot;Available on the Internet, but those web pages, files or other high-quality, authoritative information that traditional search engines are unable to index due to technical limitations or are unwilling to index after careful consideration&quot;
Features of Deep Web Information
Highly related to information needs, markets and fields；
Fastest growing new type of information on the Internet. 
more specialized and deeper than the traditional surface web. 
The full value of the deep web content is 1000-2000 times that of the surface web.
More than half is stored in thematic databases; 
% of the information on the deep web is publicly available without payment
Deep web content includes
Pages that are not referred to by search engines due to lack of directed links .
Non-web files accessible on the web, such as picture files, Pdf and word documents, etc.
A dynamic page obtained by querying the back-end online database by filling in the form.
Content that requires registration or other restrictions to access.
Let ‘s compare deep web and the content searched by search engine,
) from interface, deep web content is Dynamic Web extracted from  databases, which usually have complex interfaces, and each query interface supports queries on several attributes.
search engine, the content is searched by keyword
) the results from deep web are mainly structured data, but the results from search engine is just web pages.
) in the aspect of how to sort search results, <span class="lx-highlight" data-idx="7" style="background-color:#D2E3FC;">deep web</span> sort search results according to the result of a certain attribute value in Deep Web, search engine sort search results by Similarity between search results and submitted query.
Now let’s learn how to collect the deep web data. Deep web data collection task includes 2 stages, 1) is query interface identification. 2 fill in the form automatically, then execute the query.
For a specific website, you can obtain as much deep web data as possible by manually writing or giving crawler scripts with the assistance of wrappers and generators. 
However, this method not only requires a lot of manpower, but also its scalability is poor because of its Specific website and query interface,.
Construct a general deep web crawler in order to crawl the deep web data of many sites at once.
）Query interface recognition: Use a variety of methods including visual layout to parse HTML forms or perform syntax analysis on HTML forms to automatically discover deep web data resources;
) Add text similarity heuristic rules to associate HTML forms with specific fields to realize automatic filling</span> of forms;
) By constructing page classifiers and form classifiers to automatically find deep web databases related to tasks
) Try to Automatically fill in the form.  
1Based on domain knowledge: Use heuristic rules to associate the field of the form with the domain, thereby inputting parameters related to the domain concept.
2 Domain-independent detection: Iteratively obtain query keywords from query results based on sampling, to obtain as many query results as possible with fewer queries
Let’ s summarize the deep web data acquisition topic. 
In this session we learned 
The concept of the deep web
Features of Deep Web Information
What is Deep web content
We Compared the results of Deep Web Data Collection and Traditional Search Engine Query
We analyzed Deep web data collection task, and we understand the solution to crawl the deep web content.
In this session we learned the bigdata resources, internal and external data.
thank you for your attention, if you have any question, feel free to contact me.
      </div>
      <div class="lx-controls">
        <div class="lx-button-row">
          <button class="lx-control-btn" onclick="playPause()">▶️ Play</button>
          <button class="lx-control-btn" onclick="prevExtraction()">⏮ Previous</button>
          <button class="lx-control-btn" onclick="nextExtraction()">⏭ Next</button>
        </div>
        <div class="lx-progress-container">
          <input type="range" id="progressSlider" class="lx-progress-slider"
                 min="0" max="7" value="0"
                 onchange="jumpToExtraction(this.value)">
        </div>
        <div class="lx-status-text">
          Entity <span id="entityInfo">1/8</span> |
          Pos <span id="posInfo">[188-213]</span>
        </div>
      </div>
    </div>

    <script>
      (function() {
        const extractions = [{"index": 0, "class": "TOPIC", "text": "Deep Web Data Acquisition", "color": "#F9DEDC", "startPos": 188, "endPos": 213, "beforeText": " Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session, we will discuss\n", "extractionText": "Deep web data acquisition", "afterText": ".\nAs the picture shows, the content of all internet information could be divided into 3 parts, surface web, deep web and dark web.\non top is the surfa", "attributesHtml": "<div><strong>class:</strong> TOPIC</div><div><strong>attributes:</strong> {}</div>"}, {"index": 1, "class": "SUMMARY", "text": "The internet's content is categorized into three parts: the surface web, deep web, and dark web. The surface web consists of unstructured HTML information accessible to anyone. The deep web contains structured information stored in databases that only responds to direct queries, including academic, financial, and government records. It is characterized as being highly specialized, valuable, and the fastest-growing type of information online. The dark web primarily hosts illegal content and is accessed using tools like Tor ('the onion router'), which anonymizes users by routing encrypted traffic through a volunteer network of relay nodes. Deep web content includes unlinked pages, non-web files (like PDFs), dynamic pages generated from database queries, and restricted-access information. Unlike traditional search engines that return web pages ranked by similarity, the deep web provides structured data from complex interfaces. The process of collecting deep web data involves two main stages: identifying the query interface and automatically filling in the form to execute the query. This can be achieved with site-specific scripts or more scalable general-purpose crawlers that use techniques like visual layout analysis, page classifiers, and domain-based rules to discover and extract data.", "color": "#FEF0C3", "startPos": 218, "endPos": 221, "beforeText": " knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session, we will discuss\nDeep web data acquisition.\nAs ", "extractionText": "the", "afterText": " picture shows, the content of all internet information could be divided into 3 parts, surface web, deep web and dark web.\non top is the surface web, ", "attributesHtml": "<div><strong>class:</strong> SUMMARY</div><div><strong>attributes:</strong> {}</div>"}, {"index": 2, "class": "CONCEPT", "text": "Surface Web", "color": "#D2E3FC", "startPos": 308, "endPos": 319, "beforeText": "this session, we will discuss\nDeep web data acquisition.\nAs the picture shows, the content of all internet information could be divided into 3 parts, ", "extractionText": "surface web", "afterText": ", deep web and dark web.\non top is the surface web, Surface web, its content is basically unstructured HTML information, anyone can access it through ", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">Content that is basically unstructured HTML information, which anyone can access through the Internet.</span>}</div>"}, {"index": 3, "class": "KEYWORDS", "text": "deep web, surface web, dark web, Tor, data acquisition, web crawler, query interface, structured data, relay nodes, form filling", "color": "#C8E6C9", "startPos": 321, "endPos": 4600, "beforeText": " we will discuss\nDeep web data acquisition.\nAs the picture shows, the content of all internet information could be divided into 3 parts, surface web, ", "extractionText": "deep web and dark web.\non top is the surface web, Surface web, its content is basically unstructured HTML information, anyone can access it through the Internet.\nIn the middle is deep web,\nDeep Web concept is created by Bright Planet in 2000, which is Used to express those websites whose information content is stored in the search database and only responds to direct queries; \nthe content is mostly structured DB information. The information could be academic records, financial records, legal documents, government records and science reports.\nAt Bottom is the dark web, which mostly is the illegal information, related to drugs, weapons and others.\nIn the dark web part, besides the onion is the \u201cTOR\u201d.\nTor is the acronym for &quot;the onion router&quot;, and users can communicate anonymously on the Internet through Tor. \nThe project was originally sponsored by the U.S. Naval Research Laboratory to hide the whereabouts of government personnel collecting intelligence online;\nTor is designed and provided for free by Tor Project, a non-profit organization, and has been adopted by freedom advocates and criminals. \nTor sends chat messages, Google searches, purchase orders or emails through multiple computers in a circuitous manner, disguising the activities of Internet users like an onion wraps its core. \nInformation transmission is encrypted at every step, and there is no way to know where the user is , Location and destination of information transfer. \nVolunteers around the world provide about 5,000 computers as nodes on the transmission path to cover up new pages or chat requests. \nThe Tor Project calls these points relay nodes.\nAmerican Internet experts and librarians Chris Sher-man and Gary Price defined:\n&quot;Available on the Internet, but those web pages, files or other high-quality, authoritative information that traditional search engines are unable to index due to technical limitations or are unwilling to index after careful consideration&quot;\nFeatures of Deep Web Information\nHighly related to information needs, markets and fields\uff1b\nFastest growing new type of information on the Internet. \nmore specialized and deeper than the traditional surface web. \nThe full value of the deep web content is 1000-2000 times that of the surface web.\nMore than half is stored in thematic databases; \n% of the information on the deep web is publicly available without payment\nDeep web content includes\nPages that are not referred to by search engines due to lack of directed links .\nNon-web files accessible on the web, such as picture files, Pdf and word documents, etc.\nA dynamic page obtained by querying the back-end online database by filling in the form.\nContent that requires registration or other restrictions to access.\nLet \u2018s compare deep web and the content searched by search engine,\n) from interface, deep web content is Dynamic Web extracted from  databases, which usually have complex interfaces, and each query interface supports queries on several attributes.\nsearch engine, the content is searched by keyword\n) the results from deep web are mainly structured data, but the results from search engine is just web pages.\n) in the aspect of how to sort search results, deep web sort search results according to the result of a certain attribute value in Deep Web, search engine sort search results by Similarity between search results and submitted query.\nNow let\u2019s learn how to collect the deep web data. Deep web data collection task includes 2 stages, 1) is query interface identification. 2 fill in the form automatically, then execute the query.\nFor a specific website, you can obtain as much deep web data as possible by manually writing or giving crawler scripts with the assistance of wrappers and generators. \nHowever, this method not only requires a lot of manpower, but also its scalability is poor because of its Specific website and query interface,.\nConstruct a general deep web crawler in order to crawl the deep web data of many sites at once.\n\uff09Query interface recognition: Use a variety of methods including visual layout to parse HTML forms or perform syntax analysis on HTML forms to automatically discover deep web data resources;\n) Add text similarity heuristic rules to associate HTML forms with specific fields to realize automatic filling", "afterText": " of forms;\n) By constructing page classifiers and form classifiers to automatically find deep web databases related to tasks\n) Try to Automatically fi", "attributesHtml": "<div><strong>class:</strong> KEYWORDS</div><div><strong>attributes:</strong> {}</div>"}, {"index": 4, "class": "CONCEPT", "text": "Dark Web", "color": "#D2E3FC", "startPos": 334, "endPos": 342, "beforeText": "uss\nDeep web data acquisition.\nAs the picture shows, the content of all internet information could be divided into 3 parts, surface web, deep web and ", "extractionText": "dark web", "afterText": ".\non top is the surface web, Surface web, its content is basically unstructured HTML information, anyone can access it through the Internet.\nIn the mi", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">The part of the web that mostly contains illegal information, related to drugs, weapons, and other illicit activities.</span>}</div>"}, {"index": 5, "class": "CONCEPT", "text": "Tor", "color": "#D2E3FC", "startPos": 1023, "endPos": 1026, "beforeText": "ottom is the dark web, which mostly is the illegal information, related to drugs, weapons and others.\nIn the dark web part, besides the onion is the \u201c", "extractionText": "TOR", "afterText": "\u201d.\nTor is the acronym for &quot;the onion router&quot;, and users can communicate anonymously on the Internet through Tor. \nThe project was originally sponsored", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An acronym for &#x27;the onion router,&#x27; a system that allows users to communicate anonymously on the Internet by sending encrypted information through multiple computers (relay nodes) in a circuitous manner, disguising the user&#x27;s location and the information&#x27;s destination.</span>}</div>"}, {"index": 6, "class": "CONCEPT", "text": "Relay Nodes", "color": "#D2E3FC", "startPos": 1948, "endPos": 1959, "beforeText": "the world provide about 5,000 computers as nodes on the transmission path to cover up new pages or chat requests. \nThe Tor Project calls these points ", "extractionText": "relay nodes", "afterText": ".\nAmerican Internet experts and librarians Chris Sher-man and Gary Price defined:\n&quot;Available on the Internet, but those web pages, files or other high", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">Computers provided by volunteers around the world that act as nodes on the transmission path for Tor, used to cover up new pages or chat requests and anonymize user traffic.</span>}</div>"}, {"index": 7, "class": "CONCEPT", "text": "Deep Web", "color": "#D2E3FC", "startPos": 3507, "endPos": 3515, "beforeText": "esults from deep web are mainly structured data, but the results from search engine is just web pages.\n) in the aspect of how to sort search results, ", "extractionText": "deep web", "afterText": " sort search results according to the result of a certain attribute value in Deep Web, search engine sort search results by Similarity between search ", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">Websites whose information content is stored in a search database and only responds to direct queries; the content is mostly structured database information.</span>}</div>"}];
        let currentIndex = 0;
        let isPlaying = false;
        let animationInterval = null;
        let animationSpeed = 1.0;

        function updateDisplay() {
          const extraction = extractions[currentIndex];
          if (!extraction) return;

          document.getElementById('attributesContainer').innerHTML = extraction.attributesHtml;
          document.getElementById('entityInfo').textContent = (currentIndex + 1) + '/' + extractions.length;
          document.getElementById('posInfo').textContent = '[' + extraction.startPos + '-' + extraction.endPos + ']';
          document.getElementById('progressSlider').value = currentIndex;

          const playBtn = document.querySelector('.lx-control-btn');
          if (playBtn) playBtn.textContent = isPlaying ? '⏸ Pause' : '▶️ Play';

          const prevHighlight = document.querySelector('.lx-text-window .lx-current-highlight');
          if (prevHighlight) prevHighlight.classList.remove('lx-current-highlight');
          const currentSpan = document.querySelector('.lx-text-window span[data-idx="' + currentIndex + '"]');
          if (currentSpan) {
            currentSpan.classList.add('lx-current-highlight');
            currentSpan.scrollIntoView({block: 'center', behavior: 'smooth'});
          }
        }

        function nextExtraction() {
          currentIndex = (currentIndex + 1) % extractions.length;
          updateDisplay();
        }

        function prevExtraction() {
          currentIndex = (currentIndex - 1 + extractions.length) % extractions.length;
          updateDisplay();
        }

        function jumpToExtraction(index) {
          currentIndex = parseInt(index);
          updateDisplay();
        }

        function playPause() {
          if (isPlaying) {
            clearInterval(animationInterval);
            isPlaying = false;
          } else {
            animationInterval = setInterval(nextExtraction, animationSpeed * 1000);
            isPlaying = true;
          }
          updateDisplay();
        }

        window.playPause = playPause;
        window.nextExtraction = nextExtraction;
        window.prevExtraction = prevExtraction;
        window.jumpToExtraction = jumpToExtraction;

        updateDisplay();
      })();
    </script>