{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "Deep Web Data Acquisition", "char_interval": {"start_pos": 188, "end_pos": 213}, "alignment_status": "match_exact", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "The internet's content is categorized into three parts: the surface web, deep web, and dark web. The surface web consists of unstructured HTML information accessible to anyone. The deep web contains structured information stored in databases that only responds to direct queries, including academic, financial, and government records. It is characterized as being highly specialized, valuable, and the fastest-growing type of information online. The dark web primarily hosts illegal content and is accessed using tools like Tor ('the onion router'), which anonymizes users by routing encrypted traffic through a volunteer network of relay nodes. Deep web content includes unlinked pages, non-web files (like PDFs), dynamic pages generated from database queries, and restricted-access information. Unlike traditional search engines that return web pages ranked by similarity, the deep web provides structured data from complex interfaces. The process of collecting deep web data involves two main stages: identifying the query interface and automatically filling in the form to execute the query. This can be achieved with site-specific scripts or more scalable general-purpose crawlers that use techniques like visual layout analysis, page classifiers, and domain-based rules to discover and extract data.", "char_interval": {"start_pos": 218, "end_pos": 221}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "deep web, surface web, dark web, Tor, data acquisition, web crawler, query interface, structured data, relay nodes, form filling", "char_interval": {"start_pos": 321, "end_pos": 4600}, "alignment_status": "match_fuzzy", "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Surface Web", "char_interval": {"start_pos": 308, "end_pos": 319}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "Content that is basically unstructured HTML information, which anyone can access through the Internet."}}, {"extraction_class": "CONCEPT", "extraction_text": "Deep Web", "char_interval": {"start_pos": 3507, "end_pos": 3515}, "alignment_status": "match_exact", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "Websites whose information content is stored in a search database and only responds to direct queries; the content is mostly structured database information."}}, {"extraction_class": "CONCEPT", "extraction_text": "Dark Web", "char_interval": {"start_pos": 334, "end_pos": 342}, "alignment_status": "match_fuzzy", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "The part of the web that mostly contains illegal information, related to drugs, weapons, and other illicit activities."}}, {"extraction_class": "CONCEPT", "extraction_text": "Tor", "char_interval": {"start_pos": 1023, "end_pos": 1026}, "alignment_status": "match_fuzzy", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "An acronym for 'the onion router,' a system that allows users to communicate anonymously on the Internet by sending encrypted information through multiple computers (relay nodes) in a circuitous manner, disguising the user's location and the information's destination."}}, {"extraction_class": "CONCEPT", "extraction_text": "Relay Nodes", "char_interval": {"start_pos": 1948, "end_pos": 1959}, "alignment_status": "match_fuzzy", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "Computers provided by volunteers around the world that act as nodes on the transmission path for Tor, used to cover up new pages or chat requests and anonymize user traffic."}}], "text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session, we will discuss\nDeep web data acquisition.\nAs the picture shows, the content of all internet information could be divided into 3 parts, surface web, deep web and dark web.\non top is the surface web, Surface web, its content is basically unstructured HTML information, anyone can access it through the Internet.\nIn the middle is deep web,\nDeep Web concept is created by Bright Planet in 2000, which is Used to express those websites whose information content is stored in the search database and only responds to direct queries; \nthe content is mostly structured DB information. The information could be academic records, financial records, legal documents, government records and science reports.\nAt Bottom is the dark web, which mostly is the illegal information, related to drugs, weapons and others.\nIn the dark web part, besides the onion is the “TOR”.\nTor is the acronym for \"the onion router\", and users can communicate anonymously on the Internet through Tor. \nThe project was originally sponsored by the U.S. Naval Research Laboratory to hide the whereabouts of government personnel collecting intelligence online;\nTor is designed and provided for free by Tor Project, a non-profit organization, and has been adopted by freedom advocates and criminals. \nTor sends chat messages, Google searches, purchase orders or emails through multiple computers in a circuitous manner, disguising the activities of Internet users like an onion wraps its core. \nInformation transmission is encrypted at every step, and there is no way to know where the user is , Location and destination of information transfer. \nVolunteers around the world provide about 5,000 computers as nodes on the transmission path to cover up new pages or chat requests. \nThe Tor Project calls these points relay nodes.\nAmerican Internet experts and librarians Chris Sher-man and Gary Price defined:\n\"Available on the Internet, but those web pages, files or other high-quality, authoritative information that traditional search engines are unable to index due to technical limitations or are unwilling to index after careful consideration\"\nFeatures of Deep Web Information\nHighly related to information needs, markets and fields；\nFastest growing new type of information on the Internet. \nmore specialized and deeper than the traditional surface web. \nThe full value of the deep web content is 1000-2000 times that of the surface web.\nMore than half is stored in thematic databases; \n% of the information on the deep web is publicly available without payment\nDeep web content includes\nPages that are not referred to by search engines due to lack of directed links .\nNon-web files accessible on the web, such as picture files, Pdf and word documents, etc.\nA dynamic page obtained by querying the back-end online database by filling in the form.\nContent that requires registration or other restrictions to access.\nLet ‘s compare deep web and the content searched by search engine,\n) from interface, deep web content is Dynamic Web extracted from  databases, which usually have complex interfaces, and each query interface supports queries on several attributes.\nsearch engine, the content is searched by keyword\n) the results from deep web are mainly structured data, but the results from search engine is just web pages.\n) in the aspect of how to sort search results, deep web sort search results according to the result of a certain attribute value in Deep Web, search engine sort search results by Similarity between search results and submitted query.\nNow let’s learn how to collect the deep web data. Deep web data collection task includes 2 stages, 1) is query interface identification. 2 fill in the form automatically, then execute the query.\nFor a specific website, you can obtain as much deep web data as possible by manually writing or giving crawler scripts with the assistance of wrappers and generators. \nHowever, this method not only requires a lot of manpower, but also its scalability is poor because of its Specific website and query interface,.\nConstruct a general deep web crawler in order to crawl the deep web data of many sites at once.\n）Query interface recognition: Use a variety of methods including visual layout to parse HTML forms or perform syntax analysis on HTML forms to automatically discover deep web data resources;\n) Add text similarity heuristic rules to associate HTML forms with specific fields to realize automatic filling of forms;\n) By constructing page classifiers and form classifiers to automatically find deep web databases related to tasks\n) Try to Automatically fill in the form.  \n1Based on domain knowledge: Use heuristic rules to associate the field of the form with the domain, thereby inputting parameters related to the domain concept.\n2 Domain-independent detection: Iteratively obtain query keywords from query results based on sampling, to obtain as many query results as possible with fewer queries\nLet’ s summarize the deep web data acquisition topic. \nIn this session we learned \nThe concept of the deep web\nFeatures of Deep Web Information\nWhat is Deep web content\nWe Compared the results of Deep Web Data Collection and Traditional Search Engine Query\nWe analyzed Deep web data collection task, and we understand the solution to crawl the deep web content.\nIn this session we learned the bigdata resources, internal and external data.\nthank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_692965f8"}
