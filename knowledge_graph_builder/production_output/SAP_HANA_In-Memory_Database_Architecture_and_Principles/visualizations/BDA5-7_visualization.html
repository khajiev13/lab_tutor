<style>
.lx-highlight { position: relative; border-radius:3px; padding:1px 2px;}
.lx-highlight .lx-tooltip {
  visibility: hidden;
  opacity: 0;
  transition: opacity 0.2s ease-in-out;
  background: #333;
  color: #fff;
  text-align: left;
  border-radius: 4px;
  padding: 6px 8px;
  position: absolute;
  z-index: 1000;
  bottom: 125%;
  left: 50%;
  transform: translateX(-50%);
  font-size: 12px;
  max-width: 240px;
  white-space: normal;
  box-shadow: 0 2px 6px rgba(0,0,0,0.3);
}
.lx-highlight:hover .lx-tooltip { visibility: visible; opacity:1; }
.lx-animated-wrapper { max-width: 100%; font-family: Arial, sans-serif; }
.lx-controls {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 8px;
  padding: 12px; margin-bottom: 16px;
}
.lx-button-row {
  display: flex; justify-content: center; gap: 8px; margin-bottom: 12px;
}
.lx-control-btn {
  background: #4285f4; color: white; border: none; border-radius: 4px;
  padding: 8px 16px; cursor: pointer; font-size: 13px; font-weight: 500;
  transition: background-color 0.2s;
}
.lx-control-btn:hover { background: #3367d6; }
.lx-progress-container {
  margin-bottom: 8px;
}
.lx-progress-slider {
  width: 100%; margin: 0; appearance: none; height: 6px;
  background: #ddd; border-radius: 3px; outline: none;
}
.lx-progress-slider::-webkit-slider-thumb {
  appearance: none; width: 18px; height: 18px; background: #4285f4;
  border-radius: 50%; cursor: pointer;
}
.lx-progress-slider::-moz-range-thumb {
  width: 18px; height: 18px; background: #4285f4; border-radius: 50%;
  cursor: pointer; border: none;
}
.lx-status-text {
  text-align: center; font-size: 12px; color: #666; margin-top: 4px;
}
.lx-text-window {
  font-family: monospace; white-space: pre-wrap; border: 1px solid #90caf9;
  padding: 12px; max-height: 260px; overflow-y: auto; margin-bottom: 12px;
  line-height: 1.6;
}
.lx-attributes-panel {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 6px;
  padding: 8px 10px; margin-top: 8px; font-size: 13px;
}
.lx-current-highlight {
  border-bottom: 4px solid #ff4444;
  font-weight: bold;
  animation: lx-pulse 1s ease-in-out;
}
@keyframes lx-pulse {
  0% { text-decoration-color: #ff4444; }
  50% { text-decoration-color: #ff0000; }
  100% { text-decoration-color: #ff4444; }
}
.lx-legend {
  font-size: 12px; margin-bottom: 8px;
  padding-bottom: 8px; border-bottom: 1px solid #e0e0e0;
}
.lx-label {
  display: inline-block;
  padding: 2px 4px;
  border-radius: 3px;
  margin-right: 4px;
  color: #000;
}
.lx-attr-key {
  font-weight: 600;
  color: #1565c0;
  letter-spacing: 0.3px;
}
.lx-attr-value {
  font-weight: 400;
  opacity: 0.85;
  letter-spacing: 0.2px;
}

/* Add optimizations with larger fonts and better readability for GIFs */
.lx-gif-optimized .lx-text-window { font-size: 16px; line-height: 1.8; }
.lx-gif-optimized .lx-attributes-panel { font-size: 15px; }
.lx-gif-optimized .lx-current-highlight { text-decoration-thickness: 4px; }
</style>
    <div class="lx-animated-wrapper lx-gif-optimized">
      <div class="lx-attributes-panel">
        <div class="lx-legend">Highlights Legend: <span class="lx-label" style="background-color:#D2E3FC;">CONCEPT</span> <span class="lx-label" style="background-color:#C8E6C9;">KEYWORDS</span> <span class="lx-label" style="background-color:#FEF0C3;">SUMMARY</span> <span class="lx-label" style="background-color:#F9DEDC;">TOPIC</span></div>
        <div id="attributesContainer"></div>
      </div>
      <div class="lx-text-window" id="textWindow">
        Hello everyone, I am Haiying Che, from  Institute of Data Science and knowledge Engineering
School of Computer Science, in Beijing Institute of Technology , in this session we discuss In memory Database.
From the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing platform and engine layer, etc.
Computing models are the way that different kinds of big data is processed in different scenarios, We have learned the batch processing, stream computing, Large-scale concurrent processing model for structured data, and in this session we discuss In-memory database.
There have been multiple advances in the industry in both hardware and software architecture, which makes memory-based computing more feasible today than in the past, as outlined in the diagram below.
In a nutshell, the availability of new classes of hardware with the support of 64bit CPU can now support 2TB on a single device. In addition, the advances in software architecture and solutions toward distributed architecture and cloud make it easier to utilize these new hardware capabilities.
<span class="lx-highlight lx-current-highlight" data-idx="0" style="background-color:#D2E3FC;">In-Memory Computing</span>
In many ways, In-Memory Computing is a close relative of <span class="lx-highlight" data-idx="1" style="background-color:#FEF0C3;">In-Memory Databases</span>. As with many databases, it was designed to enable all the data management aspects that are often expected from traditional databases, such as queries and transactions, with the difference that the data is managed on RAM devices and not disks and thus comes with potentially x1000 better performance and latency according to various benchmarks.
The main differences between the traditional in-memory databases and in-memory-based-computing are that In-Memory-Computing is:
）Designed for distributed and elastic environments
）Designed for In-Memory data processing
<span class="lx-highlight" data-idx="2" style="background-color:#F9DEDC;"><span class="lx-highlight" data-idx="3" style="background-color:#D2E3FC;">HANA</span> combine the hardware advantages and software advantages to make the <span class="lx-highlight" data-idx="4" style="background-color:#C8E6C9;">in-memory Database</span> possible. 
The development of the hardware technology, multi-core architecture, parallel scaling across blades, 64-bit address space, 2 TB in current server board, 25 GB/s data throughput. 
All these made the cost-performance ratio rapidly declining, which means the advanced speed processing hardware become cheaper and affordable.
Hana database also includes some advanced software technology and special methods, which include combining <span class="lx-highlight" data-idx="5" style="background-color:#D2E3FC;">row based and <span class="lx-highlight" data-idx="6" style="background-color:#D2E3FC;">column-based store</span></span>, compression, partitioning, No aggregation tables, <span class="lx-highlight" data-idx="7" style="background-color:#D2E3FC;">insert</span> only and on-the-fly extensibility.
） row based store is designed to support transactions processing, OLTP and the column based store to support analysis processing, such as OLAP.
）because the memory size cannot be as big as the hard disk, we must try any method to reduce the size of the DB, HANA used a lot of compression methods.
） in order to increase the DB performance, the database can be partitioned to achieve the parallelization and improve the performance.
） For eliminating the aggregation tables, normally for data analysis, the data warehouse is built to collect the relevant tables data from multiple data sources and get some aggregations to facilitate the further analysis. 
But in HANA， it doesn’t need the aggregation table , the aggregation in memory can be done quickly, no need to do it in advance, on the fly aggregation performance will not be affected, and the aggregation table made the redundant data, which cost the memory space.
） And what are the benefits of the insert only approach?
Well, since the main store is compressed (which is good for reading performance) changes would be expensive. Just imagine you want to make a change in a compressed text file. 
You need to uncompress, make the change and compress again. For this reason data is only appended to the uncompressed delta store where change operations are much cheaper. 
The merge moves the data from delta to main and thus compresses only from time to time and not with every change. 
Setting the deletion flag is based on rowids and works for compressed and uncompressed data.
In the column store HANA will only insert data. That is whenever you update or delete data HANA will
just insert. For performance reasons, the column store is divided in a main and a delta part. New data is going
to the delta and will be merged frequently to the main. Changing statements work as follows:
INSERT:
An INSERT statement will just insert a new record in the delta. The merge process will move the record from
delta to main.
DELETE:
A DELETE statement will select the record and mark it as invalid by setting a flag (for main or delta). The mergeprocess will delete the record from memory once there is no open transaction active for it anymore.
UPDATE:
An UPDATE statement will insert a new version of the record. The merge process will move the latest version from
delta to main. Old versions will be deleted once there is no open transaction active for them anymore. The update
therefore, consists internally of an insert + delete.
The following picture illustrates this principle</span> schematically. Please note that this picture does not reflect the real
storage in column store. You can see a deleted record in main (id 7) and 2 old versions (marked in grey) of
records (id 8 and 14) where the name has been updated, which lead to an insert of a new version of the records.
HANA is an in-memory database. But what exactly do we mean by that? Don&#x27;t all databases use memory? Yes, but with a big difference.
Other databases store data in physical pages on hard drives, and move &quot;copies&quot; of some of the data into memory; 
this is called <span class="lx-highlight" data-idx="8" style="background-color:#D2E3FC;">caching</span>. When data is missing from the memory cache, it must be fetched from the disk, which takes a very long time, with a minimal latency of 10,000 micro-seconds, holding up the queries and calculations. 
Not only that, since the data in memory is a copy of the disk, it is stored in a way to reflect the disk-structure, in the form of linked pages, sectors, etc., which adds additional overhead when fetching the data.
HANA, on the other hand stores the data directly in memory, optimized not for how the data is stored on disk, but optimized for efficient access, and as we shall see, highly compressed, using the most efficient memory representations that computer scientists can come up with.
Even in the event of a cache-miss, it only takes 0.1 micro-seconds to fetch the missing data from DRAM. That is 100,000 times faster, and this makes all the difference. 
Note that when a transaction changes a piece of the data, HANA logs a record of that change, so that it can be recovered in the event of a crash. 
There have been two industry trends that enable HANA performance. 
The first trend is the astonishing way that memory prices have gone down in the past several years. 
What was unimaginable only a few years ago, has become totally affordable today, as you can see from this graph, which shows the price of a Gigabyte of DRAM over time.
Notice how little time it took for a 10X reduction from $100/GB to $10/GB.
Today, a Terabyte of memory costs much less than ten thousand dollars, and computer makers are now putting together servers with four, and soon up to 12 terabytes of memory in a single system.
The second trend is the way compute power has gone up. A little over a decade ago, we would buy a computer with a single processor. 
Then, several years later, Intel introduced dual-core processors, followed, not long ago, with 64-bit processors with eight, dual-threaded, cores. 
The first servers for HANA would have as many as four such processors. Now, we are seeing Intel Sandy Bridge processors with 12 cores, and servers with up to 16 processors. 
That is not just a 200X performance jump in a decade, this really heralds a challenge for modern software – in-memory now also means that the software must be written to leverage the multi-core architecture of modern processors and multi-processor servers, and even scale out beyond.
Disk-centric computing is also a major factor that forces separation of transactional and analytical workloads. 
Specialized data warehouses for reporting and analytics required the moving, transformation and pre-processing of transactional data.
This introduces a huge complexity: sometimes an enterprise may hold three different copies of the same data…
Leveraging the power of in-memory computing allows HANA to bring OLTP, transaction processing, and OLAP, data analytics, back together in one database. 
HANA reverses the industry trend for separated, specialized analytics databases.
The result is a significant landscape simplification.
This image is a simplified architectural picture of SAP HANA.
HANA is certified and delivered on powerful enterprise servers from partners like IBM, HP, Fujitsu, Cisco, Dell and others. 
These highly optimized servers scale up to 192 cores, and managed by a Linux operating system from partners like SUSE and Red Hat. 
The HANA software is carefully optimized for the hardware it runs on, and takes advantage of the massive parallelism and memory architecture of the modern processing cores on which it runs.
A variety of storage systems can be used for high availability and backup support, including solid state or flash disks.
HANA is fully compatible with other database systems, and supports SQL, JDBC, ODBC and analytical data source interfaces like MDX-Multi Dimensional Expressions. 
This means that existing applications can use HANA without any modifications. In addition, HANA supports web-based interfaces which we will discuss later in this course.
HANA was designed from the ground up to be massively parallel at every level of operation, one of the main reasons why it performs so well. 
The first level is simple: multiple concurrent users and queries are allocated their separate compute resources. 
The HANA optimizer then creates an execution plan, that parallelizes the operations within a query. 
For instance, I can scan two columns at the same time on different processing cores. 
And by partitioning the data, I can parallelize such operations even more, even over multiple servers. 
For instance, I can sum table A over as four separate concurrent aggregations, and finally sum the four results together.
And by using processor techniques like hyper-threading, we get even more concurrency.
And all of that is internal to HANA; the applications are not aware of this, except that they enjoy better performance!
Here is another very important attribute of HANA: the way that data is stored in memory. 
Traditionally, databases store data in rows. One reason is that this is much more convenient when the database is stored on a disk: you simply write a new row to the end of the file.
HANA, on the other hand, stores its tables in columnar order. 
This means that it is much more efficient to perform an aggregation (sum of sales, for instance), because all the relevant data is in consecutive memory locations, and most values will be pre-fetched by the processor. 
Not only this, but by storing data in columns like this, we can sort and compress it much more efficiently than with conventional databases.
In traditional databases, it is often impossible to perform data analytics on the raw, original transaction data. 
Therefore, modeling experts are forced to introduce extra tables with pre-calculated summations, average, etc., to increase analytic performance, giving up detail. 
For instance, we could sum sales by country in the table above, resulting in a another, smaller table, which is faster to access for generating reports. 
These extra tables are called “<span class="lx-highlight" data-idx="9" style="background-color:#D2E3FC;">materialized aggregations</span>”. 
The trouble with these aggregations is that they make applications very complicated. 
Every transaction also requires the subsequent recalculation and updating of all affected aggregation tables, which may slow operations down significantly. 
Another approach, updating the aggregates periodically, means that reports are often not up to date, or don’t match the transaction data.
But HANA can aggregate massive data sets with high performance, over 15 million aggregations per second per core, so there is often really no need for aggregates anymore.
This immediately yields: simpler data models which fewer tables; no loss of detail; Much simpler application logic, with much fewer write-locks and logs; better up-to-datedness
All of this improves the performance dramatically. 
This is one of the reasons why customers report hundreds of times of performance improvement.
Business intelligence and data analytics often use normalized data models, referred to as <span class="lx-highlight" data-idx="10" style="background-color:#D2E3FC;">star-schema</span>. 
This involves holding all the fact-data in one large table, with references to other information in separate tables, called dimensions. 
For instance, we could have all our sales data in the central fact table, and details like country, and product in dimension tables. 
HANA has a specially optimized execution engine for these types of models, which often speeds up operations on such tables ten times or more, and contributes greatly to the overall performance impact of HANA.
Scanning column data is so fast that a lot of indexes are usually not required; 
HANA by default only indexes the primary key.
this saves memory as well as index update time’
Dynamic views can be computed on the fly; at over a billion records / sec, aggregations are very fast as well.
And you can see how it is easy to change the table, for instance to add a new column, without complete table re-organization;
each column is managed separately.
And of course, looking only at some of the columns (an operation called projection), is trivial in a column-oriented database.
Compression reduces main-memory access.  
This doesn’t only represent a performance bonus due to improved cache behavior, but illustrates another advantage of an in-memory database: the same amount of memory holds a lot more data; 
it is not uncommon to see a data compression ratio of 5x.
In this session, we learned in memory database HANA, which applied the advanced hardware technologies and developed software technologies.
thank you for your attention, if you have any question, feel free to contact me.
      </div>
      <div class="lx-controls">
        <div class="lx-button-row">
          <button class="lx-control-btn" onclick="playPause()">▶️ Play</button>
          <button class="lx-control-btn" onclick="prevExtraction()">⏮ Previous</button>
          <button class="lx-control-btn" onclick="nextExtraction()">⏭ Next</button>
        </div>
        <div class="lx-progress-container">
          <input type="range" id="progressSlider" class="lx-progress-slider"
                 min="0" max="10" value="0"
                 onchange="jumpToExtraction(this.value)">
        </div>
        <div class="lx-status-text">
          Entity <span id="entityInfo">1/11</span> |
          Pos <span id="posInfo">[1806-4995]</span>
        </div>
      </div>
    </div>

    <script>
      (function() {
        const extractions = [{"index": 0, "class": "CONCEPT", "text": "In-Memory Computing", "color": "#D2E3FC", "startPos": 1145, "endPos": 1164, "beforeText": " advances in software architecture and solutions toward distributed architecture and cloud make it easier to utilize these new hardware capabilities.\n", "extractionText": "In-Memory Computing", "afterText": "\nIn many ways, In-Memory Computing is a close relative of In-Memory Databases. As with many databases, it was designed to enable all the data manageme", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A computing paradigm designed for distributed and elastic environments that manages data on RAM devices instead of disks to achieve significantly better performance and lower latency for tasks like queries and transactions.</span>}</div>"}, {"index": 1, "class": "SUMMARY", "text": "In-memory databases, such as SAP HANA, have become feasible due to significant advances in hardware and software. Hardware trends include the availability of 64-bit CPUs supporting large amounts of RAM (e.g., 2TB) and the rise of multi-core processors, coupled with a rapid decline in memory costs. Software has evolved towards distributed and cloud architectures. HANA leverages these trends to combine transactional (OLTP) and analytical (OLAP) workloads into a single system, simplifying the data landscape. It employs advanced software techniques, including a hybrid storage model with row-based stores for transactions and column-based stores for analytics. Columnar storage enables high-performance aggregations, efficient compression (often achieving a 5x ratio), and eliminates the need for most indexes and pre-calculated aggregation tables. To handle updates efficiently in a compressed environment, HANA uses an 'insert-only' approach with a delta store for new data and changes, which are periodically merged into the main store. The system is designed for massive parallelism at every level, from concurrent user queries to intra-query operations across partitioned data on multi-core processors. HANA supports standard interfaces like SQL, JDBC, and ODBC, allowing existing applications to use it without modification.", "color": "#FEF0C3", "startPos": 1222, "endPos": 1241, "beforeText": "ure and cloud make it easier to utilize these new hardware capabilities.\nIn-Memory Computing\nIn many ways, In-Memory Computing is a close relative of ", "extractionText": "In-Memory Databases", "afterText": ". As with many databases, it was designed to enable all the data management aspects that are often expected from traditional databases, such as querie", "attributesHtml": "<div><strong>class:</strong> SUMMARY</div><div><strong>attributes:</strong> {}</div>"}, {"index": 2, "class": "TOPIC", "text": "SAP HANA In-Memory Database Architecture and Principles", "color": "#F9DEDC", "startPos": 1806, "endPos": 4995, "beforeText": "in-memory-based-computing are that In-Memory-Computing is:\n\uff09Designed for distributed and elastic environments\n\uff09Designed for In-Memory data processing\n", "extractionText": "HANA combine the hardware advantages and software advantages to make the in-memory Database possible. \nThe development of the hardware technology, multi-core architecture, parallel scaling across blades, 64-bit address space, 2 TB in current server board, 25 GB/s data throughput. \nAll these made the cost-performance ratio rapidly declining, which means the advanced speed processing hardware become cheaper and affordable.\nHana database also includes some advanced software technology and special methods, which include combining row based and column-based store, compression, partitioning, No aggregation tables, insert only and on-the-fly extensibility.\n\uff09 row based store is designed to support transactions processing, OLTP and the column based store to support analysis processing, such as OLAP.\n\uff09because the memory size cannot be as big as the hard disk, we must try any method to reduce the size of the DB, HANA used a lot of compression methods.\n\uff09 in order to increase the DB performance, the database can be partitioned to achieve the parallelization and improve the performance.\n\uff09 For eliminating the aggregation tables, normally for data analysis, the data warehouse is built to collect the relevant tables data from multiple data sources and get some aggregations to facilitate the further analysis. \nBut in HANA\uff0c it doesn\u2019t need the aggregation table , the aggregation in memory can be done quickly, no need to do it in advance, on the fly aggregation performance will not be affected, and the aggregation table made the redundant data, which cost the memory space.\n\uff09 And what are the benefits of the insert only approach?\nWell, since the main store is compressed (which is good for reading performance) changes would be expensive.\u00a0Just imagine you\u00a0want to make a change in a\u00a0compressed text file. \nYou need to uncompress, make the change and compress again.\u00a0For this reason data is only appended to the uncompressed delta\u00a0store where change operations are much cheaper. \nThe merge moves\u00a0the data from delta to main and thus compresses only from time to\u00a0time and not with\u00a0every change. \nSetting the deletion flag is based on\u00a0rowids and works for compressed and uncompressed data.\nIn the column store HANA will only\u00a0insert\u00a0data. That is whenever you update or delete data HANA will\njust\u00a0insert. For performance reasons,\u00a0the column\u00a0store is divided\u00a0in a main and a delta part. New data is going\nto the\u00a0delta and will be merged frequently to the main. Changing statements work as follows:\nINSERT:\nAn INSERT statement will just\u00a0insert\u00a0a new record in the delta. The merge process will move the record from\ndelta to main.\nDELETE:\nA DELETE statement will select the record and mark it as invalid by setting a flag (for main or delta). The mergeprocess will delete the record from memory once there is no open transaction active for it anymore.\nUPDATE:\nAn UPDATE statement will\u00a0insert\u00a0a new version of the record. The merge process will move the latest version from\ndelta to main. Old versions will be deleted\u00a0once there is no open transaction active for them anymore. The update\ntherefore, consists internally of an\u00a0insert\u00a0+ delete.\nThe following picture illustrates this principle", "afterText": " schematically. Please note that this picture does not reflect the real\nstorage in column store. You can see a deleted record in main (id 7) and 2 old", "attributesHtml": "<div><strong>class:</strong> TOPIC</div><div><strong>attributes:</strong> {}</div>"}, {"index": 3, "class": "CONCEPT", "text": "HANA", "color": "#D2E3FC", "startPos": 1806, "endPos": 1810, "beforeText": "in-memory-based-computing are that In-Memory-Computing is:\n\uff09Designed for distributed and elastic environments\n\uff09Designed for In-Memory data processing\n", "extractionText": "HANA", "afterText": " combine the hardware advantages and software advantages to make the in-memory Database possible. \nThe development of the hardware technology, multi-c", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An in-memory database that leverages advances in hardware (multi-core CPUs, large memory) and software (columnar storage, compression, partitioning, parallelism) to combine OLTP and OLAP workloads in a single system.</span>}</div>"}, {"index": 4, "class": "KEYWORDS", "text": "In-Memory Database, SAP HANA, columnar storage, row-based storage, OLTP, OLAP, parallel processing, data compression, insert-only, multi-core architecture", "color": "#C8E6C9", "startPos": 1879, "endPos": 1897, "beforeText": "distributed and elastic environments\n\uff09Designed for In-Memory data processing\nHANA combine the hardware advantages and software advantages to make the ", "extractionText": "in-memory Database", "afterText": " possible. \nThe development of the hardware technology, multi-core architecture, parallel scaling across blades, 64-bit address space, 2 TB in current", "attributesHtml": "<div><strong>class:</strong> KEYWORDS</div><div><strong>attributes:</strong> {}</div>"}, {"index": 5, "class": "CONCEPT", "text": "Row-based store", "color": "#D2E3FC", "startPos": 2338, "endPos": 2370, "beforeText": "ng hardware become cheaper and affordable.\nHana database also includes some advanced software technology and special methods, which include combining ", "extractionText": "row based and column-based store", "afterText": ", compression, partitioning, No aggregation tables, insert only and on-the-fly extensibility.\n\uff09 row based store is designed to support transactions pr", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A data storage method designed to support transaction processing (OLTP).</span>}</div>"}, {"index": 6, "class": "CONCEPT", "text": "Column-based store", "color": "#D2E3FC", "startPos": 2352, "endPos": 2370, "beforeText": "come cheaper and affordable.\nHana database also includes some advanced software technology and special methods, which include combining row based and ", "extractionText": "column-based store", "afterText": ", compression, partitioning, No aggregation tables, insert only and on-the-fly extensibility.\n\uff09 row based store is designed to support transactions pr", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A data storage method where data is stored in columnar order, which is highly efficient for analytical processing (OLAP), aggregations, and compression.</span>}</div>"}, {"index": 7, "class": "CONCEPT", "text": "Insert-only approach", "color": "#D2E3FC", "startPos": 2422, "endPos": 2428, "beforeText": " software technology and special methods, which include combining row based and column-based store, compression, partitioning, No aggregation tables, ", "extractionText": "insert", "afterText": " only and on-the-fly extensibility.\n\uff09 row based store is designed to support transactions processing, OLTP and the column based store to support analy", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A data modification strategy used in HANA where updates and deletes are handled by inserting new versions of records or setting deletion flags in an uncompressed delta store. This avoids the expensive process of decompressing and recompressing the main store for every change. The delta store is periodically merged into the main store.</span>}</div>"}, {"index": 8, "class": "CONCEPT", "text": "Caching", "color": "#D2E3FC", "startPos": 5547, "endPos": 5554, "beforeText": "ith a big difference.\nOther databases store data in physical pages on hard drives, and move &quot;copies&quot; of some of the data into memory; \nthis is called ", "extractionText": "caching", "afterText": ". When data is missing from the memory cache, it must be fetched from the disk, which takes a very long time, with a minimal latency of 10,000 micro-s", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A technique used by traditional databases where copies of data from physical hard drives are moved into memory. Accessing data not in the cache requires fetching it from the disk, which introduces significant latency.</span>}</div>"}, {"index": 9, "class": "CONCEPT", "text": "Materialized Aggregations", "color": "#D2E3FC", "startPos": 11547, "endPos": 11572, "beforeText": " country in the table above, resulting in a another, smaller table, which is faster to access for generating reports. \nThese extra tables are called \u201c", "extractionText": "materialized aggregations", "afterText": "\u201d. \nThe trouble with these aggregations is that they make applications very complicated. \nEvery transaction also requires the subsequent recalculation", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">Extra tables containing pre-calculated data (summations, averages) used in traditional databases to improve analytic performance. HANA&#x27;s high-speed, on-the-fly aggregation capabilities make these tables unnecessary.</span>}</div>"}, {"index": 10, "class": "CONCEPT", "text": "Star-schema", "color": "#D2E3FC", "startPos": 12541, "endPos": 12552, "beforeText": "tomers report hundreds of times of performance improvement.\nBusiness intelligence and data analytics often use normalized data models, referred to as ", "extractionText": "star-schema", "afterText": ". \nThis involves holding all the fact-data in one large table, with references to other information in separate tables, called dimensions. \nFor instan", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A normalized data model used in business intelligence where a large fact table holds primary data with references to dimension tables containing related information. HANA has a specially optimized execution engine for this model.</span>}</div>"}];
        let currentIndex = 0;
        let isPlaying = false;
        let animationInterval = null;
        let animationSpeed = 1.0;

        function updateDisplay() {
          const extraction = extractions[currentIndex];
          if (!extraction) return;

          document.getElementById('attributesContainer').innerHTML = extraction.attributesHtml;
          document.getElementById('entityInfo').textContent = (currentIndex + 1) + '/' + extractions.length;
          document.getElementById('posInfo').textContent = '[' + extraction.startPos + '-' + extraction.endPos + ']';
          document.getElementById('progressSlider').value = currentIndex;

          const playBtn = document.querySelector('.lx-control-btn');
          if (playBtn) playBtn.textContent = isPlaying ? '⏸ Pause' : '▶️ Play';

          const prevHighlight = document.querySelector('.lx-text-window .lx-current-highlight');
          if (prevHighlight) prevHighlight.classList.remove('lx-current-highlight');
          const currentSpan = document.querySelector('.lx-text-window span[data-idx="' + currentIndex + '"]');
          if (currentSpan) {
            currentSpan.classList.add('lx-current-highlight');
            currentSpan.scrollIntoView({block: 'center', behavior: 'smooth'});
          }
        }

        function nextExtraction() {
          currentIndex = (currentIndex + 1) % extractions.length;
          updateDisplay();
        }

        function prevExtraction() {
          currentIndex = (currentIndex - 1 + extractions.length) % extractions.length;
          updateDisplay();
        }

        function jumpToExtraction(index) {
          currentIndex = parseInt(index);
          updateDisplay();
        }

        function playPause() {
          if (isPlaying) {
            clearInterval(animationInterval);
            isPlaying = false;
          } else {
            animationInterval = setInterval(nextExtraction, animationSpeed * 1000);
            isPlaying = true;
          }
          updateDisplay();
        }

        window.playPause = playPause;
        window.nextExtraction = nextExtraction;
        window.prevExtraction = prevExtraction;
        window.jumpToExtraction = jumpToExtraction;

        updateDisplay();
      })();
    </script>