{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "SAP HANA In-Memory Database Architecture and Principles", "char_interval": {"start_pos": 1806, "end_pos": 4995}, "alignment_status": "match_fuzzy", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "In-memory databases, such as SAP HANA, have become feasible due to significant advances in hardware and software. Hardware trends include the availability of 64-bit CPUs supporting large amounts of RAM (e.g., 2TB) and the rise of multi-core processors, coupled with a rapid decline in memory costs. Software has evolved towards distributed and cloud architectures. HANA leverages these trends to combine transactional (OLTP) and analytical (OLAP) workloads into a single system, simplifying the data landscape. It employs advanced software techniques, including a hybrid storage model with row-based stores for transactions and column-based stores for analytics. Columnar storage enables high-performance aggregations, efficient compression (often achieving a 5x ratio), and eliminates the need for most indexes and pre-calculated aggregation tables. To handle updates efficiently in a compressed environment, HANA uses an 'insert-only' approach with a delta store for new data and changes, which are periodically merged into the main store. The system is designed for massive parallelism at every level, from concurrent user queries to intra-query operations across partitioned data on multi-core processors. HANA supports standard interfaces like SQL, JDBC, and ODBC, allowing existing applications to use it without modification.", "char_interval": {"start_pos": 1222, "end_pos": 1241}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "In-Memory Database, SAP HANA, columnar storage, row-based storage, OLTP, OLAP, parallel processing, data compression, insert-only, multi-core architecture", "char_interval": {"start_pos": 1879, "end_pos": 1897}, "alignment_status": "match_lesser", "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "In-Memory Computing", "char_interval": {"start_pos": 1145, "end_pos": 1164}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "A computing paradigm designed for distributed and elastic environments that manages data on RAM devices instead of disks to achieve significantly better performance and lower latency for tasks like queries and transactions."}}, {"extraction_class": "CONCEPT", "extraction_text": "HANA", "char_interval": {"start_pos": 1806, "end_pos": 1810}, "alignment_status": "match_fuzzy", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "An in-memory database that leverages advances in hardware (multi-core CPUs, large memory) and software (columnar storage, compression, partitioning, parallelism) to combine OLTP and OLAP workloads in a single system."}}, {"extraction_class": "CONCEPT", "extraction_text": "Row-based store", "char_interval": {"start_pos": 2338, "end_pos": 2370}, "alignment_status": "match_fuzzy", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "A data storage method designed to support transaction processing (OLTP)."}}, {"extraction_class": "CONCEPT", "extraction_text": "Column-based store", "char_interval": {"start_pos": 2352, "end_pos": 2370}, "alignment_status": "match_exact", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "A data storage method where data is stored in columnar order, which is highly efficient for analytical processing (OLAP), aggregations, and compression."}}, {"extraction_class": "CONCEPT", "extraction_text": "Insert-only approach", "char_interval": {"start_pos": 2422, "end_pos": 2428}, "alignment_status": "match_lesser", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "A data modification strategy used in HANA where updates and deletes are handled by inserting new versions of records or setting deletion flags in an uncompressed delta store. This avoids the expensive process of decompressing and recompressing the main store for every change. The delta store is periodically merged into the main store."}}, {"extraction_class": "CONCEPT", "extraction_text": "Caching", "char_interval": {"start_pos": 5547, "end_pos": 5554}, "alignment_status": "match_exact", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "A technique used by traditional databases where copies of data from physical hard drives are moved into memory. Accessing data not in the cache requires fetching it from the disk, which introduces significant latency."}}, {"extraction_class": "CONCEPT", "extraction_text": "Materialized Aggregations", "char_interval": {"start_pos": 11547, "end_pos": 11572}, "alignment_status": "match_exact", "extraction_index": 10, "group_index": 9, "description": null, "attributes": {"definition": "Extra tables containing pre-calculated data (summations, averages) used in traditional databases to improve analytic performance. HANA's high-speed, on-the-fly aggregation capabilities make these tables unnecessary."}}, {"extraction_class": "CONCEPT", "extraction_text": "Star-schema", "char_interval": {"start_pos": 12541, "end_pos": 12552}, "alignment_status": "match_exact", "extraction_index": 11, "group_index": 10, "description": null, "attributes": {"definition": "A normalized data model used in business intelligence where a large fact table holds primary data with references to dimension tables containing related information. HANA has a specially optimized execution engine for this model."}}, {"extraction_class": "CONCEPT", "extraction_text": "OLTP (Online Transaction Processing)", "char_interval": null, "alignment_status": null, "extraction_index": 12, "group_index": 11, "description": null, "attributes": {"definition": "A class of systems that facilitate and manage transaction-oriented applications, typically for data entry and retrieval transaction processing. In HANA, this is supported by row-based storage."}}, {"extraction_class": "CONCEPT", "extraction_text": "OLAP (Online Analytical Processing)", "char_interval": null, "alignment_status": null, "extraction_index": 13, "group_index": 12, "description": null, "attributes": {"definition": "A class of systems that allow for complex analysis of data from multiple perspectives. In HANA, this is supported by column-based storage."}}], "text": "Hello everyone, I am Haiying Che, from  Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology , in this session we discuss In memory Database.\nFrom the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing platform and engine layer, etc.\nComputing models are the way that different kinds of big data is processed in different scenarios, We have learned the batch processing, stream computing, Large-scale concurrent processing model for structured data, and in this session we discuss In-memory database.\nThere have been multiple advances in the industry in both hardware and software architecture, which makes memory-based computing more feasible today than in the past, as outlined in the diagram below.\nIn a nutshell, the availability of new classes of hardware with the support of 64bit CPU can now support 2TB on a single device. In addition, the advances in software architecture and solutions toward distributed architecture and cloud make it easier to utilize these new hardware capabilities.\nIn-Memory Computing\nIn many ways, In-Memory Computing is a close relative of In-Memory Databases. As with many databases, it was designed to enable all the data management aspects that are often expected from traditional databases, such as queries and transactions, with the difference that the data is managed on RAM devices and not disks and thus comes with potentially x1000 better performance and latency according to various benchmarks.\nThe main differences between the traditional in-memory databases and in-memory-based-computing are that In-Memory-Computing is:\n）Designed for distributed and elastic environments\n）Designed for In-Memory data processing\nHANA combine the hardware advantages and software advantages to make the in-memory Database possible. \nThe development of the hardware technology, multi-core architecture, parallel scaling across blades, 64-bit address space, 2 TB in current server board, 25 GB/s data throughput. \nAll these made the cost-performance ratio rapidly declining, which means the advanced speed processing hardware become cheaper and affordable.\nHana database also includes some advanced software technology and special methods, which include combining row based and column-based store, compression, partitioning, No aggregation tables, insert only and on-the-fly extensibility.\n） row based store is designed to support transactions processing, OLTP and the column based store to support analysis processing, such as OLAP.\n）because the memory size cannot be as big as the hard disk, we must try any method to reduce the size of the DB, HANA used a lot of compression methods.\n） in order to increase the DB performance, the database can be partitioned to achieve the parallelization and improve the performance.\n） For eliminating the aggregation tables, normally for data analysis, the data warehouse is built to collect the relevant tables data from multiple data sources and get some aggregations to facilitate the further analysis. \nBut in HANA， it doesn’t need the aggregation table , the aggregation in memory can be done quickly, no need to do it in advance, on the fly aggregation performance will not be affected, and the aggregation table made the redundant data, which cost the memory space.\n） And what are the benefits of the insert only approach?\nWell, since the main store is compressed (which is good for reading performance) changes would be expensive. Just imagine you want to make a change in a compressed text file. \nYou need to uncompress, make the change and compress again. For this reason data is only appended to the uncompressed delta store where change operations are much cheaper. \nThe merge moves the data from delta to main and thus compresses only from time to time and not with every change. \nSetting the deletion flag is based on rowids and works for compressed and uncompressed data.\nIn the column store HANA will only insert data. That is whenever you update or delete data HANA will\njust insert. For performance reasons, the column store is divided in a main and a delta part. New data is going\nto the delta and will be merged frequently to the main. Changing statements work as follows:\nINSERT:\nAn INSERT statement will just insert a new record in the delta. The merge process will move the record from\ndelta to main.\nDELETE:\nA DELETE statement will select the record and mark it as invalid by setting a flag (for main or delta). The mergeprocess will delete the record from memory once there is no open transaction active for it anymore.\nUPDATE:\nAn UPDATE statement will insert a new version of the record. The merge process will move the latest version from\ndelta to main. Old versions will be deleted once there is no open transaction active for them anymore. The update\ntherefore, consists internally of an insert + delete.\nThe following picture illustrates this principle schematically. Please note that this picture does not reflect the real\nstorage in column store. You can see a deleted record in main (id 7) and 2 old versions (marked in grey) of\nrecords (id 8 and 14) where the name has been updated, which lead to an insert of a new version of the records.\nHANA is an in-memory database. But what exactly do we mean by that? Don't all databases use memory? Yes, but with a big difference.\nOther databases store data in physical pages on hard drives, and move \"copies\" of some of the data into memory; \nthis is called caching. When data is missing from the memory cache, it must be fetched from the disk, which takes a very long time, with a minimal latency of 10,000 micro-seconds, holding up the queries and calculations. \nNot only that, since the data in memory is a copy of the disk, it is stored in a way to reflect the disk-structure, in the form of linked pages, sectors, etc., which adds additional overhead when fetching the data.\nHANA, on the other hand stores the data directly in memory, optimized not for how the data is stored on disk, but optimized for efficient access, and as we shall see, highly compressed, using the most efficient memory representations that computer scientists can come up with.\nEven in the event of a cache-miss, it only takes 0.1 micro-seconds to fetch the missing data from DRAM. That is 100,000 times faster, and this makes all the difference. \nNote that when a transaction changes a piece of the data, HANA logs a record of that change, so that it can be recovered in the event of a crash. \nThere have been two industry trends that enable HANA performance. \nThe first trend is the astonishing way that memory prices have gone down in the past several years. \nWhat was unimaginable only a few years ago, has become totally affordable today, as you can see from this graph, which shows the price of a Gigabyte of DRAM over time.\nNotice how little time it took for a 10X reduction from $100/GB to $10/GB.\nToday, a Terabyte of memory costs much less than ten thousand dollars, and computer makers are now putting together servers with four, and soon up to 12 terabytes of memory in a single system.\nThe second trend is the way compute power has gone up. A little over a decade ago, we would buy a computer with a single processor. \nThen, several years later, Intel introduced dual-core processors, followed, not long ago, with 64-bit processors with eight, dual-threaded, cores. \nThe first servers for HANA would have as many as four such processors. Now, we are seeing Intel Sandy Bridge processors with 12 cores, and servers with up to 16 processors. \nThat is not just a 200X performance jump in a decade, this really heralds a challenge for modern software – in-memory now also means that the software must be written to leverage the multi-core architecture of modern processors and multi-processor servers, and even scale out beyond.\nDisk-centric computing is also a major factor that forces separation of transactional and analytical workloads. \nSpecialized data warehouses for reporting and analytics required the moving, transformation and pre-processing of transactional data.\nThis introduces a huge complexity: sometimes an enterprise may hold three different copies of the same data…\nLeveraging the power of in-memory computing allows HANA to bring OLTP, transaction processing, and OLAP, data analytics, back together in one database. \nHANA reverses the industry trend for separated, specialized analytics databases.\nThe result is a significant landscape simplification.\nThis image is a simplified architectural picture of SAP HANA.\nHANA is certified and delivered on powerful enterprise servers from partners like IBM, HP, Fujitsu, Cisco, Dell and others. \nThese highly optimized servers scale up to 192 cores, and managed by a Linux operating system from partners like SUSE and Red Hat. \nThe HANA software is carefully optimized for the hardware it runs on, and takes advantage of the massive parallelism and memory architecture of the modern processing cores on which it runs.\nA variety of storage systems can be used for high availability and backup support, including solid state or flash disks.\nHANA is fully compatible with other database systems, and supports SQL, JDBC, ODBC and analytical data source interfaces like MDX-Multi Dimensional Expressions. \nThis means that existing applications can use HANA without any modifications. In addition, HANA supports web-based interfaces which we will discuss later in this course.\nHANA was designed from the ground up to be massively parallel at every level of operation, one of the main reasons why it performs so well. \nThe first level is simple: multiple concurrent users and queries are allocated their separate compute resources. \nThe HANA optimizer then creates an execution plan, that parallelizes the operations within a query. \nFor instance, I can scan two columns at the same time on different processing cores. \nAnd by partitioning the data, I can parallelize such operations even more, even over multiple servers. \nFor instance, I can sum table A over as four separate concurrent aggregations, and finally sum the four results together.\nAnd by using processor techniques like hyper-threading, we get even more concurrency.\nAnd all of that is internal to HANA; the applications are not aware of this, except that they enjoy better performance!\nHere is another very important attribute of HANA: the way that data is stored in memory. \nTraditionally, databases store data in rows. One reason is that this is much more convenient when the database is stored on a disk: you simply write a new row to the end of the file.\nHANA, on the other hand, stores its tables in columnar order. \nThis means that it is much more efficient to perform an aggregation (sum of sales, for instance), because all the relevant data is in consecutive memory locations, and most values will be pre-fetched by the processor. \nNot only this, but by storing data in columns like this, we can sort and compress it much more efficiently than with conventional databases.\nIn traditional databases, it is often impossible to perform data analytics on the raw, original transaction data. \nTherefore, modeling experts are forced to introduce extra tables with pre-calculated summations, average, etc., to increase analytic performance, giving up detail. \nFor instance, we could sum sales by country in the table above, resulting in a another, smaller table, which is faster to access for generating reports. \nThese extra tables are called “materialized aggregations”. \nThe trouble with these aggregations is that they make applications very complicated. \nEvery transaction also requires the subsequent recalculation and updating of all affected aggregation tables, which may slow operations down significantly. \nAnother approach, updating the aggregates periodically, means that reports are often not up to date, or don’t match the transaction data.\nBut HANA can aggregate massive data sets with high performance, over 15 million aggregations per second per core, so there is often really no need for aggregates anymore.\nThis immediately yields: simpler data models which fewer tables; no loss of detail; Much simpler application logic, with much fewer write-locks and logs; better up-to-datedness\nAll of this improves the performance dramatically. \nThis is one of the reasons why customers report hundreds of times of performance improvement.\nBusiness intelligence and data analytics often use normalized data models, referred to as star-schema. \nThis involves holding all the fact-data in one large table, with references to other information in separate tables, called dimensions. \nFor instance, we could have all our sales data in the central fact table, and details like country, and product in dimension tables. \nHANA has a specially optimized execution engine for these types of models, which often speeds up operations on such tables ten times or more, and contributes greatly to the overall performance impact of HANA.\nScanning column data is so fast that a lot of indexes are usually not required; \nHANA by default only indexes the primary key.\nthis saves memory as well as index update time’\nDynamic views can be computed on the fly; at over a billion records / sec, aggregations are very fast as well.\nAnd you can see how it is easy to change the table, for instance to add a new column, without complete table re-organization;\neach column is managed separately.\nAnd of course, looking only at some of the columns (an operation called projection), is trivial in a column-oriented database.\nCompression reduces main-memory access.  \nThis doesn’t only represent a performance bonus due to improved cache behavior, but illustrates another advantage of an in-memory database: the same amount of memory holds a lot more data; \nit is not uncommon to see a data compression ratio of 5x.\nIn this session, we learned in memory database HANA, which applied the advanced hardware technologies and developed software technologies.\nthank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_7f40c1cf"}
