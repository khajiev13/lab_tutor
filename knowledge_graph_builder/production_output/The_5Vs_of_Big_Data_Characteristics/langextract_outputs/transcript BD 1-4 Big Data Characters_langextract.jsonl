{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "The 5Vs of Big Data Characteristics", "char_interval": {"start_pos": 5956, "end_pos": 6665}, "alignment_status": "match_fuzzy", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "This session discusses the five characteristics of Big Data, known as the 5Vs: Volume, Velocity, Variety, Veracity, and Value, which evolved from the original 3Vs. The scale of data has grown exponentially from megabytes in early ERP systems to petabytes from sources like sensors, social media, and mobile devices. Volume refers to the immense amount of data generated. Variety describes the diverse forms of data, including structured (relational), semi-structured (XML), and unstructured (text, video, audio), which must be linked for comprehensive analysis. Velocity is the speed at which data is created and must be processed, enabling real-time analytics for applications like e-promotions and healthcare monitoring. Veracity addresses the trustworthiness and uncertainty of data due to inconsistencies and incompleteness. Finally, Value is the most crucial aspect, emphasizing that data must be reliable and useful to provide actionable insights, even though it often has a low value density.", "char_interval": {"start_pos": 158, "end_pos": 170}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "Big Data, 5Vs, Volume, Velocity, Variety, Veracity, Value, real-time analytics, unstructured data, data sources", "char_interval": {"start_pos": 306, "end_pos": 314}, "alignment_status": "match_lesser", "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Volume", "char_interval": {"start_pos": 230, "end_pos": 236}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "Refers to the huge amounts of data that is collected and generated every second in large organizations, with data scale growing exponentially from sources like IoT devices, social media, videos, and financial transactions."}}, {"extraction_class": "CONCEPT", "extraction_text": "Variety", "char_interval": {"start_pos": 248, "end_pos": 255}, "alignment_status": "match_fuzzy", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "Refers to the different sources of data and their nature, which includes structured (Relational Data), semi-structured (XML), and unstructured data (Text, Graph, Social Network, Streaming Data, photos, audio files, videos)."}}, {"extraction_class": "CONCEPT", "extraction_text": "Velocity", "char_interval": {"start_pos": 6040, "end_pos": 6048}, "alignment_status": "match_exact", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "The speed at which data is created or generated and needs to be processed quickly to meet user demands and enable real-time decisions, as late decisions can lead to missed opportunities."}}, {"extraction_class": "CONCEPT", "extraction_text": "Veracity", "char_interval": {"start_pos": 6218, "end_pos": 6226}, "alignment_status": "match_exact", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "The degree of trustworthiness of the data, which is often in doubt due to uncertainty from data inconsistency, incompleteness, ambiguities, latency, and deception."}}, {"extraction_class": "CONCEPT", "extraction_text": "Value", "char_interval": {"start_pos": 6231, "end_pos": 6236}, "alignment_status": "match_exact", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "The most important characteristic of Big Data, indicating that data must be reliable and useful for processing and analysis to be considered valuable. It often has a low value density, meaning useful information is sparse."}}], "text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session, we will discuss about big data Characters, 5Vs, which are volume, velocity, variety, veracity and value.\nOriginally the Characters of Big data is 3 Vs, which are volume, velocity, variety\nAs the information technology developed, the data recorded by the system or devices expanded dramatically.\nShown in the diagram. \nAt the very beginning, the ERP (Enterprise resource planning) system record the purchase detail, purchase record and payment record, which is mainly transactions, data scale is MB.\nand then they expanded the content of customer, in CRM the segmentation, offer details, customer touches, supports contacts information have been recorded in system, which is transactions and interactions with the boundary of organization, data scale is GB.\nFurther with the help of Web, the web logs, offer history, A/B testing, dynamic Pricing, affiliated networks, search marketing, behavioral targeting, dynamic Funnel that useful information can also be recoded and stored. Which are transactions and interactions with the inside and outside of the organization, data scale is TB.\nIn the Big data scope, the sensors/RFID/Devices, user click stream, mobile web, sentiment analysis, user generated content, social interaction & feeds, spatial & GPS coordinates, external demographics, business data feeds, HD video, audio, images, speech to text, product/service logs, SMS/MMS and etc. the data scope is very comprehensive, which are transactions, interactions and observations, data scale is PB, \nSo, from this development, we can see, the data volume and variety, also the velocity is increased.\nLet’ s goes through these characters one by one.\nvolume\nVolume\nVolume refers to the huge amounts of data that is collected and generated every second in large organizations. \ndata scale has increased 44 times from 2009 to 2020，From 0.8 zettabytes to 35 zettabytes， The amount of data is growing exponentially\nThis data is generated from different sources such as IoT devices, social media, videos, financial transactions, and customer logs.\nData volume can vary. For example, a text file is a few kilobytes whereas a video file is a few megabytes. \nIDC's \"Data Era 2025\" report, by 2025, the global data will reach 175 ZB (218.75 times in 2009)\nThe project Earth scope\nEarth scope is the largest scientific project in the world. The observatory aims to track the geological evolution of North America, recording more than 3.8 million square miles of data and accumulating 67 TB of data. It analyzes the seismic slip in the San Andreas fault, and of course, the magma plume below Yellowstone Park, and so on. \nVariety\nAnother one of the most important Big Data characteristics is its variety. It refers to the different sources of data and their nature. \nThe sources of data have changed over the years. Earlier, it was only available in spreadsheets and databases. \nNowadays, data is present in photos, audio files, videos, text files, and PDFs.\nWhich includes \nRelational Data (Tables/Transaction/Legacy Data)\nText Data (Web)\nSemi-structured Data (XML) \nGraph Data\nSocial Network, Semantic Web (RDF), … \nStreaming Data \nThat you can only scan the data once\nA single application can be generating/collecting many types of data  \nBig Public Data (online, weather, finance, etc.)  \nIn order to extract knowledge, all these types of data need to be linked together\nThe variety of data is crucial for its storage and analysis.\nFor example, if you want to analyze your customer, you want to know all different aspects of him.\nIncluding his purchase preference, our known history in transaction system, banking, finance history,  social media, gaming and entertainment and so on, with all these data you will know comprehensive information about your customer and provide better service. \nThis term refers to the speed at which the data is created or generated. \nData is generated fast and need to be processed fast\nThis speed of data producing is also related to how fast this data is going to be processed. This is because only after analysis and processing, the data can meet the demands of the clients/users.\nMassive amounts of data are produced from sensors,  social media sites, and application logs – and all of it is continuous. If the data flow is not continuous, there is no point in investing time or effort on it.\nLate decisions will lead to miss opportunities\nFor Example,\nE-Promotions: Based on your current location, your purchase history to analyze what would you like and send promotions right now for store next to you, if the Promotion was sent when you already far from the store, it is no use.\nAnd Healthcare monitoring, sensors monitoring your activities and body, when any abnormal measurements happens, it requires immediate reaction\nReal-time/Fast Data\nA lot of fast data is real time.\nIn Social media and networks   all of us are generating data.\nScientific instruments are collecting all sorts of data.\nMobile devices are tracking all objects all the time.\nSensor technology and networks are measuring all kinds of data.\nPeople and devices are all generating data. \nThe progress and innovation is no longer hindered by the ability to collect data\nBut, by the ability to manage, analyze, summarize, visualize, and discover knowledge from the collected data in a timely manner and in a scalable fashion.\nReal-Time Analytics/Decision Requirement\nReal-Time Analytics is important, which can enable Realtime better decision, act when it is happening.\nFor example:\nProduct Recommendations that are Relevant & Compelling.\nLearning why Customers Switch to competitors and their offers in time to Counter \nImproving the Marketing Effectiveness of a Promotion while it is still in Play.\nPreventing Fraud as it is Occurring & preventing more proactively.\n5V’s\nLet’s look at the 5Vs, \nvolume, data at rest, terabytes to exabytes of existing data to process, \nvelocity, data in motion, streaming data, milliseconds to seconds to respond.\nvariety, data in many forms, structured, unstructured, text, multimedia and so on\nThe other two are veracity and value. \nVeracity: data in Doubt\nUncertainty due to data inconsistency and incompleteness, ambiguities, latency, deception, model approximations.\nThis feature of Big Data is connected to the previous one. It defines the degree of trustworthiness of the data. \nAs most of the data you encounter is unstructured, it is important to filter out the unnecessary information and use the rest for processing.\nvalue: \nAmong the characteristics of Big Data, value is perhaps the most important. No matter how fast the data is produced or its amount, it has to be reliable and useful. Otherwise, the data is not good enough for processing or analysis. Research says that poor quality data can lead to almost a 20% loss in a company’s revenue. Data scientists first convert raw data into information. Then this data set is cleaned to retrieve the most useful data. Analysis and pattern identification is done on this data set. If the process is a success, the data can be considered to be valuable.\nEven the valuable data, it is low value density, like panning for gold in the sand. A few seconds among a video of several house may be useful.\nconclusion\nThank you for your attention, if you have any question, feel free to connect me.", "document_id": "doc_444eb1d7"}
