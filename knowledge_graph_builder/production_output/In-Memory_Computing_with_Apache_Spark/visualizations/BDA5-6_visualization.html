<style>
.lx-highlight { position: relative; border-radius:3px; padding:1px 2px;}
.lx-highlight .lx-tooltip {
  visibility: hidden;
  opacity: 0;
  transition: opacity 0.2s ease-in-out;
  background: #333;
  color: #fff;
  text-align: left;
  border-radius: 4px;
  padding: 6px 8px;
  position: absolute;
  z-index: 1000;
  bottom: 125%;
  left: 50%;
  transform: translateX(-50%);
  font-size: 12px;
  max-width: 240px;
  white-space: normal;
  box-shadow: 0 2px 6px rgba(0,0,0,0.3);
}
.lx-highlight:hover .lx-tooltip { visibility: visible; opacity:1; }
.lx-animated-wrapper { max-width: 100%; font-family: Arial, sans-serif; }
.lx-controls {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 8px;
  padding: 12px; margin-bottom: 16px;
}
.lx-button-row {
  display: flex; justify-content: center; gap: 8px; margin-bottom: 12px;
}
.lx-control-btn {
  background: #4285f4; color: white; border: none; border-radius: 4px;
  padding: 8px 16px; cursor: pointer; font-size: 13px; font-weight: 500;
  transition: background-color 0.2s;
}
.lx-control-btn:hover { background: #3367d6; }
.lx-progress-container {
  margin-bottom: 8px;
}
.lx-progress-slider {
  width: 100%; margin: 0; appearance: none; height: 6px;
  background: #ddd; border-radius: 3px; outline: none;
}
.lx-progress-slider::-webkit-slider-thumb {
  appearance: none; width: 18px; height: 18px; background: #4285f4;
  border-radius: 50%; cursor: pointer;
}
.lx-progress-slider::-moz-range-thumb {
  width: 18px; height: 18px; background: #4285f4; border-radius: 50%;
  cursor: pointer; border: none;
}
.lx-status-text {
  text-align: center; font-size: 12px; color: #666; margin-top: 4px;
}
.lx-text-window {
  font-family: monospace; white-space: pre-wrap; border: 1px solid #90caf9;
  padding: 12px; max-height: 260px; overflow-y: auto; margin-bottom: 12px;
  line-height: 1.6;
}
.lx-attributes-panel {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 6px;
  padding: 8px 10px; margin-top: 8px; font-size: 13px;
}
.lx-current-highlight {
  border-bottom: 4px solid #ff4444;
  font-weight: bold;
  animation: lx-pulse 1s ease-in-out;
}
@keyframes lx-pulse {
  0% { text-decoration-color: #ff4444; }
  50% { text-decoration-color: #ff0000; }
  100% { text-decoration-color: #ff4444; }
}
.lx-legend {
  font-size: 12px; margin-bottom: 8px;
  padding-bottom: 8px; border-bottom: 1px solid #e0e0e0;
}
.lx-label {
  display: inline-block;
  padding: 2px 4px;
  border-radius: 3px;
  margin-right: 4px;
  color: #000;
}
.lx-attr-key {
  font-weight: 600;
  color: #1565c0;
  letter-spacing: 0.3px;
}
.lx-attr-value {
  font-weight: 400;
  opacity: 0.85;
  letter-spacing: 0.2px;
}

/* Add optimizations with larger fonts and better readability for GIFs */
.lx-gif-optimized .lx-text-window { font-size: 16px; line-height: 1.8; }
.lx-gif-optimized .lx-attributes-panel { font-size: 15px; }
.lx-gif-optimized .lx-current-highlight { text-decoration-thickness: 4px; }
</style>
    <div class="lx-animated-wrapper lx-gif-optimized">
      <div class="lx-attributes-panel">
        <div class="lx-legend">Highlights Legend: <span class="lx-label" style="background-color:#D2E3FC;">CONCEPT</span> <span class="lx-label" style="background-color:#C8E6C9;">TOPIC</span></div>
        <div id="attributesContainer"></div>
      </div>
      <div class="lx-text-window" id="textWindow">
        Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering
School of Computer Science, in Beijing Institute of Technology, 
in this session, we discuss in memory computing represented by Spark
The data processing system provides big data computing and processing capabilities and an application development platform.
From the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing platform layer, computing engine layer, etc.
Computing models are the way that different kinds of big data is processed in different scenarios, 
which include batch processing, stream computing, Large-scale concurrent processing (MPP) model for structured data, <span class="lx-highlight lx-current-highlight" data-idx="0" style="background-color:#C8E6C9;">In-memory Computing</span> model, and Data Flow Graph models.
Let’s look at the 4th computing model In-memory Computing model
In-memory computation (or in-memory computing) is the technique of running computer calculations entirely in computer memory (e.g., in RAM). This term typically implies large-scale, complex calculations which require specialized systems software to run the calculations on computers working together in a cluster. As a cluster, the computers pool together their RAM so the calculation is essentially run across computers and leverages the collective RAM space of all the computers together.
This term is mostly synonymous with in-memory computing and in-memory processing, with perhaps a slightly nuanced difference where in-memory computation is a specific type of task dealing with calculations, that falls under the umbrella of in-memory computing. In-memory processing can be considered a different type of task that also falls under in-memory computing.
How Does In-Memory Computation Work?
In-memory computation works by eliminating all slow data accesses and relying exclusively on data stored in RAM. Overall computation performance is greatly improved by removing the latency commonly seen when accessing hard disk drives or SSDs. Software running on one or more computers manages the computation as well as the data in memory, and in the case of multiple computers, the software divides the computation into smaller tasks which are distributed out to each computer to run in parallel. 
Speak of In-memory Computing model, we must talk about spark.
Spark was initially started by Matei Zaharia at UC Berkeley‘s AMP Lab in 2009, and open sourced in 2010. 
In 2013, donated to the Apache Software Foundation. 
one of the most active open source big data projects, Top-Level Apache Project
Parallel processing framework based on the memory computing model.
It can be built on the Hadoop platform and use the HDFS file system to store data, but a Resilient Distributed dataset architecture (RDD) is built on top of the file system for Supports efficient Distributed Memory Computing.
In 2013 Hadoop finish the task of sorting 100TB data within 72 minutes using 2100 machines.
Buit In November 2014, Spark founder M. Zaharia&#x27;s company Databricks set a new world record in large scale sorting using Spark.
Spark sorting the 100TB data within 21minutes using 207 machine, which is one tenth of the number of the machines that Hadoop used and reduced the time from 72 minute to 23 minutes.
Far more faster and more efficient than Hadoop.
<span class="lx-highlight" data-idx="1" style="background-color:#D2E3FC;">Apache Spark</span> is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching and optimized query execution for fast queries against data of any size. Simply put, Spark is a fast and general engine for large-scale data processing.
The fast part means that it’s faster than previous approaches to work with Big Data like classical MapReduce. The secret for being faster is that Spark runs on memory (RAM), and that makes the processing much faster than on disk drives.
The general part means that it can be used for multiple things like running distributed SQL, creating data pipelines, ingesting data into a database, running Machine Learning algorithms, working with graphs or data streams, and much more.
resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. 
RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. 
There are only 2 kinds of operation of <span class="lx-highlight" data-idx="2" style="background-color:#D2E3FC;">RDD, transformations and action</span>. In transformation, data can be filter, joined map, reduced but no calculation is executed, only in action the calculation can be done, and the value result can be generated.
Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.
The Transformation return value is still an RDD. It uses the chain call design pattern, after calculating one RDD, transform it into another RDD, and then this RDD can be converted again. 
This process is distributed. 
Action return value is not an RDD. It is either a normal collection of Scala, or a value, or it is empty, and eventually it is returned to the Driver program, or the RDD is written to the file system.
Action is the return value returned to the driver or stored in a file, it is the transformation from RDD to result, and Transformation is the transformation from RDD to RDD.
Only when the action is executed, the rdd will be calculated and generated, which is the root of the lazy execution of rdd.
This diagram shows the topology of the <span class="lx-highlight" data-idx="3" style="background-color:#D2E3FC;">RDD transformations</span> and the linage of RDD transformation, which the executions should follow.
Spark running process is like this: The <span class="lx-highlight" data-idx="4" style="background-color:#D2E3FC;">Spark architecture</span> uses the Master-Slave model in distributed computing. 
Master is the node containing the Master process in the corresponding cluster, and Slave is the node containing the Worker process in the cluster. 
Master is the controller of the entire cluster and is responsible for the normal operation of the entire cluster; Worker is equivalent to a computing node, receiving commands from the master node and reporting status; Executor is responsible for task execution; Client as the user&#x27;s client is responsible for submitting applications, and Driver is responsible for controlling one Application execution. 
After the Spark cluster is deployed, it is necessary to start the Master process and the Worker process on the master node and the slave node respectively to control the entire cluster. During the execution of a Spark application, Driver and Worker are two important roles. The Driver program is the starting point of application logic execution, responsible for job scheduling, that is, the distribution of Task tasks, and multiple Workers are used to manage computing nodes and create Executor parallel processing tasks.
In the execution phase, the Driver serializes the files and jars that the Task and Task depend on and transfers them to the corresponding Worker machine. At the same time, the Executor processes the tasks of the corresponding data partition.Excecutor /Task each program has its own, different programs are isolated from each other, task is multi-threaded in parallel, The cluster is transparent to Spark, as long as Spark can obtain related nodes and processes。Driver keeps communication with Executor and cooperates in processing.
Features of Spark include 
In-memory Computation
PySpark provides provision of <span class="lx-highlight" data-idx="5" style="background-color:#D2E3FC;">in-memory computation</span>. Computed results are stored in distributed memory (RAM) instead of stable storage (disk). It provides very fast computation
<span class="lx-highlight" data-idx="6" style="background-color:#D2E3FC;">Lazy Evolution</span>
Transformation in PySpark RDDs is lazy. It doesn&#x27;t compute the result immediately means that execution does not start until an action is triggered. When we call some operation in RDD for transformation, it does not execute immediately. Lazy Evolution plays an important role in saving calculation overhead. It provides the optimization by reducing the number of queries.
Fault Tolerant
RDDs track data lineage information to reconstruct lost data automatically. If failure occurs in any partition of RDDs, then that partition can be re-computed from the original fault tolerant input dataset to create it.
Immutability
The created data can be retrieved anytime but its value can&#x27;t be changed. RDDs can only be created through deterministic operations.
Partitioning
RDDs are the collection of various data items that are so huge in size. Because of its size they cannot fit into a single node and must be partitioned across various nodes.
Persistence
It is an optimization technique where we can save the result of RDD evaluation. It stores the intermediate result so that we can use it further if required. It reduces the computation complexity.
Coarse-Grained Operation
The coarse-grained operation means that we can transform the whole dataset but not individual element on the dataset. On the other hand, fine grained mean we can transform individual element on the dataset.
Apache <span class="lx-highlight" data-idx="7" style="background-color:#D2E3FC;">Spark Core</span> – Spark Core is the underlying general execution engine for the Spark platform that all other functionality is built upon. It provides in-memory computing and referencing datasets in external storage systems.
<span class="lx-highlight" data-idx="8" style="background-color:#D2E3FC;">Spark SQL</span> – Spark SQL is Apache Spark’s module for working with structured data. The interfaces offered by Spark SQL provides Spark with more information about the structure of both the data and the computation being performed.
<span class="lx-highlight" data-idx="9" style="background-color:#D2E3FC;">Spark Streaming</span> – This component allows Spark to process real-time streaming data. Data can be ingested from many sources like Kafka, Flume, and HDFS (Hadoop Distributed File System). Then the data can be processed using complex algorithms and pushed out to file systems, databases, and live dashboards.
<span class="lx-highlight" data-idx="10" style="background-color:#D2E3FC;">MLlib (Machine Learning Library)</span> – Apache Spark is equipped with a rich library known as MLlib. This library contains a wide array of machine learning algorithms- classification, regression, clustering, and collaborative filtering. It also includes other tools for constructing, evaluating, and tuning ML Pipelines. All these functionalities help Spark scale out across a cluster.
<span class="lx-highlight" data-idx="11" style="background-color:#D2E3FC;">GraphX</span> – Spark also comes with a library to manipulate graph databases and perform computations called GraphX. GraphX unifies ETL (Extract, Transform, and Load) process, exploratory analysis, and iterative graph computation within a single system.
Spark makes it easy to start working with distributed computing systems.
Through its core API support for multiple languages, native libraries that enable easy streaming, machine learning, graph computing, and SQL - the Spark ecosystem offers some of the most extensive capabilities and features of any technology out there.
Other third-party contributions also make using Spark much easier and versatile.
Spark Advantages include
Fast processing – The most important feature of Apache Spark that has made the big data world choose this technology over others is its speed. Big data is characterized by volume, variety, velocity, and veracity which needs to be processed at a higher speed. Spark contains <span class="lx-highlight" data-idx="12" style="background-color:#D2E3FC;">Resilient Distributed Dataset (RDD)</span> which saves time in reading and writing operations, allowing it to run almost ten to one hundred times faster than Hadoop.
Flexibility – Apache Spark supports multiple languages and allows the developers to write applications in Java, Scala, R, or Python.
In-memory computing – Spark stores the data in the RAM of servers which allows quick access and in turn accelerates the speed of analytics.
Real-time processing – Spark is able to process real-time streaming data. Unlike MapReduce which processes only stored data, Spark is able to process real-time data and is, therefore, able to produce instant outcomes.
Better analytics – In contrast to MapReduce that I ncludes Map and Reduce functions, Spark includes much more than that. Apache Spark consists of a rich set of SQL queries, machine learning algorithms, complex analytics, etc. With all these functionalities, analytics can be performed in a better fashion with the help of Spark.
Spark is the most popular open-source distributed computing engine for big data analysis.
Used by data engineers and data scientists alike in thousands of organizations worldwide, Spark is the industry standard analytics engine for big data processing and machine learning. 
Spark enables you to process data at lightning speed for both batch and streaming workloads.
Spark can run on Hadoop, mongo DB, Cassandra Kubernetes, YARN, or standalone - and it works with a wide range of data inputs and outputs.
Apache Spark has seen immense growth over the past several years, becoming the most effective data processing and AI engine in enterprises today due to its speed, ease of use, and sophisticated analytics. However, the cost of Spark is high as it requires lots of RAM to run in-memory.
Spark unifies data and AI by simplifying data preparation at a massive scale across various sources. Moreover, it provides a consistent set of APIs for both data engineering and data science workloads, along with seamless integration of popular libraries such as TensorFlow, PyTorch, R and SciKit-Learn.
In this session we learned in memory computing represented by Spark.
Thank you for your attention, if you have any question, feel free to contact me.
      </div>
      <div class="lx-controls">
        <div class="lx-button-row">
          <button class="lx-control-btn" onclick="playPause()">▶️ Play</button>
          <button class="lx-control-btn" onclick="prevExtraction()">⏮ Previous</button>
          <button class="lx-control-btn" onclick="nextExtraction()">⏭ Next</button>
        </div>
        <div class="lx-progress-container">
          <input type="range" id="progressSlider" class="lx-progress-slider"
                 min="0" max="12" value="0"
                 onchange="jumpToExtraction(this.value)">
        </div>
        <div class="lx-status-text">
          Entity <span id="entityInfo">1/13</span> |
          Pos <span id="posInfo">[757-776]</span>
        </div>
      </div>
    </div>

    <script>
      (function() {
        const extractions = [{"index": 0, "class": "TOPIC", "text": "In-Memory Computing with Apache Spark", "color": "#C8E6C9", "startPos": 757, "endPos": 776, "beforeText": "ocessed in different scenarios, \nwhich include batch processing, stream computing, Large-scale concurrent processing (MPP) model for structured data, ", "extractionText": "In-memory Computing", "afterText": " model, and Data Flow Graph models.\nLet\u2019s look at the 4th computing model In-memory Computing model\nIn-memory computation (or in-memory computing) is ", "attributesHtml": "<div><strong>class:</strong> TOPIC</div><div><strong>attributes:</strong> {}</div>"}, {"index": 1, "class": "CONCEPT", "text": "Apache Spark", "color": "#D2E3FC", "startPos": 3315, "endPos": 3327, "beforeText": "nth of the number of the machines that Hadoop used and reduced the time from 72 minute to 23 minutes.\nFar more faster and more efficient than Hadoop.\n", "extractionText": "Apache Spark", "afterText": " is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching and optimized query execution for fast qu", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An open-source, distributed processing system used for big data workloads. It is a fast and general engine for large-scale data processing that utilizes in-memory caching and optimized query execution.</span>}</div>"}, {"index": 2, "class": "CONCEPT", "text": "RDD Action", "color": "#D2E3FC", "startPos": 4446, "endPos": 4477, "beforeText": "er Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. \nThere are only 2 kinds of operation of ", "extractionText": "RDD, transformations and action", "afterText": ". In transformation, data can be filter, joined map, reduced but no calculation is executed, only in action the calculation can be done, and the value", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An operation on an RDD that triggers the execution of all preceding transformations and returns a final result to the driver program or writes it to a file system. The return value is not an RDD.</span>}</div>"}, {"index": 3, "class": "CONCEPT", "text": "RDD Transformation", "color": "#D2E3FC", "startPos": 5585, "endPos": 5604, "beforeText": " action is executed, the rdd will be calculated and generated, which is the root of the lazy execution of rdd.\nThis diagram shows the topology of the ", "extractionText": "RDD transformations", "afterText": " and the linage of RDD transformation, which the executions should follow.\nSpark running process is like this: The Spark architecture uses the Master-", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An operation on an RDD (such as filter, join, map) that returns a new RDD. The calculation is not executed immediately due to lazy evolution; it only defines the computation steps.</span>}</div>"}, {"index": 4, "class": "CONCEPT", "text": "Spark Architecture", "color": "#D2E3FC", "startPos": 5719, "endPos": 5737, "beforeText": "topology of the RDD transformations and the linage of RDD transformation, which the executions should follow.\nSpark running process is like this: The ", "extractionText": "Spark architecture", "afterText": " uses the Master-Slave model in distributed computing. \nMaster is the node containing the Master process in the corresponding cluster, and Slave is th", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A Master-Slave model where a Master process controls the cluster, Worker nodes receive commands and report status, a Driver program controls the application&#x27;s execution and job scheduling, and Executors on worker nodes are responsible for task execution.</span>}</div>"}, {"index": 5, "class": "CONCEPT", "text": "In-memory Computation", "color": "#D2E3FC", "startPos": 7479, "endPos": 7500, "beforeText": "Driver keeps communication with Executor and cooperates in processing.\nFeatures of Spark include \nIn-memory Computation\nPySpark provides provision of ", "extractionText": "in-memory computation", "afterText": ". Computed results are stored in distributed memory (RAM) instead of stable storage (disk). It provides very fast computation\nLazy Evolution\nTransform", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">The technique of running computer calculations entirely in computer memory (e.g., in RAM), typically for large-scale, complex calculations on a cluster where computers pool their RAM. It improves performance by eliminating slow data access from hard disk drives or SSDs.</span>}</div>"}, {"index": 6, "class": "CONCEPT", "text": "Lazy Evolution", "color": "#D2E3FC", "startPos": 7626, "endPos": 7640, "beforeText": "of in-memory computation. Computed results are stored in distributed memory (RAM) instead of stable storage (disk). It provides very fast computation\n", "extractionText": "Lazy Evolution", "afterText": "\nTransformation in PySpark RDDs is lazy. It doesn&#x27;t compute the result immediately means that execution does not start until an action is triggered. W", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A feature of Spark where transformations on RDDs are not computed immediately. Execution does not start until an action is triggered, which helps optimize performance by reducing the number of queries.</span>}</div>"}, {"index": 7, "class": "CONCEPT", "text": "Spark Core", "color": "#D2E3FC", "startPos": 9026, "endPos": 9036, "beforeText": "le dataset but not individual element on the dataset. On the other hand, fine grained mean we can transform individual element on the dataset.\nApache ", "extractionText": "Spark Core", "afterText": "\u00a0\u2013 Spark Core is the underlying general execution engine for the Spark platform that all other functionality is built upon. It provides in-memory comp", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">The underlying general execution engine for the Spark platform that all other functionality is built upon. It provides in-memory computing and references datasets in external storage systems.</span>}</div>"}, {"index": 8, "class": "CONCEPT", "text": "Spark SQL", "color": "#D2E3FC", "startPos": 9246, "endPos": 9255, "beforeText": " the Spark platform that all other functionality is built upon. It provides in-memory computing and referencing datasets in external storage systems.\n", "extractionText": "Spark SQL", "afterText": "\u00a0\u2013 Spark SQL is Apache Spark\u2019s module for working with structured data. The interfaces offered by Spark SQL provides Spark with more information about", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A module in Apache Spark for working with structured data, providing more information about the structure of both the data and the computation being performed.</span>}</div>"}, {"index": 9, "class": "CONCEPT", "text": "Spark Streaming", "color": "#D2E3FC", "startPos": 9474, "endPos": 9489, "beforeText": "a. The interfaces offered by Spark SQL provides Spark with more information about the structure of both the data and the computation being performed.\n", "extractionText": "Spark Streaming", "afterText": "\u00a0\u2013 This component allows Spark to process real-time streaming data. Data can be ingested from many sources like Kafka, Flume, and HDFS (Hadoop Distrib", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A component that allows Spark to process real-time streaming data ingested from sources like Kafka, Flume, and HDFS.</span>}</div>"}, {"index": 10, "class": "CONCEPT", "text": "MLlib (Machine Learning Library)", "color": "#D2E3FC", "startPos": 9778, "endPos": 9810, "beforeText": "oop Distributed File System). Then the data can be processed using complex algorithms and pushed out to file systems, databases, and live dashboards.\n", "extractionText": "MLlib (Machine Learning Library)", "afterText": "\u00a0\u2013 Apache Spark is equipped with a rich library known as MLlib. This library contains a wide array of machine learning algorithms- classification, reg", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A rich library in Apache Spark that contains a wide array of machine learning algorithms, including classification, regression, clustering, and collaborative filtering, as well as tools for building and tuning ML pipelines.</span>}</div>"}, {"index": 11, "class": "CONCEPT", "text": "GraphX", "color": "#D2E3FC", "startPos": 10159, "endPos": 10165, "beforeText": " It also includes other tools for constructing, evaluating, and tuning ML Pipelines. All these functionalities help Spark scale out across a cluster.\n", "extractionText": "GraphX", "afterText": "\u00a0\u2013 Spark also comes with a library to manipulate graph databases and perform computations called GraphX. GraphX unifies ETL (Extract, Transform, and L", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A library in Spark used to manipulate graph databases and perform graph computations, unifying ETL, exploratory analysis, and iterative graph computation in a single system.</span>}</div>"}, {"index": 12, "class": "CONCEPT", "text": "Resilient Distributed Dataset (RDD)", "color": "#D2E3FC", "startPos": 11112, "endPos": 11147, "beforeText": "hers is its speed. Big data is characterized by volume, variety, velocity, and veracity which needs to be processed at a higher speed. Spark contains\u00a0", "extractionText": "Resilient Distributed Dataset (RDD)", "afterText": "\u00a0which saves time in reading and writing operations, allowing it to run almost\u00a0ten to one hundred times faster than Hadoop.\nFlexibility\u00a0\u2013 Apache Spark", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are fault-tolerant and can be persisted in memory for efficient reuse across operations.</span>}</div>"}];
        let currentIndex = 0;
        let isPlaying = false;
        let animationInterval = null;
        let animationSpeed = 1.0;

        function updateDisplay() {
          const extraction = extractions[currentIndex];
          if (!extraction) return;

          document.getElementById('attributesContainer').innerHTML = extraction.attributesHtml;
          document.getElementById('entityInfo').textContent = (currentIndex + 1) + '/' + extractions.length;
          document.getElementById('posInfo').textContent = '[' + extraction.startPos + '-' + extraction.endPos + ']';
          document.getElementById('progressSlider').value = currentIndex;

          const playBtn = document.querySelector('.lx-control-btn');
          if (playBtn) playBtn.textContent = isPlaying ? '⏸ Pause' : '▶️ Play';

          const prevHighlight = document.querySelector('.lx-text-window .lx-current-highlight');
          if (prevHighlight) prevHighlight.classList.remove('lx-current-highlight');
          const currentSpan = document.querySelector('.lx-text-window span[data-idx="' + currentIndex + '"]');
          if (currentSpan) {
            currentSpan.classList.add('lx-current-highlight');
            currentSpan.scrollIntoView({block: 'center', behavior: 'smooth'});
          }
        }

        function nextExtraction() {
          currentIndex = (currentIndex + 1) % extractions.length;
          updateDisplay();
        }

        function prevExtraction() {
          currentIndex = (currentIndex - 1 + extractions.length) % extractions.length;
          updateDisplay();
        }

        function jumpToExtraction(index) {
          currentIndex = parseInt(index);
          updateDisplay();
        }

        function playPause() {
          if (isPlaying) {
            clearInterval(animationInterval);
            isPlaying = false;
          } else {
            animationInterval = setInterval(nextExtraction, animationSpeed * 1000);
            isPlaying = true;
          }
          updateDisplay();
        }

        window.playPause = playPause;
        window.nextExtraction = nextExtraction;
        window.prevExtraction = prevExtraction;
        window.jumpToExtraction = jumpToExtraction;

        updateDisplay();
      })();
    </script>