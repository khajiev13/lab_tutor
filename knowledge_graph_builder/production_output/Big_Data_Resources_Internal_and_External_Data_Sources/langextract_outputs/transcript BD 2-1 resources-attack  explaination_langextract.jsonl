{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "Big Data Resources: Internal and External Data Sources", "char_interval": {"start_pos": 362, "end_pos": 459}, "alignment_status": "match_fuzzy", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "Big data resources can be classified by organizational boundary into internal and external data. The methods for acquiring each type differ. Internal data is created and stored within an organization, including structured data from business systems (ERP, CRM, finance), historical legacy data like documents and emails, and sensor data from internal IoT devices. External data originates outside the organization and includes data from other organizations, government-published data, and internet data from sources like social media, blogs, and Wikipedia. Government data is particularly valuable due to its high credibility and integrity. A typical internal data analysis workflow involves collecting data from various systems, storing it in databases (SQL Server, Hive, MongoDB), processing it with platforms like Hadoop or Spark, and analyzing it with tools like TensorFlow or R to gain business insights. Collecting external internet data presents challenges, including varied website structures, anti-crawler policies, diverse data formats (text, audio, video), and lower data quality and authenticity. Websites employ anti-crawler measures like robot detection, browser validity checks, and CC attack protection to manage web crawler traffic.", "char_interval": null, "alignment_status": null, "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "internal data, external data, data resources, big data analysis, data collection, internet data, government data, web crawlers, anti-crawler policies, CC attack", "char_interval": {"start_pos": 1101, "end_pos": 5036}, "alignment_status": "match_fuzzy", "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Internal Data Resources", "char_interval": {"start_pos": 497, "end_pos": 520}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "All the data created by an organization and stored inside it, including data from self-operated systems (business, manufacturing, finance), historical legacy data (documents, emails), and data from internal IoT devices."}}, {"extraction_class": "CONCEPT", "extraction_text": "External Data", "char_interval": {"start_pos": 302, "end_pos": 315}, "alignment_status": "match_fuzzy", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "Data from outside a specific organization, which includes data from other organizations' platforms, IoT devices, government-published data, and internet or mobile internet data from sources like social media, blogs, and Wikipedia."}}, {"extraction_class": "CONCEPT", "extraction_text": "Government Data", "char_interval": {"start_pos": 1763, "end_pos": 1778}, "alignment_status": "match_fuzzy", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "Data generated by government department systems for social management purposes, characterized by high credibility, good integrity, strong real-time performance, and clear entity descriptions."}}, {"extraction_class": "CONCEPT", "extraction_text": "Self-operated Data", "char_interval": {"start_pos": 2470, "end_pos": 2488}, "alignment_status": "match_fuzzy", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "Data from an organization's own IT systems, such as ERP, SCM, online office systems, and online transaction systems."}}, {"extraction_class": "CONCEPT", "extraction_text": "Web Crawlers", "char_interval": {"start_pos": 4287, "end_pos": 4299}, "alignment_status": "match_fuzzy", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "Tools that facilitate network information collection and querying, but can negatively impact websites by consuming server bandwidth, being used for DoS attacks, or stealing critical data."}}, {"extraction_class": "CONCEPT", "extraction_text": "Robot Detection", "char_interval": {"start_pos": 5179, "end_pos": 5194}, "alignment_status": "match_exact", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "An anti-crawler policy where a Web Application Firewall (WAF) identifies the User-Agent to detect and block threats like malicious crawlers, scanners, and web shells."}}, {"extraction_class": "CONCEPT", "extraction_text": "Anti-Crawler Protection", "char_interval": {"start_pos": 5347, "end_pos": 5370}, "alignment_status": "match_exact", "extraction_index": 10, "group_index": 9, "description": null, "attributes": {"definition": "An anti-crawler policy where a Web Application Firewall (WAF) dynamically analyzes website service models to identify crawler behavior and verify browser validity."}}, {"extraction_class": "CONCEPT", "extraction_text": "CC Attack Protection", "char_interval": {"start_pos": 5604, "end_pos": 5606}, "alignment_status": "match_lesser", "extraction_index": 11, "group_index": 10, "description": null, "attributes": {"definition": "An anti-crawler policy that limits access to a specific URL based on IP address, cookie, or referrer to mitigate the impact of CC attacks."}}, {"extraction_class": "CONCEPT", "extraction_text": "CC Attack", "char_interval": {"start_pos": 5677, "end_pos": 5686}, "alignment_status": "match_exact", "extraction_index": 12, "group_index": 11, "description": null, "attributes": {"definition": "A type of DDoS attack that uses a proxy server to send a large number of seemingly legitimate requests to a victim server."}}], "text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session, we will discuss\nSome idea about Data Resources.\nAccording to organization boundary, we can divide the data into internal data and external data. By the way， the reason we want to divide the big data resources into internal and external is, the methods and  tools of getting internal data and external data are different.\nThe internal data resources refer to all the data created by the organization and stored inside the organization, \nIncluding organization self operated system, business transaction system, manufacturing system, finance system, human resources system, etc. these are mainly structured data.\nOrganization internal data also include some historical legacy data, such as some documents data, emails and etc.\nBesides there are some internal IoT devices can generate some data, for example, there are lots of sensors in the manufacturing Automated production line, these sensors can collect many production status data, which can facilitate the manufacture analysis.\nExternal data refer to the data outside the specific organization.\nExternal data includes\n1 other organization business operating platform data, like data from other organization’s business transaction system, manufacturing system, finance system, human resources system, accounting system etc. \n2 other organization’s IoT devices generated data\n3 the government collected and published data \n4 internet or mobile internet data, published by all the internet nodes, like social media, blog data, YouTube data, Wikipedia data etc.\nAll these internal and external data could be the data resources for analysis.\nGovernment data, various departments are set up for social management purposes, such as the Public Security Bureau, the Ministry of Finance, etc., these organizations will build many business systems, for the purpose of effectively fulfilling the functions of the department. \nThe data generated by these systems is mainly stored in the corresponding data center in a specific structure. The data contains great value inside, which can provide data support for the formulation of government macro policies, national security prevention and control, and effective social management.\nGovernment data has the characteristics of high credibility, good integrity, strong real-time, and clear entity description.\nSelf-operated data refer to the IT system, ERP, SCM, online office system, online transaction system, etc. of different organization.\nIncorporate IoT data collection into the consideration of data enrichment. \nInside the organization, all the business data was collected from different business systems, like CRM, ERP, marketing system, finance system, HR system, Supply chain system and other data.\nAll these data were extracted and stored in the data storage, which could be the relational database like SQL Server, data warehouse like Hive, or document database like Mongo DB, or graph database like neo4j, \nand then process the data using data processing platform like Hadoop, spark etc.\nAnd analysis the data with the tools like TensorFlow, R, IBM Watson etc.\nAfter the data has been processed and analyzed, we can also visualize the data using visualization tools like tableau.\nThrough the big data analysis, we can get the business improvement, benefit analysis, revenue analysis, customer profiles, suitable pricing, some business patterns and so on, all these can help organization understand the customer and market better, provide better product and services, make better decision and improve their business strategy.\nIn addition to the internal data, there are also external data available for analysis.\nExternal data is mainly internet data, whose channel includes web portal, government open information, social media, E-business public data and some special forum focusing specific topic.\nInternet has huge amount of data, but when you collect data from internet you should be aware of some issues.\n） there are Different IT level and structure of different website, there is no unified collection method for all the websites.\n）Different websites have different control policy of crawlers, Why?\nBecause Web crawlers facilitate network information collection and query, but they also introduce the following negative impacts:\n1 Web crawlers always consume too much server bandwidth and increase server load as they use specific policies to browser as much information as possible.\n2 Bad actors may use web crawlers to launch DoS attacks against websites. As a result, websites may fail to provide normal services due to resource exhaustion.\n3 Bad actors may use web crawlers to steal mission-critical data on your websites, which will damage your economic interests.\n((Some WAF-web application firewall provides three anti-crawler policies,\n）robot detection by identifying User-Agent, 2）website anti-crawler by checking browser validity, and 3）CC attack protection by limiting the access frequency, to comprehensively mitigate crawler attacks against your websites.\n1 So some website will enable Robot Detection to Identify User-Agent\nIf you enable robot detection, WAF can detect and block threats such as malicious crawlers, scanners, and web shells.\n2 Enabling Anti-Crawler Protection to Verify Browser Validity\nIf you enable anti-crawler protection, WAF dynamically analyzes website service models and accurately identifies crawler behavior based on data risk control and bot identification approaches.\n3 Configuring CC-Challenge Collapsar挑战黑洞 Attack Protection to Limit Access Frequency\nA CC attack- 攻击是DDoS攻击的一种类型，使用代理服务器向受害服务器发送大量貌似合法的请求(Challenge the black hole attack is a type of DDoS attack that uses a proxy server to send a large number of seemingly legitimate requests to the victim server, protection rule uses a specific IP address, cookie, or referrer to limit the access to a specific path (URL), mitigating the impact of CC attacks on web services.))\n） the internet data could be form of text, tables, audio, and video, the different form increase the difficulty of collection. Because you need to design different ways to collect them.\n） Its authenticity and data quality are inferior to other data\nBecause anyone can publish data in the internet, nobody is responsible for the data authenticity and data quality, which make Its authenticity and data quality are inferior to other data, for example the internal data.\nLet’s summarize the data channels\nIf we want to do big data analysis, we need to have enough data, the data can be collected from inside or outside the organization, which are internal and external data.\nInternal data includes the business system data, archived data, like documents, emails etc. and organizational IoT data.\nExternal data includes government public data, other organization data, internet data, and outside IoT data.\nIn this session we learned the bigdata resources, internal and external data.\nthank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_8cf2c17b"}
