{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "Data Transformation and Integration Techniques", "char_interval": {"start_pos": 52, "end_pos": 56}, "alignment_status": "match_lesser", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "This session covers data transformation, which includes data integration and data transformation itself. Data integration involves combining data from multiple sources into a consistent storage by addressing three key tasks: pattern matching to map entities from different sources (e.g., A.cust-id = B.customer_no), handling data redundancy where attributes have different names or can be derived, and resolving data value conflicts arising from different representations, scales, or units (e.g., kg vs. pounds). Following integration, data transformation changes the data's form for more efficient analysis. This is achieved through several methods: smoothing or discretization to eliminate noise and reduce data volume; aggregation using functions like sum() or avg() to summarize detailed data (e.g., daily sales to monthly totals); data generalization to replace low-level data with higher-level concepts (e.g., street to city); data normalization to scale data into a specific range using techniques like Min-Max or Z-score normalization; and attribute construction to create new, useful attributes from existing ones (e.g., creating 'area' from 'width' and 'height').", "char_interval": {"start_pos": 158, "end_pos": 170}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "Data Integration, Data Transformation, Pattern Matching, Data Redundancy, Data Value Conflict, Aggregation, Data Generalization, Data Normalization, Attribute Construction", "char_interval": {"start_pos": 3496, "end_pos": 3512}, "alignment_status": "match_lesser", "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Data Integration", "char_interval": {"start_pos": 233, "end_pos": 249}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "The process of integrating data from multiple data sources into a consistent storage, which involves tasks like pattern matching, data redundancy processing, and resolving data value conflicts."}}, {"extraction_class": "CONCEPT", "extraction_text": "Pattern Matching", "char_interval": {"start_pos": 425, "end_pos": 441}, "alignment_status": "match_fuzzy", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "A data integration task that involves integrating metadata from different data sources to recognize and map real-world entities from those sources."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Redundancy", "char_interval": {"start_pos": 443, "end_pos": 458}, "alignment_status": "match_fuzzy", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "An issue in data integration where the same attribute may have different field names in different databases, or one attribute can be derived from another. This can be detected by correlation analysis."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Value Conflict", "char_interval": {"start_pos": 474, "end_pos": 493}, "alignment_status": "match_fuzzy", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "A problem where a real-world entity's attribute values from different data sources are different due to variations in representation, scale, or coding (e.g., metric vs. imperial units)."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Transform", "char_interval": {"start_pos": 3517, "end_pos": 3531}, "alignment_status": "match_exact", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "The process of changing data from one form to another to facilitate efficient analysis."}}, {"extraction_class": "CONCEPT", "extraction_text": "Aggregation", "char_interval": {"start_pos": 1978, "end_pos": 1989}, "alignment_status": "match_fuzzy", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "A data transformation technique that uses calculations like avg(), count(), sum(), min(), and max() to represent detailed data with a summary result, such as aggregating daily sales to get a monthly total."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Generalization", "char_interval": {"start_pos": 2295, "end_pos": 2314}, "alignment_status": "match_fuzzy", "extraction_index": 10, "group_index": 9, "description": null, "attributes": {"definition": "A transformation method that replaces low-level data objects with more abstract or higher-level concepts, such as generalizing a street to a city or a specific age to an age group like 'young' or 'old'."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Normalization", "char_interval": {"start_pos": 2666, "end_pos": 2684}, "alignment_status": "match_fuzzy", "extraction_index": 11, "group_index": 10, "description": null, "attributes": {"definition": "The process of scaling data proportionally to make it fall into a specific area (e.g., [-1.0, 1.0]) to eliminate deviations in mining results caused by different attribute scales. Methods include Min-Max and Z-score normalization."}}, {"extraction_class": "CONCEPT", "extraction_text": "Attribute Construction", "char_interval": {"start_pos": 3158, "end_pos": 3180}, "alignment_status": "match_fuzzy", "extraction_index": 12, "group_index": 11, "description": null, "attributes": {"definition": "A transformation technique where new attributes are constructed from an existing attribute set to help discover deeper patterns and improve mining accuracy, for example, creating an 'area' attribute from 'width' and 'height'."}}], "text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session, we will discuss  Data transform .\nData transform can include Data integration and Data transform. Let’s look at Data integration first.\nData integration: \nIntegrate data from multiple data sources into a consistent storage, which could involve the task Pattern matching, Data redundancy processing and Data value conflict solving;\nLet’s look these 3 tasks one by one. \nFirst one is Pattern matching. Integrate metadata from different data sources.\nWe can recognize the real-world entity from different data sources and mapping them together, \nlike A.cust-id=B.customer_no.\nThen deal with Data redundancy issue,\nThe same attribute will have different field names in different databases.\nOne attribute can be derived from another attribute. For example, the average monthly income attribute in a customer data table can be calculated based on the monthly income attribute.\nAnd Some redundancy can be detected by correlation analysis.\nThen solve the Data value conflict problem.\nFor a real-world entity, its attribute values from different data sources may be different.\nSuch as Differences in representation, different scales, or differences in coding, etc. \nFor example: \nthe weight attribute uses the metric system, like kg, g in one system, but uses the imperial system like pound in another system. \nSame price attributes in different locations using different currency units, $, pound, RMB\nAfter data integration , then let’ come to data transform.\nData transform means, in order to facilitate efficient analysis , we need change the data from one form to another form.\nWe can use smooth methods like Binning Clustering Regression to eliminate the noise Or discretize continuous data, and increase granularity.\nBy doing this, we can reduce the data amount for further analysis.\nSecond transformation is Aggregation. \nBy doing avg(), count(), sum(), min(), max()...calculation, \nwe can use calculation result to represent the detail data. \nFor example: daily sales (data) can be aggregated to get the monthly or annual total.\nAnd these aggregation data are more convenient for big data analysis.\nThird  transformation is Data generalization. Which means Replace low-level data objects with more abstract (higher-level) concepts.\nFor example: \nstreet attributes can be generalized to higher-level concepts, such as: city, country. \nSimilarly, numeric attributes, such as age attributes, can be mapped to higher-level concepts, such as young, middle-aged, and old.\nThe fourth transformation is Data Normalization. \nWhich means The data is scaled proportionally to make it fall into a specific area, so as to eliminate the deviation of the mining results caused by the different sizes of the numerical attributes. \nSuch as mapping the salary income attribute value to the range of [-1.0,1.0].\nThe methods for Data Normalization could be \n(1) Min-Max normalization\n(2) Zero-mean normalization (z-score normalization)\n(3) Standardization of decimal calibration\nThe fifth transformation is Attribute construction. \nUse the existing attribute set to construct new attributes and add them to the existing attribute set to help dig deeper pattern knowledge and improve the accuracy of mining results.\nFor example: According to the width and height attributes, a new attribute can be constructed: area.\nIn this session we discussed Data integration and data transform.\nthank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_33115bd3"}
