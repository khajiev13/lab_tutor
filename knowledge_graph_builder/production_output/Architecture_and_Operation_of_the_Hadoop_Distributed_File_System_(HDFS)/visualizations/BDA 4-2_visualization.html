<style>
.lx-highlight { position: relative; border-radius:3px; padding:1px 2px;}
.lx-highlight .lx-tooltip {
  visibility: hidden;
  opacity: 0;
  transition: opacity 0.2s ease-in-out;
  background: #333;
  color: #fff;
  text-align: left;
  border-radius: 4px;
  padding: 6px 8px;
  position: absolute;
  z-index: 1000;
  bottom: 125%;
  left: 50%;
  transform: translateX(-50%);
  font-size: 12px;
  max-width: 240px;
  white-space: normal;
  box-shadow: 0 2px 6px rgba(0,0,0,0.3);
}
.lx-highlight:hover .lx-tooltip { visibility: visible; opacity:1; }
.lx-animated-wrapper { max-width: 100%; font-family: Arial, sans-serif; }
.lx-controls {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 8px;
  padding: 12px; margin-bottom: 16px;
}
.lx-button-row {
  display: flex; justify-content: center; gap: 8px; margin-bottom: 12px;
}
.lx-control-btn {
  background: #4285f4; color: white; border: none; border-radius: 4px;
  padding: 8px 16px; cursor: pointer; font-size: 13px; font-weight: 500;
  transition: background-color 0.2s;
}
.lx-control-btn:hover { background: #3367d6; }
.lx-progress-container {
  margin-bottom: 8px;
}
.lx-progress-slider {
  width: 100%; margin: 0; appearance: none; height: 6px;
  background: #ddd; border-radius: 3px; outline: none;
}
.lx-progress-slider::-webkit-slider-thumb {
  appearance: none; width: 18px; height: 18px; background: #4285f4;
  border-radius: 50%; cursor: pointer;
}
.lx-progress-slider::-moz-range-thumb {
  width: 18px; height: 18px; background: #4285f4; border-radius: 50%;
  cursor: pointer; border: none;
}
.lx-status-text {
  text-align: center; font-size: 12px; color: #666; margin-top: 4px;
}
.lx-text-window {
  font-family: monospace; white-space: pre-wrap; border: 1px solid #90caf9;
  padding: 12px; max-height: 260px; overflow-y: auto; margin-bottom: 12px;
  line-height: 1.6;
}
.lx-attributes-panel {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 6px;
  padding: 8px 10px; margin-top: 8px; font-size: 13px;
}
.lx-current-highlight {
  border-bottom: 4px solid #ff4444;
  font-weight: bold;
  animation: lx-pulse 1s ease-in-out;
}
@keyframes lx-pulse {
  0% { text-decoration-color: #ff4444; }
  50% { text-decoration-color: #ff0000; }
  100% { text-decoration-color: #ff4444; }
}
.lx-legend {
  font-size: 12px; margin-bottom: 8px;
  padding-bottom: 8px; border-bottom: 1px solid #e0e0e0;
}
.lx-label {
  display: inline-block;
  padding: 2px 4px;
  border-radius: 3px;
  margin-right: 4px;
  color: #000;
}
.lx-attr-key {
  font-weight: 600;
  color: #1565c0;
  letter-spacing: 0.3px;
}
.lx-attr-value {
  font-weight: 400;
  opacity: 0.85;
  letter-spacing: 0.2px;
}

/* Add optimizations with larger fonts and better readability for GIFs */
.lx-gif-optimized .lx-text-window { font-size: 16px; line-height: 1.8; }
.lx-gif-optimized .lx-attributes-panel { font-size: 15px; }
.lx-gif-optimized .lx-current-highlight { text-decoration-thickness: 4px; }
</style>
    <div class="lx-animated-wrapper lx-gif-optimized">
      <div class="lx-attributes-panel">
        <div class="lx-legend">Highlights Legend: <span class="lx-label" style="background-color:#D2E3FC;">CONCEPT</span> <span class="lx-label" style="background-color:#C8E6C9;">SUMMARY</span> <span class="lx-label" style="background-color:#FEF0C3;">TOPIC</span></div>
        <div id="attributesContainer"></div>
      </div>
      <div class="lx-text-window" id="textWindow">
        Hello everyone, I am Haiying Che, from  Institute of Data Science and knowledge Engineering
School of Computer Science, in Beijing Institute of Technology , in this session we discuss about distributed file system .
<span class="lx-highlight lx-current-highlight" data-idx="0" style="background-color:#C8E6C9;">The</span> big data computing system can be summarized into three categories: 
Data storing system, Data processing system, Data application system
The data storage architecture is the foundation of big data computing. 
In data storing system , there are 4 parts to accomplish different tasks, 
which are Data collection and modeling, Distributed file system, Distributed database/data warehouse and  Unified Data Access Interface.
we learned data collection and modeling, now we focus on <span class="lx-highlight" data-idx="1" style="background-color:#D2E3FC;">distributed file system</span>.
Actually File system could be Centralized or distributed file system, but in the big data scenario, mostly we use distributed file system to achieve the scale and the efficiency.
The distributed file system provides a physical storage architecture for data.
File system with data stored on servers.  In distributed file system the data is accessed and processed as if it was stored on the local client machine. 
It is Convenient to share information and files among users on a network in a controlled and authorized way, which is transparent to the users.
And it reaches multiplied storage of a single server.
At present, there are the two main file systems in the big data computing architecture, which are
the open source community’s architecture HDFS and Google’s <span class="lx-highlight" data-idx="2" style="background-color:#D2E3FC;">GFS (Google file system)</span> which has evolved into Colossus.
Let ‘s look at the HDFS
HDFS adopts a <span class="lx-highlight" data-idx="3" style="background-color:#D2E3FC;">master-slave</span> structure. And HDFS save 3 copies of each <span class="lx-highlight" data-idx="4" style="background-color:#D2E3FC;">data block</span>, which means use redundancy to achieve high availability.
An HDFS cluster includes a Name node, which is the master node, and several <span class="lx-highlight" data-idx="5" style="background-color:#D2E3FC;">Data</span> Nodes, which are slave nodes.
As the central service node, the Name node is responsible for managing the file system namespace, the mapping relationship from data files to data blocks to Data nodes, 
and client scheduling of file access. And the metadata is stored in memory for quick access.
HDFS also has a secondary name node, which is regularly connected to the primary name node, and the instant image of the system directory is stored on the local disk.
When the primary name node is fails or crashes, the <span class="lx-highlight" data-idx="6" style="background-color:#D2E3FC;">secondary name node can provide the name node rollback recovery and restart functions.
Data node Store file data block, Realize the mapping of data blocks to the local file system of the data node and Data blocks are stored on the local disk
In HDFS, each storage file is first divided into multiple data blocks with a fixed length of 64MB or 128MB, 
these data blocks are replicated to 3 copies and stored on different Data nodes according to a certain rule.
When one data node crashes, we can still retrieve the same data blocks from other 2 copies of another 2 data nodes.
Once the data is written, it can’t be changed , so the data in HDFS is immutable. So HDFS just support batch reading <span class="lx-highlight" data-idx="7" style="background-color:#FEF0C3;">and writing operation, but doesn’t support updating operation.
This means that a Data Node can store data blocks from different files. 
Each data node runs a node program or process, which is responsible for processing read and write requests from the file system client.
The creation, deletion, and replication of data blocks are performed under the unified scheduling of the Name node. 
The master node Name node and the slave node Data node perform their respective tasks
Now let’s understand the Process of writing data in HDFS , The diagram summarizes file write operation in Hadoop.
The client creates the file by calling create() method on DistributedFileSystem.
DistributedFileSystem makes an RPC call to the <span class="lx-highlight" data-idx="8" style="background-color:#D2E3FC;">namenode</span></span> to create a new file in the filesystem’s namespace, with no blocks associated with it.
The namenode performs various checks to make sure the file doesn’t already exist and the client has the right permissions to create the file. If all these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an IOException. TheDistributedFileSystem returns an FSDataOutputStream for the client to start writing data to <span class="lx-highlight" data-idx="9" style="background-color:#D2E3FC;">datanode</span>. FSDataOutputStream wraps a DFSOutputStream which handles communication with the datanodes and namenode.
As the client writes data, DFSOutputStream splits it into packets, which it writes to an internal queue, called the data queue. The data queue is consumed by the DataStreamer, which is responsible for asking the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas. The list of datanodes forms a pipeline, and default replication level is three, so there are three nodes in the pipeline. The DataStreamer streams the packets to the first datanode in the pipeline, which stores the packet and forwards it to the second datanode in the pipeline.
Similarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipeline.
DFSOutputStream also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the ack queue. A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline.
When the client has finished writing data, it calls close() on the stream.It flushes all the remaining packets to the datanode pipeline and waits for acknowledgments before contacting the namenode to signal that the file is complete The namenode already knows which blocks the file is made up of , so it only has to wait for blocks to be minimally replicated before returning successfully.
The Process of reading data in HDFS 2.0 is like this, 
The client opens the file by calling open() method on DistributedFileSystem.
DistributedFileSystem makes an RPC call to the namenode to determine location of datanodes where files is stored in form of blocks.For each blocks,the namenode returns address of datanodes(metadata of blocks and datanodes) that have a copy of block. Datanodes are sorted according to proximity(depending of network topology information).
The DistributedFileSystem returns an FSDataInputStream (an input stream that supports file seeks) to the client for it to read data from. FSDataInputStream in turn wraps a DFSInputStream, which manages the datanode and namenode I/O.
The client then calls read() on the stream. DFSInputStream, which has stored the datanode addresses for the first few blocks in the file, then connects to the first (closest) datanode for the first block in the file. 
Data is streamed from the datanode back to the client (in the form of packets) and read () is repeatedly called on the stream by client.
When the end of the block is reached, DFSInputStream will close the connection to the datanode, then find the best datanode for the next block (Step 5)
When the client has finished reading, it calls close() on the FSDataInputStream (step 6).
In addition, During reading, if the DFSInputStream encounters an error while communicating with a datanode, it will try the next closest one for that block.It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks. 
The DFSInputStream also verifies checksums for the data transferred to it from the datanode. If a corrupted block is found, the DFSInputStream attempts to read a replica of the block from another datanode; it also reports the corrupted block to the namenode. 
In this session we learned the big data distributed file system mechanism using HDFS</span> as an example, which is physical store of big data .
We learned the architecture of HDFS, name node, data node, and their responsibilities, 
We also learned the data writing and data reading process of HDFS.
thank you for your attention, if you have any question, feel free to contact me.
      </div>
      <div class="lx-controls">
        <div class="lx-button-row">
          <button class="lx-control-btn" onclick="playPause()">▶️ Play</button>
          <button class="lx-control-btn" onclick="prevExtraction()">⏮ Previous</button>
          <button class="lx-control-btn" onclick="nextExtraction()">⏭ Next</button>
        </div>
        <div class="lx-progress-container">
          <input type="range" id="progressSlider" class="lx-progress-slider"
                 min="0" max="9" value="0"
                 onchange="jumpToExtraction(this.value)">
        </div>
        <div class="lx-status-text">
          Entity <span id="entityInfo">1/10</span> |
          Pos <span id="posInfo">[2996-7546]</span>
        </div>
      </div>
    </div>

    <script>
      (function() {
        const extractions = [{"index": 0, "class": "SUMMARY", "text": "The lecture introduces distributed file systems as the foundational physical storage architecture for big data computing, contrasting them with centralized systems. It focuses on the Hadoop Distributed File System (HDFS), which, like Google's GFS, is a key technology in this area. HDFS employs a master-slave architecture consisting of a single master NameNode and multiple slave DataNodes. The NameNode manages the file system's namespace, metadata, and the mapping of files to data blocks, storing this information in memory for fast access. DataNodes store the actual data blocks on their local disks. To ensure high availability, HDFS replicates each data block (typically 3 copies) across different DataNodes. Data in HDFS is immutable, supporting batch read/write operations but not updates. The session details the HDFS data writing process, where a client communicates with the NameNode to create a file, then streams data through a pipeline of DataNodes for replication. It also explains the data reading process, where the client first gets block locations from the NameNode and then reads data directly from the closest DataNode, with built-in mechanisms for handling node failures and data corruption.", "color": "#C8E6C9", "startPos": 216, "endPos": 219, "beforeText": "and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology , in this session we discuss about distributed file system .\n", "extractionText": "The", "afterText": " big data computing system can be summarized into three categories: \nData storing system, Data processing system, Data application system\nThe data sto", "attributesHtml": "<div><strong>class:</strong> SUMMARY</div><div><strong>attributes:</strong> {}</div>"}, {"index": 1, "class": "CONCEPT", "text": "Distributed File System", "color": "#D2E3FC", "startPos": 698, "endPos": 721, "beforeText": "ributed file system, Distributed database/data warehouse and  Unified Data Access Interface.\nwe learned data collection and modeling, now we focus on ", "extractionText": "distributed file system", "afterText": ".\nActually File system could be Centralized or distributed file system, but in the big data scenario, mostly we use distributed file system to achieve", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A file system where data is stored on servers but is accessed and processed as if it was stored on the local client machine, allowing users on a network to share information and files in a controlled, authorized, and transparent way.</span>}</div>"}, {"index": 2, "class": "CONCEPT", "text": "GFS (Google File System)", "color": "#D2E3FC", "startPos": 1490, "endPos": 1514, "beforeText": "ent, there are the two main file systems in the big data computing architecture, which are\nthe open source community\u2019s architecture HDFS and Google\u2019s ", "extractionText": "GFS (Google file system)", "afterText": " which has evolved into Colossus.\nLet \u2018s look at the HDFS\nHDFS adopts a master-slave structure. And HDFS save 3 copies of each data block, which means", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A distributed file system developed by Google, which has evolved into Colossus, and is one of the main file systems in big data computing architecture alongside HDFS.</span>}</div>"}, {"index": 3, "class": "CONCEPT", "text": "Master-Slave Architecture", "color": "#D2E3FC", "startPos": 1586, "endPos": 1598, "beforeText": "pen source community\u2019s architecture HDFS and Google\u2019s GFS (Google file system) which has evolved into Colossus.\nLet \u2018s look at the HDFS\nHDFS adopts a ", "extractionText": "master-slave", "afterText": " structure. And HDFS save 3 copies of each data block, which means use redundancy to achieve high availability.\nAn HDFS cluster includes a Name node, ", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An architectural model used by HDFS, consisting of a master node (NameNode) and several slave nodes (DataNodes) that perform their respective tasks under the master&#x27;s coordination.</span>}</div>"}, {"index": 4, "class": "CONCEPT", "text": "Data Block", "color": "#D2E3FC", "startPos": 1641, "endPos": 1651, "beforeText": "FS (Google file system) which has evolved into Colossus.\nLet \u2018s look at the HDFS\nHDFS adopts a master-slave structure. And HDFS save 3 copies of each ", "extractionText": "data block", "afterText": ", which means use redundancy to achieve high availability.\nAn HDFS cluster includes a Name node, which is the master node, and several Data Nodes, whi", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">In HDFS, a fixed-length segment of a file (e.g., 64MB or 128MB). Files are divided into these blocks, which are then replicated and stored on different DataNodes.</span>}</div>"}, {"index": 5, "class": "CONCEPT", "text": "Data Immutability", "color": "#D2E3FC", "startPos": 1786, "endPos": 1790, "beforeText": "each data block, which means use redundancy to achieve high availability.\nAn HDFS cluster includes a Name node, which is the master node, and several ", "extractionText": "Data", "afterText": " Nodes, which are slave nodes.\nAs the central service node, the Name node is responsible for managing the file system namespace, the mapping relations", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A characteristic of data in HDFS where once data is written, it cannot be changed. HDFS supports batch reading and writing but does not support updating operations on existing data.</span>}</div>"}, {"index": 6, "class": "CONCEPT", "text": "Secondary NameNode", "color": "#D2E3FC", "startPos": 2303, "endPos": 3721, "beforeText": "the primary name node, and the instant image of the system directory is stored on the local disk.\nWhen the primary name node is fails or crashes, the ", "extractionText": "secondary name node can provide the name node rollback recovery and restart functions.\nData node Store file data block, Realize the mapping of data blocks to the local file system of the data node and Data blocks are stored on the local disk\nIn HDFS, each storage file is first divided into multiple data blocks with a fixed length of 64MB or 128MB, \nthese data blocks are replicated to 3 copies and stored on different Data nodes according to a certain rule.\nWhen one data node crashes, we can still retrieve the same data blocks from other 2 copies of another 2 data nodes.\nOnce the data is written, it can\u2019t be changed , so the data in HDFS is immutable. So HDFS just support batch reading and writing operation, but doesn\u2019t support updating operation.\nThis means that a Data Node can store data blocks from different files. \nEach data node runs a node program or process, which is responsible for processing read and write requests from the file system client.\nThe creation, deletion, and replication of data blocks are performed under the unified scheduling of the Name node. \nThe master node Name node and the slave node Data node perform their respective tasks\nNow let\u2019s understand the Process of writing data in HDFS , The diagram summarizes file write operation in Hadoop.\nThe client creates the file by calling create() method on DistributedFileSystem.\nDistributedFileSystem makes an RPC call to the namenode", "afterText": " to create a new file in the filesystem\u2019s namespace, with no blocks associated with it.\nThe namenode performs various checks to make sure the file doe", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A component in HDFS that regularly connects to the primary NameNode to store an instant image of the system directory on its local disk, providing rollback recovery and restart functions if the primary NameNode fails.</span>}</div>"}, {"index": 7, "class": "TOPIC", "text": "Architecture and Operation of the Hadoop Distributed File System (HDFS)", "color": "#FEF0C3", "startPos": 2996, "endPos": 7546, "beforeText": " copies of another 2 data nodes.\nOnce the data is written, it can\u2019t be changed , so the data in HDFS is immutable. So HDFS just support batch reading ", "extractionText": "and writing operation, but doesn\u2019t support updating operation.\nThis means that a Data Node can store data blocks from different files. \nEach data node runs a node program or process, which is responsible for processing read and write requests from the file system client.\nThe creation, deletion, and replication of data blocks are performed under the unified scheduling of the Name node. \nThe master node Name node and the slave node Data node perform their respective tasks\nNow let\u2019s understand the Process of writing data in HDFS , The diagram summarizes file write operation in Hadoop.\nThe client creates the file by calling create() method on DistributedFileSystem.\nDistributedFileSystem makes an RPC call to the namenode to create a new file in the filesystem\u2019s namespace, with no blocks associated with it.\nThe namenode performs various checks to make sure the file doesn\u2019t already exist and the client has the right permissions to create the file. If all these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an IOException. TheDistributedFileSystem returns an FSDataOutputStream for the client to start writing data to datanode. FSDataOutputStream wraps a DFSOutputStream which handles communication with the datanodes and namenode.\nAs the client writes data, DFSOutputStream splits it into packets, which it writes to an internal queue, called the data queue. The data queue is consumed by the DataStreamer, which is responsible for asking the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas. The list of datanodes forms a pipeline, and default replication level is three, so there are three nodes in the pipeline. The DataStreamer streams the packets to the first datanode in the pipeline, which stores the packet and forwards it to the second datanode in the pipeline.\nSimilarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipeline.\nDFSOutputStream also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the ack queue. A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline.\nWhen the client has finished writing data, it calls close() on the stream.It flushes all the remaining packets to the datanode pipeline and waits for acknowledgments before contacting the namenode to signal that the file is complete The namenode already knows which blocks the file is made up of , so it only has to wait for blocks to be minimally replicated before returning successfully.\nThe Process of reading data in HDFS 2.0 is like this, \nThe client opens the file by calling\u00a0open() method\u00a0on DistributedFileSystem.\nDistributedFileSystem makes\u00a0an RPC call to the namenode to determine location of datanodes\u00a0where files is stored in form of blocks.For each blocks,the namenode returns address of datanodes(metadata of blocks and datanodes) that have a copy of block. Datanodes are sorted according to proximity(depending of network topology information).\nThe DistributedFileSystem returns an\u00a0FSDataInputStream\u00a0(an input stream that supports file seeks) to the client for it to read data from. FSDataInputStream in turn wraps a DFSInputStream, which manages the datanode and namenode I/O.\nThe client then calls\u00a0read() on the stream. DFSInputStream, which has stored the datanode addresses for the first few blocks in the file, then connects to the first (closest) datanode for the first block in the file.\u00a0\nData is streamed from the datanode back to the client (in the form of packets) and\u00a0read () is repeatedly called on the stream by client.\nWhen the end of the block is reached, DFSInputStream will close the connection to the datanode, then find the\u00a0best datanode for the next block\u00a0(Step 5)\nWhen the client has finished reading, it calls close() on the FSDataInputStream (step 6).\nIn addition, During reading, if the DFSInputStream encounters an error while communicating with a datanode, it will try the next closest one for that block.It will also remember datanodes that have failed so that it doesn\u2019t needlessly retry them for later blocks.\u00a0\nThe DFSInputStream also verifies checksums for the data transferred to it from the datanode. If a corrupted block is found, the DFSInputStream attempts to read a replica of the block from another datanode; it also reports the corrupted block to the namenode.\u00a0\nIn this session we learned the big data distributed file system mechanism using HDFS", "afterText": " as an example, which is physical store of big data .\nWe learned the architecture of HDFS, name node, data node, and their responsibilities, \nWe also ", "attributesHtml": "<div><strong>class:</strong> TOPIC</div><div><strong>attributes:</strong> {}</div>"}, {"index": 8, "class": "CONCEPT", "text": "NameNode", "color": "#D2E3FC", "startPos": 3713, "endPos": 3721, "beforeText": " operation in Hadoop.\nThe client creates the file by calling create() method on DistributedFileSystem.\nDistributedFileSystem makes an RPC call to the ", "extractionText": "namenode", "afterText": " to create a new file in the filesystem\u2019s namespace, with no blocks associated with it.\nThe namenode performs various checks to make sure the file doe", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">The master node in an HDFS cluster responsible for managing the file system namespace, the mapping of data files to data blocks to DataNodes, and scheduling client file access. It stores metadata in memory for quick access.</span>}</div>"}, {"index": 9, "class": "CONCEPT", "text": "DataNode", "color": "#D2E3FC", "startPos": 4189, "endPos": 4197, "beforeText": "reation fails and the client is thrown an IOException. TheDistributedFileSystem returns an FSDataOutputStream for the client to start writing data to ", "extractionText": "datanode", "afterText": ". FSDataOutputStream wraps a DFSOutputStream which handles communication with the datanodes and namenode.\nAs the client writes data, DFSOutputStream s", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">The slave nodes in an HDFS cluster that store file data blocks on their local disks. They are responsible for processing read and write requests from clients and perform creation, deletion, and replication of data blocks under the unified scheduling of the NameNode.</span>}</div>"}];
        let currentIndex = 0;
        let isPlaying = false;
        let animationInterval = null;
        let animationSpeed = 1.0;

        function updateDisplay() {
          const extraction = extractions[currentIndex];
          if (!extraction) return;

          document.getElementById('attributesContainer').innerHTML = extraction.attributesHtml;
          document.getElementById('entityInfo').textContent = (currentIndex + 1) + '/' + extractions.length;
          document.getElementById('posInfo').textContent = '[' + extraction.startPos + '-' + extraction.endPos + ']';
          document.getElementById('progressSlider').value = currentIndex;

          const playBtn = document.querySelector('.lx-control-btn');
          if (playBtn) playBtn.textContent = isPlaying ? '⏸ Pause' : '▶️ Play';

          const prevHighlight = document.querySelector('.lx-text-window .lx-current-highlight');
          if (prevHighlight) prevHighlight.classList.remove('lx-current-highlight');
          const currentSpan = document.querySelector('.lx-text-window span[data-idx="' + currentIndex + '"]');
          if (currentSpan) {
            currentSpan.classList.add('lx-current-highlight');
            currentSpan.scrollIntoView({block: 'center', behavior: 'smooth'});
          }
        }

        function nextExtraction() {
          currentIndex = (currentIndex + 1) % extractions.length;
          updateDisplay();
        }

        function prevExtraction() {
          currentIndex = (currentIndex - 1 + extractions.length) % extractions.length;
          updateDisplay();
        }

        function jumpToExtraction(index) {
          currentIndex = parseInt(index);
          updateDisplay();
        }

        function playPause() {
          if (isPlaying) {
            clearInterval(animationInterval);
            isPlaying = false;
          } else {
            animationInterval = setInterval(nextExtraction, animationSpeed * 1000);
            isPlaying = true;
          }
          updateDisplay();
        }

        window.playPause = playPause;
        window.nextExtraction = nextExtraction;
        window.prevExtraction = prevExtraction;
        window.jumpToExtraction = jumpToExtraction;

        updateDisplay();
      })();
    </script>