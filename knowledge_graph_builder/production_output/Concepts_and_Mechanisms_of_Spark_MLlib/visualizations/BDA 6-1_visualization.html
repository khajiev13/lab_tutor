<style>
.lx-highlight { position: relative; border-radius:3px; padding:1px 2px;}
.lx-highlight .lx-tooltip {
  visibility: hidden;
  opacity: 0;
  transition: opacity 0.2s ease-in-out;
  background: #333;
  color: #fff;
  text-align: left;
  border-radius: 4px;
  padding: 6px 8px;
  position: absolute;
  z-index: 1000;
  bottom: 125%;
  left: 50%;
  transform: translateX(-50%);
  font-size: 12px;
  max-width: 240px;
  white-space: normal;
  box-shadow: 0 2px 6px rgba(0,0,0,0.3);
}
.lx-highlight:hover .lx-tooltip { visibility: visible; opacity:1; }
.lx-animated-wrapper { max-width: 100%; font-family: Arial, sans-serif; }
.lx-controls {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 8px;
  padding: 12px; margin-bottom: 16px;
}
.lx-button-row {
  display: flex; justify-content: center; gap: 8px; margin-bottom: 12px;
}
.lx-control-btn {
  background: #4285f4; color: white; border: none; border-radius: 4px;
  padding: 8px 16px; cursor: pointer; font-size: 13px; font-weight: 500;
  transition: background-color 0.2s;
}
.lx-control-btn:hover { background: #3367d6; }
.lx-progress-container {
  margin-bottom: 8px;
}
.lx-progress-slider {
  width: 100%; margin: 0; appearance: none; height: 6px;
  background: #ddd; border-radius: 3px; outline: none;
}
.lx-progress-slider::-webkit-slider-thumb {
  appearance: none; width: 18px; height: 18px; background: #4285f4;
  border-radius: 50%; cursor: pointer;
}
.lx-progress-slider::-moz-range-thumb {
  width: 18px; height: 18px; background: #4285f4; border-radius: 50%;
  cursor: pointer; border: none;
}
.lx-status-text {
  text-align: center; font-size: 12px; color: #666; margin-top: 4px;
}
.lx-text-window {
  font-family: monospace; white-space: pre-wrap; border: 1px solid #90caf9;
  padding: 12px; max-height: 260px; overflow-y: auto; margin-bottom: 12px;
  line-height: 1.6;
}
.lx-attributes-panel {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 6px;
  padding: 8px 10px; margin-top: 8px; font-size: 13px;
}
.lx-current-highlight {
  border-bottom: 4px solid #ff4444;
  font-weight: bold;
  animation: lx-pulse 1s ease-in-out;
}
@keyframes lx-pulse {
  0% { text-decoration-color: #ff4444; }
  50% { text-decoration-color: #ff0000; }
  100% { text-decoration-color: #ff4444; }
}
.lx-legend {
  font-size: 12px; margin-bottom: 8px;
  padding-bottom: 8px; border-bottom: 1px solid #e0e0e0;
}
.lx-label {
  display: inline-block;
  padding: 2px 4px;
  border-radius: 3px;
  margin-right: 4px;
  color: #000;
}
.lx-attr-key {
  font-weight: 600;
  color: #1565c0;
  letter-spacing: 0.3px;
}
.lx-attr-value {
  font-weight: 400;
  opacity: 0.85;
  letter-spacing: 0.2px;
}

/* Add optimizations with larger fonts and better readability for GIFs */
.lx-gif-optimized .lx-text-window { font-size: 16px; line-height: 1.8; }
.lx-gif-optimized .lx-attributes-panel { font-size: 15px; }
.lx-gif-optimized .lx-current-highlight { text-decoration-thickness: 4px; }
</style>
    <div class="lx-animated-wrapper lx-gif-optimized">
      <div class="lx-attributes-panel">
        <div class="lx-legend">Highlights Legend: <span class="lx-label" style="background-color:#D2E3FC;">CONCEPT</span> <span class="lx-label" style="background-color:#C8E6C9;">KEYWORDS</span> <span class="lx-label" style="background-color:#FEF0C3;">SUMMARY</span> <span class="lx-label" style="background-color:#F9DEDC;">TOPIC</span></div>
        <div id="attributesContainer"></div>
      </div>
      <div class="lx-text-window" id="textWindow">
        Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering
School of Computer Science, in Beijing Institute of Technology, from this session on, we will discuss something about real application,
Including some platform and big data application and in this session, we will talk about the <span class="lx-highlight lx-current-highlight" data-idx="0" style="background-color:#F9DEDC;">concepts and</span> mechanism of <span class="lx-highlight" data-idx="1" style="background-color:#D2E3FC;">Spark MLlib</span>.
From <span class="lx-highlight" data-idx="2" style="background-color:#FEF0C3;">this</span> chapter ,we will introduce some useful platform， Spark  MLlib and  TensorFlow.
We will show some case experiments applying the corresponding platform; 
and we will also explain 2 kinds of typical big data analysis scenarios, Recommendation System and Social Networking.
Apache Spark’s Machine Learning Library (MLlib) is designed for simplicity, scalability, and easy integration with other tools. 
With the scalability, language compatibility, and speed of Spark, data scientists can focus on their data problems and models instead of solving the complexities surrounding distributed data (such as infrastructure, configurations, and so on). 
Spark MLlib seamlessly integrates with other Spark components such as Spark SQL, Spark Streaming, and DataFrames API. 
Machine learning can be applied to a wide variety of data types, such as vectors, text, images, and structured data. 
This API adopts the DataFrame from Spark SQL to support a variety of data types.
The library is usable in Java, Scala, and Python as part of Spark applications, so that you can include it in complete workflows. 
MLlib allows for preprocessing, munging咀嚼, training of models, and making predictions at scale on data.
You can even use models trained in MLlib to make predictions in Structured Streaming. 
In the scope of descriptive analysis, it supports statistical descriptions analysis and clustering.
In the scope of predictive analysis, it supports Feature modeling, which includes feature extractor like TF-IDF, Feature <span class="lx-highlight" data-idx="3" style="background-color:#D2E3FC;">transformer</span> like Vector Slicer etc. and feature selector like Chi-square selector. It also supports prediction algorithms like binary classification, multiclass classification and regression.
MLlib is Apache Spark&#x27;s scalable machine learning library.
Ease of use
Usable in Java, Scala, Python, and R.
MLlib fits into Spark&#x27;s APIs and interoperates with NumPy in Python (as of Spark 0.9) and R libraries (as of Spark 1.5). 
You can use any Hadoop data source (e.g. HDFS, HBase, or local files), making it easy to plug into Hadoop workflows.
Performance
High-quality algorithms, 100x faster than MapReduce.
Spark excels at iterative computation, enabling MLlib to run fast. 
At the same time, we care about algorithmic performance: MLlib contains high-quality algorithms that leverage iteration, and can yield better results than the one yield on MapReduce.
Runs everywhere
Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud, against diverse data sources.
You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, on Mesos, or on Kubernetes. Access data in HDFS, Apache Cassandra, Apache HBase, Apache Hive, and hundreds of other data sources.
To support Python with Spark, the Apache Spark community released a tool, <span class="lx-highlight" data-idx="4" style="background-color:#D2E3FC;">PySpark</span>. Using PySpark, one can work with RDDs in Python programming language.
Built on top of Spark, MLlib is a scalable machine learning library consisting of 4 main components, Algorithms, Featurization, Pipeline and Utilities.
common learning algorithms including classification, regression, clustering, collaborative filtering, dimensionality reduction, and underlying optimization primitives. 
and utilities, which include Linear algebra, statistics etc. 
And the Featurization include extracting the features and transforming the features.
<span class="lx-highlight" data-idx="5" style="background-color:#D2E3FC;">ML Pipelines</span> provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines.
the mechanism to finish the ML tasks is to construct the pipeline to finish all the needed steps. Like constructing model, train the model, evaluating the model, tuning the parameters and persistence the model.
ML algorithms include:
Classification: logistic regression, naive Bayes, ...
Regression: generalized linear regression, survival regression, ...
Decision trees, random forests, and gradient-boosted trees
Recommendation: alternating least squares (ALS)
Clustering: K-means, Gaussian mixtures (GMMs), ...
Topic modeling: latent Dirichlet allocation (LDA)
Frequent item sets, association rules, and sequential pattern mining
ML workflow utilities include:
Feature transformations: standardization, normalization, hashing, ...
ML Pipeline construction
Model evaluation and hyper-parameter tuning
<span class="lx-highlight" data-idx="6" style="background-color:#D2E3FC;">ML persistence</span>: saving and loading models and Pipelines
Other utilities include:
Distributed linear algebra: SVD, PCA, ...
Statistics: summary statistics, hypothesis testing, ...
On the left, it is the normal machine learning pipeline, 1Load/clean data, 2 feature extraction, 3 model training and 4model evaluation and parameter tuning, then repeat the workflow process. 
And on the right, it is the Mllib Pipeline Concepts, from 1 load/clean data, 2 transformers, which is corresponding to feature engineering, 3 <span class="lx-highlight" data-idx="7" style="background-color:#D2E3FC;">Estimator</span>, which is corresponding to model training, and 4 <span class="lx-highlight" data-idx="8" style="background-color:#D2E3FC;">Evaluator</span>, which is responsible for model evaluation. Let me explain them one by one
A Transformer is an abstraction that includes feature transformers and learned models. 
which Transforming data into consumable format, 
Take input column, transform it to an output column.
Technically, a Transformer implements a method transform (), which converts one DataFrame into another, generally by appending one or more columns. 
For example:
A feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.
A learning model might take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and output a new DataFrame with predicted labels appended as a column.
Examples such as : 1 Normalize the data, 2 Tokenization (which means dividing the sentences into words) and  3 Converting categorical values into numbers.
Like showed in the diagram, transform the data frame 1 into data frame 2.
An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. 
Technically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer. 
For example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.
Evaluator is designed to Evaluate the model performance based certain metrics, like ROC<span class="lx-highlight" data-idx="9" style="background-color:#C8E6C9;">, RMSE.
And Evaluator can Help with automating the model tuning process through Comparing model performance
And Selecting the best model for generating predictions.
The Example here is BinaryClassificationEvaluator with <span class="lx-highlight" data-idx="10" style="background-color:#D2E3FC;">CrossValidator</span>.
Input is the data and output is the best model selected from all the options.
In machine learning, it is common to run a sequence of algorithms to process and learn from data. 
E.g., a simple text document processing workflow might include several stages:
Split each document’s text into words.
Convert each document’s words into a numerical feature vector.
Learn a prediction model using the feature vectors and labels.
MLlib represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. 
The pipeline Leverages the uniform API of Transformer &amp; Estimator. It Can be persisted.
These stages are run in order, and the input DataFrame is transformed as it passes through each stage. 
For Transformer stages, the transform () method is called on the DataFrame. 
For Estimator stages, the fit () method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline), and that Transformer’s transform () method is called on the DataFrame.
We illustrate this for the simple text document workflow. 
The figure in the middle, the top row represents a Pipeline with three stages. 
The first two (Tokenizer and HashingTF) are Transformers (blue), and the third (LogisticRegression) is an Estimator (red).
The bottom row represents data flowing through the pipeline, where cylinders indicate DataFrames. 
The Pipeline.fit() method is called on the original DataFrame, which has raw text documents and labels. 
The Tokenizer.transform() method splits the raw text documents into words, adding a new column with words to the DataFrame. 
The HashingTF.transform() method converts the words column into feature vectors, adding a new column with those vectors to the DataFrame. 
Now, since LogisticRegression is an Estimator, the Pipeline first calls LogisticRegression.fit() to produce a LogisticRegressionModel. 
If the Pipeline had more Estimators, it would call the LogisticRegressionModel’s transform() method on the DataFrame before passing the DataFrame to the next stage.
A Pipeline is an Estimator. 
Thus, after a Pipeline’s fit() method runs, it produces a PipelineModel, which is a Transformer (which means the model is transformer). 
This PipelineModel is used at test time; the figure at the bottom illustrates this usage. 
the PipelineModel has the same number of stages as the original Pipeline, but all Estimators in the original Pipeline have become Transformers. 
When the PipelineModel’s transform () method is called on a test dataset, the data are passed through the fitted pipeline in order. 
Each stage’s transform () method updates the dataset and passes it to the next stage.
Pipelines and PipelineModels help to ensure that training and test data go through identical feature processing steps.
MLlib Estimators and Transformers use a uniform API for specifying parameters.
A <span class="lx-highlight" data-idx="11" style="background-color:#D2E3FC;">Param</span> is a named parameter with self-contained documentation. 
A <span class="lx-highlight" data-idx="12" style="background-color:#D2E3FC;">ParamMap</span> is a set of (parameter, value) pairs.
There are two main ways to pass parameters to an algorithm:
Set parameters for an instance. E.g., if lr is an instance of LogisticRegression, one could call lr.setMaxIter(10) (maximum iteration)to make lr.fit() use at most 10 iterations. This API resembles the API used in spark.mllib package.
Pass a ParamMap to fit() or transform(). Any parameters in the ParamMap will override parameters previously specified via setter methods.
Parameters belong to specific instances of Estimators and Transformers. For example, if we have two LogisticRegression instances lr1 and lr2, then we can build a ParamMap with both maxIter parameters specified: ParamMap(lr1.maxIter -&gt; 10, lr2.maxIter -&gt; 20). This is useful if there are two algorithms with the maxIter parameter in a Pipeline.
After you build a pipeline in MLlib, it can Automate the model tuning process. 
A very important task in ML is model selection, or to say, the use of data to find the best model or parameters for a given task. 
This is called parameter tuning. Tuning can be performed on a single Estimators (such as LogisticRegression) or 
on the entire Pipeline (which can include multiple algorithms, characterization and other steps).
We need to Build a param grid for grid search-based model selection,
To build parameter grid, we can use <span class="lx-highlight" data-idx="13" style="background-color:#D2E3FC;">ParamGridBuilder</span> tool Class.
ParamGridBuilder() allows to specify different values for a single parameters, 
and then compare the entire set of parameters to choose the best options, which define the best model. 
CrossValidator divides the data set into several folds, which can be used for independent training and test sets. 
For example: when k=5 folds, CrossValidator will generate 5 (training, test) pairs, each of which uses 4/5 data as the training set and 1/5 as the test set. 
To evaluate a ParamMap, use Estimator to fit 5 models on 5 different data pairs, and CrossValidator will calculate the average of 5 evaluation metrics.
After selecting the best ParamMap, CrossValidator will finally use the corresponding Estimator and the best ParamMap to refit the entire data set.
ML persistence means Saving and Loading Pipelines
Often it is worth it to save a model or a pipeline to disk for later use. 
The data scientist creates a model or a pipeline and the data engineer can deploy model at scale and monitor its application.
In Spark 1.6, a model import/export functionality was added to the Pipeline API. 
As of Spark 2.3, the DataFrame-based API in spark.ml and pyspark.ml has complete coverage.
ML persistence works across Scala, Java and Python. 
However, R currently uses a modified format, so models saved in R can only be loaded back in R; this should be fixed in the future and is tracked in SPARK-15572.
Backwards compatibility for ML persistence
In general, MLlib maintains backwards compatibility for ML persistence. 
I.e., if you save an ML model or Pipeline in one version of Spark, then you should be able to load it back and use it in a future version of Spark. 
Consider that Spark’s ML Lib is suitable when you’re doing relatively simple ML on a large data set. 
ML Lib is not computationally efficient for small data sets, and you’re better off using scikit-learn</span> for small and medium sized data sets (megabytes, up to a few gigabytes). 
We have the hands on carried out on sklearn, which include the algorithm and high-level tools.
<span class="lx-highlight" data-idx="14" style="background-color:#D2E3FC;">Scikit-learn (Sklearn)</span> is the most useful and robust library for machine learning in Python. 
It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python.
In this session, we learned concepts and mechanism of spark MLlib.
thank you for your attention, if you have any question, feel free to contact me.
      </div>
      <div class="lx-controls">
        <div class="lx-button-row">
          <button class="lx-control-btn" onclick="playPause()">▶️ Play</button>
          <button class="lx-control-btn" onclick="prevExtraction()">⏮ Previous</button>
          <button class="lx-control-btn" onclick="nextExtraction()">⏭ Next</button>
        </div>
        <div class="lx-progress-container">
          <input type="range" id="progressSlider" class="lx-progress-slider"
                 min="0" max="14" value="0"
                 onchange="jumpToExtraction(this.value)">
        </div>
        <div class="lx-status-text">
          Entity <span id="entityInfo">1/15</span> |
          Pos <span id="posInfo">[320-332]</span>
        </div>
      </div>
    </div>

    <script>
      (function() {
        const extractions = [{"index": 0, "class": "TOPIC", "text": "Concepts and Mechanisms of Spark MLlib", "color": "#F9DEDC", "startPos": 320, "endPos": 332, "beforeText": "on on, we will discuss something about real application,\nIncluding some platform and big data application and in this session, we will talk about the ", "extractionText": "concepts and", "afterText": " mechanism of Spark MLlib.\nFrom this chapter ,we will introduce some useful platform\uff0c Spark  MLlib and  TensorFlow.\nWe will show some case experiments", "attributesHtml": "<div><strong>class:</strong> TOPIC</div><div><strong>attributes:</strong> {}</div>"}, {"index": 1, "class": "CONCEPT", "text": "Spark MLlib", "color": "#D2E3FC", "startPos": 346, "endPos": 357, "beforeText": "ething about real application,\nIncluding some platform and big data application and in this session, we will talk about the concepts and mechanism of ", "extractionText": "Spark MLlib", "afterText": ".\nFrom this chapter ,we will introduce some useful platform\uff0c Spark  MLlib and  TensorFlow.\nWe will show some case experiments applying the correspondi", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">Apache Spark&#x27;s scalable machine learning library designed for simplicity, scalability, and easy integration with other tools. It is built on top of Spark and consists of components like Algorithms, Featurization, Pipeline, and Utilities.</span>}</div>"}, {"index": 2, "class": "SUMMARY", "text": "This session introduces Apache Spark's Machine Learning Library (MLlib), designed for simplicity, scalability, and integration with other Spark components like Spark SQL and Streaming. MLlib supports Java, Scala, and Python (via PySpark) and is optimized for large-scale data processing, running up to 100x faster than MapReduce. The library's core components include algorithms, featurization tools, utilities, and ML Pipelines. The ML Pipeline provides a high-level API built on DataFrames to create and tune machine learning workflows. A Pipeline consists of a sequence of stages, primarily Transformers and Estimators. A Transformer converts one DataFrame into another by appending columns, handling tasks like feature extraction and normalization. An Estimator is a learning algorithm that trains on a DataFrame to produce a Model, which is itself a Transformer. An Evaluator assesses model performance using metrics like ROC or RMSE. This framework ensures that training and test data undergo identical processing steps. MLlib also provides robust tools for parameter tuning, such as ParamGridBuilder for creating parameter grids and CrossValidator for automated model selection. Finally, the session covers ML persistence, which allows models and pipelines to be saved and loaded across different platforms, and contrasts MLlib's suitability for large datasets with Scikit-learn's efficiency for small to medium-sized data.", "color": "#FEF0C3", "startPos": 364, "endPos": 368, "beforeText": "application,\nIncluding some platform and big data application and in this session, we will talk about the concepts and mechanism of Spark MLlib.\nFrom ", "extractionText": "this", "afterText": " chapter ,we will introduce some useful platform\uff0c Spark  MLlib and  TensorFlow.\nWe will show some case experiments applying the corresponding platform", "attributesHtml": "<div><strong>class:</strong> SUMMARY</div><div><strong>attributes:</strong> {}</div>"}, {"index": 3, "class": "CONCEPT", "text": "Transformer", "color": "#D2E3FC", "startPos": 1874, "endPos": 1885, "beforeText": "ons analysis and clustering.\nIn the scope of predictive analysis, it supports Feature modeling, which includes feature extractor like TF-IDF, Feature ", "extractionText": "transformer", "afterText": " like Vector Slicer etc. and feature selector like Chi-square selector. It also supports prediction algorithms like binary classification, multiclass ", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An abstraction that includes feature transformers and learned models. It implements a method `transform()`, which converts one DataFrame into another, generally by appending one or more columns.</span>}</div>"}, {"index": 4, "class": "CONCEPT", "text": "PySpark", "color": "#D2E3FC", "startPos": 3134, "endPos": 3141, "beforeText": "e Cassandra,\u00a0Apache HBase,\u00a0Apache Hive, and hundreds of other data sources.\nTo support Python with Spark, the Apache Spark community released a tool, ", "extractionText": "PySpark", "afterText": ". Using PySpark, one can work with RDDs in Python programming language.\nBuilt on top of Spark, MLlib is a scalable machine learning library consisting", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A tool released by the Apache Spark community to support Python with Spark, allowing users to work with RDDs in the Python programming language.</span>}</div>"}, {"index": 5, "class": "CONCEPT", "text": "ML Pipeline", "color": "#D2E3FC", "startPos": 3681, "endPos": 3693, "beforeText": ". \nand utilities, which include Linear algebra, statistics etc. \nAnd the Featurization include extracting the features and transforming the features.\n", "extractionText": "ML Pipelines", "afterText": " provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines.\nthe mechani", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A representation of a machine learning workflow as a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. It provides a uniform set of high-level APIs built on top of DataFrames.</span>}</div>"}, {"index": 6, "class": "CONCEPT", "text": "ML Persistence", "color": "#D2E3FC", "startPos": 4635, "endPos": 4649, "beforeText": "s include:\nFeature transformations: standardization, normalization, hashing, ...\nML Pipeline construction\nModel evaluation and hyper-parameter tuning\n", "extractionText": "ML persistence", "afterText": ": saving and loading models and Pipelines\nOther utilities include:\nDistributed linear algebra: SVD, PCA, ...\nStatistics: summary statistics, hypothesi", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">The functionality to save a model or a pipeline to disk for later use, allowing a data scientist to create a model and a data engineer to deploy it at scale.</span>}</div>"}, {"index": 7, "class": "CONCEPT", "text": "Estimator", "color": "#D2E3FC", "startPos": 5149, "endPos": 5158, "beforeText": "ocess. \nAnd on the right, it is the Mllib Pipeline Concepts, from 1 load/clean data, 2 transformers, which is corresponding to feature engineering, 3 ", "extractionText": "Estimator", "afterText": ", which is corresponding to model training, and 4 Evaluator, which is responsible for model evaluation. Let me explain them one by one\nA\u00a0Transformer\u00a0i", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An abstraction of a learning algorithm or any algorithm that fits or trains on data. It implements a method `fit()`, which accepts a DataFrame and produces a Model, which is a Transformer.</span>}</div>"}, {"index": 8, "class": "CONCEPT", "text": "Evaluator", "color": "#D2E3FC", "startPos": 5208, "endPos": 5217, "beforeText": ", from 1 load/clean data, 2 transformers, which is corresponding to feature engineering, 3 Estimator, which is corresponding to model training, and 4 ", "extractionText": "Evaluator", "afterText": ", which is responsible for model evaluation. Let me explain them one by one\nA\u00a0Transformer\u00a0is an abstraction that includes feature transformers and lea", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A component designed to evaluate model performance based on certain metrics, like ROC or RMSE, and to help automate the model tuning process by comparing model performance and selecting the best model.</span>}</div>"}, {"index": 9, "class": "KEYWORDS", "text": "Spark MLlib, Machine Learning Pipeline, Transformer, Estimator, Evaluator, Parameter Tuning, CrossValidator, PySpark, ML Persistence, Scikit-learn", "color": "#C8E6C9", "startPos": 6748, "endPos": 13199, "beforeText": "sticRegressionModel, which is a\u00a0Model\u00a0and hence a\u00a0Transformer.\nEvaluator is designed to Evaluate the model performance based certain metrics, like ROC", "extractionText": ", RMSE.\nAnd Evaluator can Help with automating the model tuning process through Comparing model performance\nAnd Selecting the best model for generating predictions.\nThe Example here is BinaryClassificationEvaluator with CrossValidator.\nInput is the data and output is the best model selected from all the options.\nIn machine learning, it is common to run a sequence of algorithms to process and learn from data. \nE.g., a simple text document processing workflow might include several stages:\nSplit each document\u2019s text into words.\nConvert each document\u2019s words into a numerical feature vector.\nLearn a prediction model using the feature vectors and labels.\nMLlib represents such a workflow as a\u00a0Pipeline, which consists of a sequence of\u00a0PipelineStages (Transformers and\u00a0Estimators) to be run in a specific order. \nThe pipeline Leverages the uniform API of Transformer &amp; Estimator. It Can be persisted.\nThese stages are run in order, and the input\u00a0DataFrame\u00a0is transformed as it passes through each stage. \nFor\u00a0Transformer\u00a0stages, the\u00a0transform ()\u00a0method is called on the\u00a0DataFrame. \nFor\u00a0Estimator\u00a0stages, the\u00a0fit ()\u00a0method is called to produce a\u00a0Transformer\u00a0(which becomes part of the\u00a0PipelineModel, or fitted\u00a0Pipeline), and that\u00a0Transformer\u2019s\u00a0transform ()\u00a0method is called on the\u00a0DataFrame.\nWe illustrate this for the simple text document workflow. \nThe figure in the middle, the top row represents a\u00a0Pipeline\u00a0with three stages. \nThe first two (Tokenizer\u00a0and\u00a0HashingTF) are\u00a0Transformers (blue), and the third (LogisticRegression) is an\u00a0Estimator\u00a0(red).\nThe bottom row represents data flowing through the pipeline, where cylinders indicate\u00a0DataFrames. \nThe\u00a0Pipeline.fit()\u00a0method is called on the original\u00a0DataFrame, which has raw text documents and labels. \nThe\u00a0Tokenizer.transform()\u00a0method splits the raw text documents into words, adding a new column with words to the\u00a0DataFrame. \nThe\u00a0HashingTF.transform()\u00a0method converts the words column into feature vectors, adding a new column with those vectors to the\u00a0DataFrame. \nNow, since\u00a0LogisticRegression\u00a0is an\u00a0Estimator, the\u00a0Pipeline\u00a0first calls\u00a0LogisticRegression.fit()\u00a0to produce a\u00a0LogisticRegressionModel. \nIf the\u00a0Pipeline\u00a0had more\u00a0Estimators, it would call the\u00a0LogisticRegressionModel\u2019s\u00a0transform()\u00a0method on the\u00a0DataFrame\u00a0before passing the\u00a0DataFrame\u00a0to the next stage.\nA\u00a0Pipeline\u00a0is an\u00a0Estimator. \nThus, after a\u00a0Pipeline\u2019s\u00a0fit()\u00a0method runs, it produces a\u00a0PipelineModel, which is a\u00a0Transformer (which means the model is transformer). \nThis\u00a0PipelineModel\u00a0is used at\u00a0test time; the figure at the bottom illustrates this usage. \nthe\u00a0PipelineModel\u00a0has the same number of stages as the original\u00a0Pipeline, but all\u00a0Estimators in the original\u00a0Pipeline\u00a0have become\u00a0Transformers. \nWhen the\u00a0PipelineModel\u2019s\u00a0transform ()\u00a0method is called on a test dataset, the data are passed through the fitted pipeline in order. \nEach stage\u2019s\u00a0transform ()\u00a0method updates the dataset and passes it to the next stage.\nPipelines and\u00a0PipelineModels help to ensure that training and test data go through identical feature processing steps.\nMLlib Estimators and Transformers use a uniform API for specifying parameters.\nA Param is a named parameter with self-contained documentation. \nA ParamMap is a set of (parameter, value) pairs.\nThere are two main ways to pass parameters to an algorithm:\nSet parameters for an instance. E.g., if lr is an instance of LogisticRegression, one could call lr.setMaxIter(10) (maximum iteration)to make lr.fit() use at most 10 iterations. This API resembles the API used in spark.mllib package.\nPass a ParamMap to fit() or transform(). Any parameters in the ParamMap will override parameters previously specified via setter methods.\nParameters belong to specific instances of Estimators and Transformers. For example, if we have two LogisticRegression instances lr1 and lr2, then we can build a ParamMap with both maxIter parameters specified: ParamMap(lr1.maxIter -&gt; 10, lr2.maxIter -&gt; 20). This is useful if there are two algorithms with the maxIter parameter in a Pipeline.\nAfter you build a pipeline in MLlib, it can Automate the model tuning process. \nA very important task in ML is model selection, or to say, the use of data to find the best model or parameters for a given task. \nThis is called parameter tuning. Tuning can be performed on a single Estimators (such as LogisticRegression) or \non the entire Pipeline (which can include multiple algorithms, characterization and other steps).\nWe need to Build a param grid for grid search-based model selection,\nTo build parameter grid, we can use ParamGridBuilder tool Class.\nParamGridBuilder()\u00a0allows to specify different values for a single parameters, \nand then compare the entire set of parameters to choose the best options, which define the best model.\u00a0\nCrossValidator divides the data set into several folds, which can be used for independent training and test sets. \nFor example: when k=5 folds, CrossValidator will generate 5 (training, test) pairs, each of which uses 4/5 data as the training set and 1/5 as the test set. \nTo evaluate a ParamMap, use Estimator to fit 5 models on 5 different data pairs, and CrossValidator will calculate the average of 5 evaluation metrics.\nAfter selecting the best ParamMap, CrossValidator will finally use the corresponding Estimator and the best ParamMap to refit the entire data set.\nML persistence means Saving and Loading Pipelines\nOften it is worth it to save a model or a pipeline to disk for later use. \nThe data scientist creates a model or a pipeline and the data engineer can deploy model at scale and monitor its application.\nIn Spark 1.6, a model import/export functionality was added to the Pipeline API. \nAs of Spark 2.3, the DataFrame-based API in\u00a0spark.ml\u00a0and\u00a0pyspark.ml\u00a0has complete coverage.\nML persistence works across Scala, Java and Python. \nHowever, R currently uses a modified format, so models saved in R can only be loaded back in R; this should be fixed in the future and is tracked in\u00a0SPARK-15572.\nBackwards compatibility for ML persistence\nIn general, MLlib maintains backwards compatibility for ML persistence. \nI.e., if you save an ML model or Pipeline in one version of Spark, then you should be able to load it back and use it in a future version of Spark. \nConsider that Spark\u2019s ML Lib is suitable when you\u2019re doing relatively simple ML on a large data set. \nML Lib is not computationally efficient for small data sets, and you\u2019re better off using scikit-learn", "afterText": " for small and medium sized data sets (megabytes, up to a few gigabytes). \nWe have the hands on carried out on sklearn, which include the algorithm an", "attributesHtml": "<div><strong>class:</strong> KEYWORDS</div><div><strong>attributes:</strong> {}</div>"}, {"index": 10, "class": "CONCEPT", "text": "CrossValidator", "color": "#D2E3FC", "startPos": 6968, "endPos": 6982, "beforeText": "s through Comparing model performance\nAnd Selecting the best model for generating predictions.\nThe Example here is BinaryClassificationEvaluator with ", "extractionText": "CrossValidator", "afterText": ".\nInput is the data and output is the best model selected from all the options.\nIn machine learning, it is common to run a sequence of algorithms to p", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A tool that divides a dataset into several folds to create independent training and test sets. It evaluates a ParamMap by fitting models on these different data pairs, calculates the average evaluation metric, and selects the best model.</span>}</div>"}, {"index": 11, "class": "CONCEPT", "text": "Param", "color": "#D2E3FC", "startPos": 9892, "endPos": 9897, "beforeText": "raining and test data go through identical feature processing steps.\nMLlib Estimators and Transformers use a uniform API for specifying parameters.\nA ", "extractionText": "Param", "afterText": " is a named parameter with self-contained documentation. \nA ParamMap is a set of (parameter, value) pairs.\nThere are two main ways to pass parameters ", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A named parameter with self-contained documentation used by MLlib Estimators and Transformers.</span>}</div>"}, {"index": 12, "class": "CONCEPT", "text": "ParamMap", "color": "#D2E3FC", "startPos": 9957, "endPos": 9965, "beforeText": "ps.\nMLlib Estimators and Transformers use a uniform API for specifying parameters.\nA Param is a named parameter with self-contained documentation. \nA ", "extractionText": "ParamMap", "afterText": " is a set of (parameter, value) pairs.\nThere are two main ways to pass parameters to an algorithm:\nSet parameters for an instance. E.g., if lr is an i", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A set of (parameter, value) pairs used to pass parameters to an algorithm.</span>}</div>"}, {"index": 13, "class": "CONCEPT", "text": "ParamGridBuilder", "color": "#D2E3FC", "startPos": 11307, "endPos": 11323, "beforeText": "gorithms, characterization and other steps).\nWe need to Build a param grid for grid search-based model selection,\nTo build parameter grid, we can use ", "extractionText": "ParamGridBuilder", "afterText": " tool Class.\nParamGridBuilder()\u00a0allows to specify different values for a single parameters, \nand then compare the entire set of parameters to choose t", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A tool class that allows the specification of different values for parameters to build a parameter grid for grid search-based model selection.</span>}</div>"}, {"index": 14, "class": "CONCEPT", "text": "Scikit-learn (Sklearn)", "color": "#D2E3FC", "startPos": 13369, "endPos": 13391, "beforeText": "m sized data sets (megabytes, up to a few gigabytes). \nWe have the hands on carried out on sklearn, which include the algorithm and high-level tools.\n", "extractionText": "Scikit-learn (Sklearn)", "afterText": " is the most\u00a0useful and robust library for machine learning in Python. \nIt provides a selection of efficient tools for machine learning and statistica", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A useful and robust library for machine learning in Python that provides efficient tools for tasks like classification, regression, clustering, and dimensionality reduction, and is better suited for small and medium-sized datasets.</span>}</div>"}];
        let currentIndex = 0;
        let isPlaying = false;
        let animationInterval = null;
        let animationSpeed = 1.0;

        function updateDisplay() {
          const extraction = extractions[currentIndex];
          if (!extraction) return;

          document.getElementById('attributesContainer').innerHTML = extraction.attributesHtml;
          document.getElementById('entityInfo').textContent = (currentIndex + 1) + '/' + extractions.length;
          document.getElementById('posInfo').textContent = '[' + extraction.startPos + '-' + extraction.endPos + ']';
          document.getElementById('progressSlider').value = currentIndex;

          const playBtn = document.querySelector('.lx-control-btn');
          if (playBtn) playBtn.textContent = isPlaying ? '⏸ Pause' : '▶️ Play';

          const prevHighlight = document.querySelector('.lx-text-window .lx-current-highlight');
          if (prevHighlight) prevHighlight.classList.remove('lx-current-highlight');
          const currentSpan = document.querySelector('.lx-text-window span[data-idx="' + currentIndex + '"]');
          if (currentSpan) {
            currentSpan.classList.add('lx-current-highlight');
            currentSpan.scrollIntoView({block: 'center', behavior: 'smooth'});
          }
        }

        function nextExtraction() {
          currentIndex = (currentIndex + 1) % extractions.length;
          updateDisplay();
        }

        function prevExtraction() {
          currentIndex = (currentIndex - 1 + extractions.length) % extractions.length;
          updateDisplay();
        }

        function jumpToExtraction(index) {
          currentIndex = parseInt(index);
          updateDisplay();
        }

        function playPause() {
          if (isPlaying) {
            clearInterval(animationInterval);
            isPlaying = false;
          } else {
            animationInterval = setInterval(nextExtraction, animationSpeed * 1000);
            isPlaying = true;
          }
          updateDisplay();
        }

        window.playPause = playPause;
        window.nextExtraction = nextExtraction;
        window.prevExtraction = prevExtraction;
        window.jumpToExtraction = jumpToExtraction;

        updateDisplay();
      })();
    </script>