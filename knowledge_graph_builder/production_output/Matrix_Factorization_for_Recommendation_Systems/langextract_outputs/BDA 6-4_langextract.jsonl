{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "Matrix Factorization for Recommendation Systems", "char_interval": {"start_pos": 1114, "end_pos": 3158}, "alignment_status": "match_fuzzy", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "This session introduces Matrix Factorization as a typical Latent Factor model for recommendation systems, contrasting it with previously learned methods like user-based and item-based filtering. The core idea is to find hidden patterns or characters in a user-item rating matrix by decomposing it into two separate matrices: a user-character rating matrix and an item-character rating matrix. While Singular Value Decomposition (SVD) is a powerful matrix factorization technique, it is unsuitable for recommendation systems because it requires a dense matrix, whereas user-item rating matrices are typically sparse with many missing values. Therefore, an alternative Matrix Decomposition approach is used, which decomposes the rating matrix R into a user-latent factor matrix (P) and an item-latent factor matrix (Q). The goal is to find P and Q such that their product can predict the missing ratings. This is achieved by minimizing a cost function, which includes the Mean Squared Error (MSE) of predicted ratings and a regularization term to prevent overfitting. Two methods for minimizing this cost function are discussed: Alternating Least Square (ALS), which iteratively optimizes P and Q, and Gradient Descent, which updates P and Q based on partial derivatives. The session concludes by outlining a series of hands-on experiments, one for user-based filtering and another for Matrix Decomposition using the Surprise library and a movie ratings dataset.", "char_interval": {"start_pos": 302, "end_pos": 314}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "Matrix Factorization, Latent Factor model, Recommendation System, Singular Value Decomposition (SVD), Cost Function, Alternating Least Square (ALS), Gradient Descent, Collaborative Filtering, Surprise library", "char_interval": {"start_pos": 292, "end_pos": 3757}, "alignment_status": "match_fuzzy", "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Matrix Factorization", "char_interval": {"start_pos": 4363, "end_pos": 4383}, "alignment_status": "match_exact", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "A typical Latent Factor model used in recommendation systems to find hidden patterns in a rating matrix by decomposing it into an item-character rating matrix and a user-character rating matrix."}}, {"extraction_class": "CONCEPT", "extraction_text": "Singular Value Decomposition (SVD)", "char_interval": {"start_pos": 807, "end_pos": 841}, "alignment_status": "match_fuzzy", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "A factorization of a real or complex matrix that generalizes the eigen decomposition. A key limitation is that it requires a dense matrix with no missing values, making it unsuitable for sparse user-item rating matrices."}}, {"extraction_class": "CONCEPT", "extraction_text": "Matrix Decomposition", "char_interval": {"start_pos": 1629, "end_pos": 1649}, "alignment_status": "match_fuzzy", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "A method to handle sparse matrices in recommendation systems by decomposing the user-item rating matrix (R) into two matrices: a user-latent factor matrix (P) and an item-latent factor matrix (Q). The resulting dense matrices can be used to predict missing values in the original matrix."}}, {"extraction_class": "CONCEPT", "extraction_text": "Cost Function", "char_interval": {"start_pos": 1968, "end_pos": 1981}, "alignment_status": "match_fuzzy", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "A function used to evaluate and find the optimal P and Q matrices in Matrix Decomposition. It consists of the Mean Squared Error (MSE) between predicted and true ratings, plus a regularization term to prevent overfitting. The goal is to minimize this function."}}, {"extraction_class": "CONCEPT", "extraction_text": "Alternating Least Square (ALS)", "char_interval": null, "alignment_status": null, "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "An iterative method to minimize the cost function in Matrix Decomposition. It works by first fixing matrix P to compute Q, then fixing Q to compute P, and repeating this process until a stopping condition is met."}}, {"extraction_class": "CONCEPT", "extraction_text": "Gradient Descent", "char_interval": {"start_pos": 2767, "end_pos": 2783}, "alignment_status": "match_fuzzy", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "An iterative optimization method to minimize the cost function by computing the partial derivatives with respect to matrices P and Q and updating them in each iteration until a stopping condition is met."}}], "text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology\nin last session we learned 3 main Recommendation System Algorithms,\nUser-based filtering, Item-based filtering and Content-Based Filtering, and \nin this session we will learn a typical Latent Factor model, Matrix Factorization.\nLet’s watch a video “How Recommender Systems Work (NetflixAmazon) \nFrom the video we know there is hidden patterns in the Rating Matrix, \nWe want to Find some character the items may have.\nAnd we Decompose the rating matrix into item-character rating & user-character rating.\nMoreover, we don’t want to know the meaning of characters. Which is Abstract model, we just suppose the number of characters.\nIn linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. \nIt generalizes the eigen decomposition of a square normal matrix with an orthonormal eigen basis to any m*n matrix.\nIt is related to the polar decomposition.\nSpecifically, the singular value decomposition of an m*n complex matrix M is a factorization of the form U, \nwhere UΣV*, where U is an m*m complex unitary matrix, \nΣ is an m*n rectangular diagonal matrix with non-negative real numbers on the diagonal, \nand V is an n*n complex unitary matrix. \nIf M is real, U and V can also be guaranteed to be real orthogonal matrices. \nIn such contexts, the SVD is often denoted  UΣ(V transpose)\nSVD requires dense matrix, that is the matrix doesn’t have missing values. \nEvidently, user-item rating matrix has lots of missing values. \nSo, use Matrix Decomposition to replace SVD.\nDecompose the matrix into two matrix, that is , where  is  user-item rating matrix,  is  user-LF(Latent factor ) matrix,  and is  item-LF(Latent factor ) matrix.\nFor u-user and i-item, their rating is:\nIf get two dense matrix , from  we can predict the missing value in .\nSo how to calculate  ?\nDefine cost function as showed in the formula, and use the cost function to evaluate the better choice of P and Q. We only calculate cost function with the already given rating values by the users.\nThe first part in the cost function is the MSE of the predict rating value Rui hat and true value Rui. \nAnd the second part in the cost function is regular value, which prevent overfitting.\nWe do the iteration to minimize the cost function.\nTwo ways to minimize cost function:\n) using ALS (Alternating Least Square) to minimize cost function :which means fix P, compute Q to make c min; then, fix Q, compute P to make c min; \nEnd until reach max iteration or c satisfies threshold condition.\nCompute partial derivative of C with respect to Pu, and make the formula equals to 0, get .\nSimilarly get .\n) using Gradient Descent to minimize cost function, which means Compute partial derivative of C with respect to Pu\nand partial derivative of C with respect to Qi.\nthen Do Iteration using the formula in the slide (where   is step size)\nAfter each iteration, the Pu and Qi are updated until the end, until reach max iteration or c satisfies threshold condition.\nTo understand the recommendation system, A series of experiments were designed, \nit includes User-based filtering recommendation and Matrix Decomposition.\nIn User-based filtering recommendation, it consist preprocessing and Collaborative Filtering.\nIn 1.1 preprocessing, it includes load the data and relate the two original tables, and create a new data.csv file, and make a dictionary by deleting the duplicate records. \nIn 1.2 Collaborative Filtering, first compute the user similarity, and then list top 10 similar users to the current user, and make the recommendation.\nFor matrix decomposition experiment, we need import the library surprise, the used data set includes 100,000 user’s ratings on movies.\nThe related models include Funk or Bias SVD, Grid Search for training.\nThe goal is to Train and test on the best model and Get the best parameters for SVD,\nThe process will be \n）Import library \n）import data \n）Grid search SVD training \n）Use the best parameters obtained by grid search for raining and prediction \nAnd finally visualize the Result.\nAll the experiments material including the manual and codes are provided on the platform, which can help you to do the hands-on.\nIn this session, we learned a typical Latent Factor model, Matrix Factorization.\nthank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_80fcad25"}
