<style>
.lx-highlight { position: relative; border-radius:3px; padding:1px 2px;}
.lx-highlight .lx-tooltip {
  visibility: hidden;
  opacity: 0;
  transition: opacity 0.2s ease-in-out;
  background: #333;
  color: #fff;
  text-align: left;
  border-radius: 4px;
  padding: 6px 8px;
  position: absolute;
  z-index: 1000;
  bottom: 125%;
  left: 50%;
  transform: translateX(-50%);
  font-size: 12px;
  max-width: 240px;
  white-space: normal;
  box-shadow: 0 2px 6px rgba(0,0,0,0.3);
}
.lx-highlight:hover .lx-tooltip { visibility: visible; opacity:1; }
.lx-animated-wrapper { max-width: 100%; font-family: Arial, sans-serif; }
.lx-controls {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 8px;
  padding: 12px; margin-bottom: 16px;
}
.lx-button-row {
  display: flex; justify-content: center; gap: 8px; margin-bottom: 12px;
}
.lx-control-btn {
  background: #4285f4; color: white; border: none; border-radius: 4px;
  padding: 8px 16px; cursor: pointer; font-size: 13px; font-weight: 500;
  transition: background-color 0.2s;
}
.lx-control-btn:hover { background: #3367d6; }
.lx-progress-container {
  margin-bottom: 8px;
}
.lx-progress-slider {
  width: 100%; margin: 0; appearance: none; height: 6px;
  background: #ddd; border-radius: 3px; outline: none;
}
.lx-progress-slider::-webkit-slider-thumb {
  appearance: none; width: 18px; height: 18px; background: #4285f4;
  border-radius: 50%; cursor: pointer;
}
.lx-progress-slider::-moz-range-thumb {
  width: 18px; height: 18px; background: #4285f4; border-radius: 50%;
  cursor: pointer; border: none;
}
.lx-status-text {
  text-align: center; font-size: 12px; color: #666; margin-top: 4px;
}
.lx-text-window {
  font-family: monospace; white-space: pre-wrap; border: 1px solid #90caf9;
  padding: 12px; max-height: 260px; overflow-y: auto; margin-bottom: 12px;
  line-height: 1.6;
}
.lx-attributes-panel {
  background: #fafafa; border: 1px solid #90caf9; border-radius: 6px;
  padding: 8px 10px; margin-top: 8px; font-size: 13px;
}
.lx-current-highlight {
  border-bottom: 4px solid #ff4444;
  font-weight: bold;
  animation: lx-pulse 1s ease-in-out;
}
@keyframes lx-pulse {
  0% { text-decoration-color: #ff4444; }
  50% { text-decoration-color: #ff0000; }
  100% { text-decoration-color: #ff4444; }
}
.lx-legend {
  font-size: 12px; margin-bottom: 8px;
  padding-bottom: 8px; border-bottom: 1px solid #e0e0e0;
}
.lx-label {
  display: inline-block;
  padding: 2px 4px;
  border-radius: 3px;
  margin-right: 4px;
  color: #000;
}
.lx-attr-key {
  font-weight: 600;
  color: #1565c0;
  letter-spacing: 0.3px;
}
.lx-attr-value {
  font-weight: 400;
  opacity: 0.85;
  letter-spacing: 0.2px;
}

/* Add optimizations with larger fonts and better readability for GIFs */
.lx-gif-optimized .lx-text-window { font-size: 16px; line-height: 1.8; }
.lx-gif-optimized .lx-attributes-panel { font-size: 15px; }
.lx-gif-optimized .lx-current-highlight { text-decoration-thickness: 4px; }
</style>
    <div class="lx-animated-wrapper lx-gif-optimized">
      <div class="lx-attributes-panel">
        <div class="lx-legend">Highlights Legend: <span class="lx-label" style="background-color:#D2E3FC;">CONCEPT</span> <span class="lx-label" style="background-color:#C8E6C9;">KEYWORDS</span> <span class="lx-label" style="background-color:#FEF0C3;">SUMMARY</span> <span class="lx-label" style="background-color:#F9DEDC;">TOPIC</span></div>
        <div id="attributesContainer"></div>
      </div>
      <div class="lx-text-window" id="textWindow">
        Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering
School of Computer Science, in Beijing Institute of Technology
in last session we learned 3 main Recommendation System Algorithms,
User-based filtering, Item-based filtering and Content-Based Filtering<span class="lx-highlight lx-current-highlight" data-idx="0" style="background-color:#C8E6C9;">, and 
in <span class="lx-highlight" data-idx="1" style="background-color:#FEF0C3;">this session</span> we will learn a typical Latent Factor model, Matrix Factorization.
Let’s watch a video “How Recommender Systems Work (NetflixAmazon) 
From the video we know there is hidden patterns in the Rating Matrix, 
We want to Find some character the items may have.
And we Decompose the rating matrix into item-character rating &amp; user-character rating.
Moreover, we don’t want to know the meaning of characters. Which is Abstract model, we just suppose the number of characters.
In linear algebra, the <span class="lx-highlight" data-idx="2" style="background-color:#D2E3FC;">singular value decomposition (SVD)</span> is a factorization of a real or complex matrix. 
It generalizes the eigen decomposition of a square normal matrix with an orthonormal eigen basis to any m*n matrix.
It is related to the polar decomposition.
Specifically, the singular value decomposition of an m*n complex <span class="lx-highlight" data-idx="3" style="background-color:#F9DEDC;">matrix M is a factorization of the form U, 
where UΣV*, where U is an m*m complex unitary matrix, 
Σ is an m*n rectangular diagonal matrix with non-negative real numbers on the diagonal, 
and V is an n*n complex unitary matrix. 
If M is real, U and V can also be guaranteed to be real orthogonal matrices. 
In such contexts, the SVD is often denoted  UΣ(V transpose)
SVD requires dense matrix, that is the matrix doesn’t have missing values. 
Evidently, user-item rating matrix has lots of missing values. 
So, use <span class="lx-highlight" data-idx="4" style="background-color:#D2E3FC;">Matrix Decomposition</span> to replace SVD.
Decompose the matrix into two matrix, that is , where  is  user-item rating matrix,  is  user-LF(Latent factor ) matrix,  and is  item-LF(Latent factor ) matrix.
For u-user and i-item, their rating is:
If get two dense matrix , from  we can predict the missing value in .
So how to calculate  ?
Define <span class="lx-highlight" data-idx="5" style="background-color:#D2E3FC;">cost function</span> as showed in the formula, and use the cost function to evaluate the better choice of P and Q. We only calculate cost function with the already given rating values by the users.
The first part in the cost function is the MSE of the predict rating value Rui hat and true value Rui. 
And the second part in the cost function is regular value, which prevent overfitting.
We do the iteration to minimize the cost function.
Two ways to minimize cost function:
) using ALS (Alternating Least Square) to minimize cost function :which means fix P, compute Q to make c min; then, fix Q, compute P to make c min; 
End until reach max iteration or c satisfies threshold condition.
Compute partial derivative of C with respect to Pu, and make the formula equals to 0, get .
Similarly get .
) using <span class="lx-highlight" data-idx="6" style="background-color:#D2E3FC;">Gradient Descent</span> to minimize cost function, which means Compute partial derivative of C with respect to Pu
and partial derivative of C with respect to Qi.
then Do Iteration using the formula in the slide (where   is step size)
After each iteration, the Pu and Qi are updated until the end, until reach max iteration or c satisfies threshold condition.
To understand the recommendation system</span>, A series of experiments were designed, 
it includes User-based filtering recommendation and Matrix Decomposition.
In User-based filtering recommendation, it consist preprocessing and Collaborative Filtering.
In 1.1 preprocessing, it includes load the data and relate the two original tables, and create a new data.csv file, and make a dictionary by deleting the duplicate records. 
In 1.2 Collaborative Filtering, first compute the user similarity, and then list top 10 similar users to the current user, and make the recommendation.
For matrix decomposition experiment, we need import the library</span> surprise, the used data set includes 100,000 user’s ratings on movies.
The related models include Funk or Bias SVD, Grid Search for training.
The goal is to Train and test on the best model and Get the best parameters for SVD,
The process will be 
）Import library 
）import data 
）Grid search SVD training 
）Use the best parameters obtained by grid search for raining and prediction 
And finally visualize the Result.
All the experiments material including the manual and codes are provided on the platform, which can help you to do the hands-on.
In this session, we learned a typical Latent Factor model, <span class="lx-highlight" data-idx="7" style="background-color:#D2E3FC;">Matrix Factorization</span>.
thank you for your attention, if you have any question, feel free to contact me.
      </div>
      <div class="lx-controls">
        <div class="lx-button-row">
          <button class="lx-control-btn" onclick="playPause()">▶️ Play</button>
          <button class="lx-control-btn" onclick="prevExtraction()">⏮ Previous</button>
          <button class="lx-control-btn" onclick="nextExtraction()">⏭ Next</button>
        </div>
        <div class="lx-progress-container">
          <input type="range" id="progressSlider" class="lx-progress-slider"
                 min="0" max="7" value="0"
                 onchange="jumpToExtraction(this.value)">
        </div>
        <div class="lx-status-text">
          Entity <span id="entityInfo">1/8</span> |
          Pos <span id="posInfo">[1114-3158]</span>
        </div>
      </div>
    </div>

    <script>
      (function() {
        const extractions = [{"index": 0, "class": "KEYWORDS", "text": "Matrix Factorization, Latent Factor model, Recommendation System, Singular Value Decomposition (SVD), Cost Function, Alternating Least Square (ALS), Gradient Descent, Collaborative Filtering, Surprise library", "color": "#C8E6C9", "startPos": 292, "endPos": 3757, "beforeText": " Technology\nin last session we learned 3 main Recommendation System Algorithms,\nUser-based filtering, Item-based filtering and Content-Based Filtering", "extractionText": ", and \nin this session we will learn a typical Latent Factor model, Matrix Factorization.\nLet\u2019s watch a video \u201cHow Recommender Systems Work (NetflixAmazon) \nFrom the video we know there is hidden patterns in the Rating Matrix, \nWe want to Find some character the items may have.\nAnd we Decompose the rating matrix into item-character rating &amp; user-character rating.\nMoreover, we don\u2019t want to know the meaning of characters. Which is Abstract model, we just suppose the number of characters.\nIn\u00a0linear algebra, the\u00a0singular value decomposition\u00a0(SVD) is a\u00a0factorization\u00a0of a\u00a0real\u00a0or\u00a0complex\u00a0matrix. \nIt generalizes the\u00a0eigen decomposition\u00a0of a square\u00a0normal matrix\u00a0with an orthonormal eigen basis to any\u00a0m*n\u00a0matrix.\nIt is related to the\u00a0polar decomposition.\nSpecifically, the singular value decomposition of an\u00a0m*n\u00a0complex matrix\u00a0M\u00a0is a factorization of the form\u00a0U, \nwhere\u00a0U\u03a3V*, where U\u00a0is an\u00a0m*m\u00a0complex\u00a0unitary matrix,\u00a0\n\u03a3\u00a0is an\u00a0m*n\u00a0rectangular diagonal matrix\u00a0with non-negative real numbers on the diagonal, \nand\u00a0V\u00a0is an\u00a0n*n\u00a0complex unitary matrix. \nIf\u00a0M\u00a0is real,\u00a0U\u00a0and\u00a0V\u00a0can also be guaranteed to be real\u00a0orthogonal\u00a0matrices. \nIn such contexts, the SVD is often denoted\u00a0\u00a0U\u03a3(V transpose)\nSVD requires dense matrix, that is the matrix doesn\u2019t have missing values. \nEvidently, user-item rating matrix has lots of missing values. \nSo, use Matrix Decomposition to replace SVD.\nDecompose the matrix into two matrix, that is , where  is  user-item rating matrix,  is  user-LF(Latent factor ) matrix,  and is  item-LF(Latent factor ) matrix.\nFor u-user and i-item, their rating is:\nIf get two dense matrix , from  we can predict the missing value in .\nSo how to calculate  ?\nDefine cost function as showed in the formula, and use the cost function to evaluate the better choice of P and Q. We only calculate cost function with the already given rating values by the users.\nThe first part in the cost function is the MSE of the predict rating value Rui hat and true value Rui. \nAnd the second part in the cost function is regular value, which prevent overfitting.\nWe do the iteration to minimize the cost function.\nTwo ways to minimize cost function:\n) using ALS (Alternating Least Square) to minimize cost function :which means fix P, compute Q to make c min; then, fix Q, compute P to make c min; \nEnd until reach max iteration or c satisfies threshold condition.\nCompute partial derivative of C with respect to Pu, and make the formula equals to 0, get .\nSimilarly get .\n) using Gradient Descent to minimize cost function, which means Compute partial derivative of C with respect to Pu\nand partial derivative of C with respect to Qi.\nthen Do Iteration using the formula in the slide (where   is step size)\nAfter each iteration, the Pu and Qi are updated until the end, until reach max iteration or c satisfies threshold condition.\nTo understand the recommendation system, A series of experiments were designed, \nit includes User-based filtering recommendation and Matrix Decomposition.\nIn User-based filtering recommendation, it consist preprocessing and Collaborative Filtering.\nIn 1.1 preprocessing, it includes load the data and relate the two original tables, and create a new data.csv file, and make a dictionary by deleting the duplicate records. \nIn 1.2 Collaborative Filtering, first compute the user similarity, and then list top 10 similar users to the current user, and make the recommendation.\nFor matrix decomposition experiment, we need import the library", "afterText": " surprise, the used data set includes 100,000 user\u2019s ratings on movies.\nThe related models include Funk or Bias SVD, Grid Search for training.\nThe goa", "attributesHtml": "<div><strong>class:</strong> KEYWORDS</div><div><strong>attributes:</strong> {}</div>"}, {"index": 1, "class": "SUMMARY", "text": "This session introduces Matrix Factorization as a typical Latent Factor model for recommendation systems, contrasting it with previously learned methods like user-based and item-based filtering. The core idea is to find hidden patterns or characters in a user-item rating matrix by decomposing it into two separate matrices: a user-character rating matrix and an item-character rating matrix. While Singular Value Decomposition (SVD) is a powerful matrix factorization technique, it is unsuitable for recommendation systems because it requires a dense matrix, whereas user-item rating matrices are typically sparse with many missing values. Therefore, an alternative Matrix Decomposition approach is used, which decomposes the rating matrix R into a user-latent factor matrix (P) and an item-latent factor matrix (Q). The goal is to find P and Q such that their product can predict the missing ratings. This is achieved by minimizing a cost function, which includes the Mean Squared Error (MSE) of predicted ratings and a regularization term to prevent overfitting. Two methods for minimizing this cost function are discussed: Alternating Least Square (ALS), which iteratively optimizes P and Q, and Gradient Descent, which updates P and Q based on partial derivatives. The session concludes by outlining a series of hands-on experiments, one for user-based filtering and another for Matrix Decomposition using the Surprise library and a movie ratings dataset.", "color": "#FEF0C3", "startPos": 302, "endPos": 314, "beforeText": "y\nin last session we learned 3 main Recommendation System Algorithms,\nUser-based filtering, Item-based filtering and Content-Based Filtering, and \nin ", "extractionText": "this session", "afterText": " we will learn a typical Latent Factor model, Matrix Factorization.\nLet\u2019s watch a video \u201cHow Recommender Systems Work (NetflixAmazon) \nFrom the video ", "attributesHtml": "<div><strong>class:</strong> SUMMARY</div><div><strong>attributes:</strong> {}</div>"}, {"index": 2, "class": "CONCEPT", "text": "Singular Value Decomposition (SVD)", "color": "#D2E3FC", "startPos": 807, "endPos": 841, "beforeText": "\nMoreover, we don\u2019t want to know the meaning of characters. Which is Abstract model, we just suppose the number of characters.\nIn\u00a0linear algebra, the\u00a0", "extractionText": "singular value decomposition\u00a0(SVD)", "afterText": " is a\u00a0factorization\u00a0of a\u00a0real\u00a0or\u00a0complex\u00a0matrix. \nIt generalizes the\u00a0eigen decomposition\u00a0of a square\u00a0normal matrix\u00a0with an orthonormal eigen basis to ", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A factorization of a real or complex matrix that generalizes the eigen decomposition. A key limitation is that it requires a dense matrix with no missing values, making it unsuitable for sparse user-item rating matrices.</span>}</div>"}, {"index": 3, "class": "TOPIC", "text": "Matrix Factorization for Recommendation Systems", "color": "#F9DEDC", "startPos": 1114, "endPos": 3158, "beforeText": "orthonormal eigen basis to any\u00a0m*n\u00a0matrix.\nIt is related to the\u00a0polar decomposition.\nSpecifically, the singular value decomposition of an\u00a0m*n\u00a0complex ", "extractionText": "matrix\u00a0M\u00a0is a factorization of the form\u00a0U, \nwhere\u00a0U\u03a3V*, where U\u00a0is an\u00a0m*m\u00a0complex\u00a0unitary matrix,\u00a0\n\u03a3\u00a0is an\u00a0m*n\u00a0rectangular diagonal matrix\u00a0with non-negative real numbers on the diagonal, \nand\u00a0V\u00a0is an\u00a0n*n\u00a0complex unitary matrix. \nIf\u00a0M\u00a0is real,\u00a0U\u00a0and\u00a0V\u00a0can also be guaranteed to be real\u00a0orthogonal\u00a0matrices. \nIn such contexts, the SVD is often denoted\u00a0\u00a0U\u03a3(V transpose)\nSVD requires dense matrix, that is the matrix doesn\u2019t have missing values. \nEvidently, user-item rating matrix has lots of missing values. \nSo, use Matrix Decomposition to replace SVD.\nDecompose the matrix into two matrix, that is , where  is  user-item rating matrix,  is  user-LF(Latent factor ) matrix,  and is  item-LF(Latent factor ) matrix.\nFor u-user and i-item, their rating is:\nIf get two dense matrix , from  we can predict the missing value in .\nSo how to calculate  ?\nDefine cost function as showed in the formula, and use the cost function to evaluate the better choice of P and Q. We only calculate cost function with the already given rating values by the users.\nThe first part in the cost function is the MSE of the predict rating value Rui hat and true value Rui. \nAnd the second part in the cost function is regular value, which prevent overfitting.\nWe do the iteration to minimize the cost function.\nTwo ways to minimize cost function:\n) using ALS (Alternating Least Square) to minimize cost function :which means fix P, compute Q to make c min; then, fix Q, compute P to make c min; \nEnd until reach max iteration or c satisfies threshold condition.\nCompute partial derivative of C with respect to Pu, and make the formula equals to 0, get .\nSimilarly get .\n) using Gradient Descent to minimize cost function, which means Compute partial derivative of C with respect to Pu\nand partial derivative of C with respect to Qi.\nthen Do Iteration using the formula in the slide (where   is step size)\nAfter each iteration, the Pu and Qi are updated until the end, until reach max iteration or c satisfies threshold condition.\nTo understand the recommendation system", "afterText": ", A series of experiments were designed, \nit includes User-based filtering recommendation and Matrix Decomposition.\nIn User-based filtering recommenda", "attributesHtml": "<div><strong>class:</strong> TOPIC</div><div><strong>attributes:</strong> {}</div>"}, {"index": 4, "class": "CONCEPT", "text": "Matrix Decomposition", "color": "#D2E3FC", "startPos": 1629, "endPos": 1649, "beforeText": ")\nSVD requires dense matrix, that is the matrix doesn\u2019t have missing values. \nEvidently, user-item rating matrix has lots of missing values. \nSo, use ", "extractionText": "Matrix Decomposition", "afterText": " to replace SVD.\nDecompose the matrix into two matrix, that is , where  is  user-item rating matrix,  is  user-LF(Latent factor ) matrix,  and is  ite", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A method to handle sparse matrices in recommendation systems by decomposing the user-item rating matrix (R) into two matrices: a user-latent factor matrix (P) and an item-latent factor matrix (Q). The resulting dense matrices can be used to predict missing values in the original matrix.</span>}</div>"}, {"index": 5, "class": "CONCEPT", "text": "Cost Function", "color": "#D2E3FC", "startPos": 1968, "endPos": 1981, "beforeText": ") matrix.\nFor u-user and i-item, their rating is:\nIf get two dense matrix , from  we can predict the missing value in .\nSo how to calculate  ?\nDefine ", "extractionText": "cost function", "afterText": " as showed in the formula, and use the cost function to evaluate the better choice of P and Q. We only calculate cost function with the already given ", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A function used to evaluate and find the optimal P and Q matrices in Matrix Decomposition. It consists of the Mean Squared Error (MSE) between predicted and true ratings, plus a regularization term to prevent overfitting. The goal is to minimize this function.</span>}</div>"}, {"index": 6, "class": "CONCEPT", "text": "Gradient Descent", "color": "#D2E3FC", "startPos": 2767, "endPos": 2783, "beforeText": " c satisfies threshold condition.\nCompute partial derivative of C with respect to Pu, and make the formula equals to 0, get .\nSimilarly get .\n) using ", "extractionText": "Gradient Descent", "afterText": " to minimize cost function, which means Compute partial derivative of C with respect to Pu\nand partial derivative of C with respect to Qi.\nthen Do Ite", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">An iterative optimization method to minimize the cost function by computing the partial derivatives with respect to matrices P and Q and updating them in each iteration until a stopping condition is met.</span>}</div>"}, {"index": 7, "class": "CONCEPT", "text": "Matrix Factorization", "color": "#D2E3FC", "startPos": 4363, "endPos": 4383, "beforeText": " the manual and codes are provided on the platform, which can help you to do the hands-on.\nIn this session, we learned a typical Latent Factor model, ", "extractionText": "Matrix Factorization", "afterText": ".\nthank you for your attention, if you have any question, feel free to contact me.", "attributesHtml": "<div><strong>class:</strong> CONCEPT</div><div><strong>attributes:</strong> {<span class=\"lx-attr-key\">definition</span>: <span class=\"lx-attr-value\">A typical Latent Factor model used in recommendation systems to find hidden patterns in a rating matrix by decomposing it into an item-character rating matrix and a user-character rating matrix.</span>}</div>"}];
        let currentIndex = 0;
        let isPlaying = false;
        let animationInterval = null;
        let animationSpeed = 1.0;

        function updateDisplay() {
          const extraction = extractions[currentIndex];
          if (!extraction) return;

          document.getElementById('attributesContainer').innerHTML = extraction.attributesHtml;
          document.getElementById('entityInfo').textContent = (currentIndex + 1) + '/' + extractions.length;
          document.getElementById('posInfo').textContent = '[' + extraction.startPos + '-' + extraction.endPos + ']';
          document.getElementById('progressSlider').value = currentIndex;

          const playBtn = document.querySelector('.lx-control-btn');
          if (playBtn) playBtn.textContent = isPlaying ? '⏸ Pause' : '▶️ Play';

          const prevHighlight = document.querySelector('.lx-text-window .lx-current-highlight');
          if (prevHighlight) prevHighlight.classList.remove('lx-current-highlight');
          const currentSpan = document.querySelector('.lx-text-window span[data-idx="' + currentIndex + '"]');
          if (currentSpan) {
            currentSpan.classList.add('lx-current-highlight');
            currentSpan.scrollIntoView({block: 'center', behavior: 'smooth'});
          }
        }

        function nextExtraction() {
          currentIndex = (currentIndex + 1) % extractions.length;
          updateDisplay();
        }

        function prevExtraction() {
          currentIndex = (currentIndex - 1 + extractions.length) % extractions.length;
          updateDisplay();
        }

        function jumpToExtraction(index) {
          currentIndex = parseInt(index);
          updateDisplay();
        }

        function playPause() {
          if (isPlaying) {
            clearInterval(animationInterval);
            isPlaying = false;
          } else {
            animationInterval = setInterval(nextExtraction, animationSpeed * 1000);
            isPlaying = true;
          }
          updateDisplay();
        }

        window.playPause = playPause;
        window.nextExtraction = nextExtraction;
        window.prevExtraction = prevExtraction;
        window.jumpToExtraction = jumpToExtraction;

        updateDisplay();
      })();
    </script>