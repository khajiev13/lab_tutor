{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "MapReduce Batch Processing Computing Model", "char_interval": {"start_pos": 176, "end_pos": 216}, "alignment_status": "match_fuzzy", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "This session introduces the batch processing computing model, specifically MapReduce, as a core component of big data processing systems. Data processing systems are layered, with the computing model layer defining how data is handled in various scenarios, including batch, stream, and in-memory computing. MapReduce parallelizes large computing tasks in a distributed environment to enhance efficiency. The process begins by retrieving data from HDFS data blocks and organizing it into splits for map tasks. The output of the map tasks undergoes a shuffle phase involving sorting, copying, and merging. These intermediate results then feed into the reduce tasks to produce the final output. The MapReduce architecture consists of four main parts: the Client, which submits jobs; the JobTracker, which handles resource monitoring and job scheduling; the TaskTracker, which executes tasks on nodes and reports status via heartbeats; and the Task, which is either a Map Task or a Reduce Task. Resource management is handled through 'slots' (Map and Reduce slots representing CPU and memory) allocated by the Hadoop scheduler. For optimal performance, HDFS and MapReduce should be co-located on the same cluster to minimize data transmission overhead.", "char_interval": {"start_pos": 160, "end_pos": 172}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "MapReduce, batch processing, HDFS, JobTracker, TaskTracker, shuffle phase, slot, Hadoop scheduler, distributed computing, data processing system", "char_interval": {"start_pos": 233, "end_pos": 3941}, "alignment_status": "match_fuzzy", "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Computing Models", "char_interval": {"start_pos": 201, "end_pos": 216}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "The way that different kinds of big data is processed in different scenarios, which include batch processing, stream computing, Large-scale concurrent processing (MPP) model for structured data, In-memory Computing model, and Data Flow Graph models."}}, {"extraction_class": "CONCEPT", "extraction_text": "MapReduce", "char_interval": {"start_pos": 3948, "end_pos": 3957}, "alignment_status": "match_exact", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "A batch processing model that parallelizes large computing tasks in a distributed environment to improve efficiency. It involves map tasks, a shuffle phase, and reduce tasks to process data."}}, {"extraction_class": "CONCEPT", "extraction_text": "Client (MapReduce)", "char_interval": {"start_pos": 1695, "end_pos": 1868}, "alignment_status": "match_fuzzy", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "The component through which a user submits a MapReduce program to the JobTracker and can view the job's running status."}}, {"extraction_class": "CONCEPT", "extraction_text": "JobTracker", "char_interval": {"start_pos": 1665, "end_pos": 1675}, "alignment_status": "match_fuzzy", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "A component responsible for resource monitoring and job scheduling. It monitors the health of all TaskTrackers and jobs, tracks task progress, and informs the task scheduler about resource availability."}}, {"extraction_class": "CONCEPT", "extraction_text": "TaskTracker", "char_interval": {"start_pos": 1676, "end_pos": 1687}, "alignment_status": "match_fuzzy", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "A component that periodically reports resource usage and task progress to the JobTracker via a 'heartbeat' and executes commands from the JobTracker, such as starting or killing tasks."}}, {"extraction_class": "CONCEPT", "extraction_text": "Task (MapReduce)", "char_interval": {"start_pos": 2205, "end_pos": 2339}, "alignment_status": "match_fuzzy", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "A unit of work, divided into Map Task and Reduce Task, which is started by a TaskTracker."}}, {"extraction_class": "CONCEPT", "extraction_text": "Slot", "char_interval": {"start_pos": 2671, "end_pos": 2675}, "alignment_status": "match_fuzzy", "extraction_index": 10, "group_index": 9, "description": null, "attributes": {"definition": "Represents a specific amount of resources (CPU, memory, etc.) on a node. A task can run after it acquires a slot. Slots are divided into Map slots and Reduce slots for their respective tasks."}}, {"extraction_class": "CONCEPT", "extraction_text": "Hadoop Scheduler", "char_interval": {"start_pos": 2814, "end_pos": 2830}, "alignment_status": "match_fuzzy", "extraction_index": 11, "group_index": 10, "description": null, "attributes": {"definition": "A component whose role is to allocate idle slots on each TaskTracker to a waiting Task."}}], "text": "Hello everyone, I am Haiying Che, from  Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology , in this session we discuss batch processing computing model, represented by MapReduce.\nThe data processing system provides big data computing and processing capabilities and an application development platform.\nFrom the perspective of computing architecture, the data processing system is divided into data algorithm layer, computing model layer, computing platform layer, computing engine layer, etc.\nComputing models are the way that different kinds of big data is processed in different scenarios, \nwhich include batch processing, stream computing, Large-scale concurrent processing (MPP) model for structured data, In-memory Computing model, and Data Flow Graph models.\nWe first look at the batch processing model represented by MapReduce\nLet’s watch a video about “Learn MapReduce with Playing Cards “ to easily understand the mechanism of map reduce process Intuitively\nFrom the video we understand the basic working mechanism of Map Reduce. \nMR tries to parallelize the big computing tasks in distributed environment to improve efficiency.\nthe storing unit in HDFS is data blocks, Retrieve the data from HDFS data blocks , then organize input data as data split feed into the map tasks.\nThe output of map task are going through the shuffle  phase through sort , copy and merge operations showed in the diagram.\nAfter shuffle the reorganized middle results become the input of the reduce tasks, and after reduce phase , the final result is calculated.\nMapReduce architecture mainly four parts: Client, JobTracker,TaskTracker，Task\n) Client\nMapReduce program written by the user is submitted to the JobTracker through the client\nUsers can view job running status through some interfaces provided by Client\n) JobTracker\nJobTracker is responsible for resource monitoring and job scheduling\nJobTracker monitors the health status of all TaskTrackers and Jobs, and if it finds a failure, it will transfer the corresponding tasks to other nodes\nJobTracker will track the task execution progress, resource usage, and other information, and inform the task scheduler (TaskScheduler), and the scheduler will select the appropriate task to use these resources when resources become free\n) TaskTracker\nTaskTracker will periodically report the usage of resources on the node and the progress of the task to the JobTracker through the \"heartbeat\", \nand at the same time receive the commands sent by the JobTracker and perform the corresponding operations (such as starting new tasks, killing tasks, etc.)\nTaskTracker uses \"slot\" to divide the amount of resources (CPU, memory, etc.) on this node. A Task has a chance to run after it gets a slot, and the role of the Hadoop scheduler is to allocate idle slots on each TaskTracker to the Task. Slots are divided into Map slot and Reduce slot, which are used by MapTask and Reduce Task respectively.\n) Task\nTask is divided into Map Task and Reduce Task, which are started by TaskTracker\nTask scheduler is responsible for Selecting the appropriate task to use these resources when resources become free\nSlot is amount of resources (CPU, memory, etc.)., which include Map slot and Reduce slot.\nHadoop scheduler is to allocate idle slots on each TaskTracker to the Task.\nLet’s combine the HDFS and MapReduce together. HDFS and MapReduce  should be built on the same cluster. The master node should be both name node in HDFS and Job tracker in MapReduce, \nand the slave node should be both the datanode in HDFS and tasktracker in MapReduce.\nFor example we build a one master node and three slave nodes HDFS and MapReduce cluster.\nIn order to reduce the data transmission overhead, we should try to make the input data of corresponding map task to be as close as possible, it is better on the same machine.\nIn this session, we learned batch processing model MapReduce.\nthank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_3d96a3b1"}
