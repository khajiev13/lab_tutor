{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "Introduction to Data Preprocessing", "char_interval": {"start_pos": 4681, "end_pos": 5097}, "alignment_status": "match_fuzzy", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "This session provides an overview of Data Preprocessing, which consists of three main components: data cleaning, data transformation, and data reduction. Data cleaning is necessary to handle 'dirty data'—such as missing values, repeated records, conflicting information, and outliers—which can damage analysis results. Methods for cleaning include ignoring tuples, manually filling missing values, or using techniques like binning, regression, and clustering for noisy data. Data transformation involves processes like normalization, attribute selection, and discretization. Data reduction aims to make mining massive datasets more effective by reducing data volume through techniques like data cube aggregation, dimensionality reduction (e.g., stepwise selection), reducing the amount of data (e.g., histograms, clustering), and data discretization, which converts continuous attributes into discrete ones. The lecture also touches on data integration, which consolidates data into a complete set, and data specification, which eliminates irrelevant attributes.", "char_interval": {"start_pos": 67, "end_pos": 79}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "Data Preprocessing, Data Cleaning, Data Transformation, Data Reduction, Dirty Data, Missing Data, Noisy Data, Dimensionality Reduction, Data Discretization", "char_interval": {"start_pos": 5079, "end_pos": 5097}, "alignment_status": "match_lesser", "extraction_index": 3, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Data Preprocessing", "char_interval": {"start_pos": 97, "end_pos": 115}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 0, "description": null, "attributes": {"definition": "A process that mainly includes data cleaning, data transformation, and data reduction to make data mining more effective."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Cleaning", "char_interval": {"start_pos": 152, "end_pos": 165}, "alignment_status": "match_fuzzy", "extraction_index": 5, "group_index": 1, "description": null, "attributes": {"definition": "A process to handle 'dirty data' such as missing, repeated, conflicting, or abnormal records. It includes missing data processing and noisy data processing."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Transformation", "char_interval": {"start_pos": 166, "end_pos": 185}, "alignment_status": "match_fuzzy", "extraction_index": 6, "group_index": 2, "description": null, "attributes": {"definition": "A component of data preprocessing that includes Normalization, attribute selection, discretization, and concept hierarchy generation."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Reduction", "char_interval": {"start_pos": 190, "end_pos": 204}, "alignment_status": "match_fuzzy", "extraction_index": 7, "group_index": 3, "description": null, "attributes": {"definition": "A component of data preprocessing that includes data cube aggregation, attribute subset selection, numerosity reduction (reducing the amount of data), and dimensionality reduction."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Integration", "char_interval": {"start_pos": 2082, "end_pos": 2098}, "alignment_status": "match_fuzzy", "extraction_index": 8, "group_index": 4, "description": null, "attributes": {"definition": "The process of storing all data in a database, data warehouse, or file to form a complete data set."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Specification", "char_interval": {"start_pos": 2186, "end_pos": 2204}, "alignment_status": "match_fuzzy", "extraction_index": 9, "group_index": 5, "description": null, "attributes": {"definition": "The process of eliminating data attributes that cannot describe the key characteristics of the system, retaining only a subset that can."}}, {"extraction_class": "CONCEPT", "extraction_text": "Deduplication of data", "char_interval": {"start_pos": 2944, "end_pos": 2965}, "alignment_status": "match_fuzzy", "extraction_index": 10, "group_index": 6, "description": null, "attributes": {"definition": "The process of cleaning data to remove duplicate records, which is particularly important when integrating data from different sources."}}, {"extraction_class": "CONCEPT", "extraction_text": "Dimensionality Reduction", "char_interval": {"start_pos": 2054, "end_pos": 2078}, "alignment_status": "match_fuzzy", "extraction_index": 11, "group_index": 7, "description": null, "attributes": {"definition": "A data reduction method that involves deleting redundant data attributes, often requiring business knowledge. Common methods include stepwise forward selection, stepwise backward deletion, and decision tree induction."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Discretization", "char_interval": {"start_pos": 4831, "end_pos": 4850}, "alignment_status": "match_fuzzy", "extraction_index": 12, "group_index": 8, "description": null, "attributes": {"definition": "A technology that converts continuous attributes into discrete attribute values to reduce the number of attribute values and thereby decrease computing time."}}], "text": "School of Computer Science, in Beijing Institute of Technology, in this session, we will discuss Data Preprocessing.\nData Preprocessing mainly includes data cleaning Data transformation and data reduction\nWhy do we need to do data cleaning? Because when we collect data from different data resources, we inevitably have some dirty data in the results.\nLet’s look at an example, in this table, in the second row , for the record id=2, the age attributes is missing. \nIf we calculate the average age, this missing age value will Serious impact the average result.\nand the third and fourth records are repeated. And the sixth record is conflict record, according to the age and the birth data.\nthe seventh record is abnormal age 101 according to the birth data. All these dirty data will damage the analysis results.\nAccording to Murphy's Law: As long as everything can go wrong, it will go wrong.\nHow to prevent dirty data from appearing, we can try 2 different ways. \n) Develop data standards:\nUnify attribute value encoding of multiple data sources\nGive the attribute name and attribute value as clear as possible\n) Optimize system design:\nUse options as much as possible for key attributes instead of manually filling in the entry.\nImportant attributes appear in a prominent position, use required options\nOutliers should be modified\nData preprocessing could include data cleaning; data transformation and data reduction\n) data cleaning includes missing data processing and noisy data processing.\n1 For missing data we can directly ignore the tuple, or record.  Or we can fill the missing values manually, by mean or by most probable value.\n2 eliminating the noisy data methods include binning methods, regression and clustering.\n) As for data transformation, it includes Normalization, attributes selection, discretization and concept hierarchy generation.\n) The third aspect of data preprocessing is data reduction.\nData reduction includes data cube aggregation, attribute subset selection, numerosity reduction which is Reduce the amount of data and dimensionality reduction.\n) Data integration: \nStore all data in a database, data warehouse or file to form a complete data set. \n) Data specification: \nEliminate the data attributes that cannot describe the key characteristics of the system, and only retain part of the data attribute set that can describe the key characteristics.\nAnd others…\nThe research on data cleaning technology first began with the correction of the US Social Security number. Later, with the rapid development of information and commerce, \nthe research in this area was accelerated, including\n) Perform anomaly detection on the data set, \nusually using statistical methods to detect the numerical attributes of the data. By calculating the mean and standard deviation of the attribute values ​​and other indicators, identify the anomalous attributes and records within the confidence interval of each attribute.\n) Deduplication of data:\nThe process of data deduplication is to repeat the process of cleaning data records.\nThis process is particularly important in data warehouse applications, because when integrating data from different data sources, many duplicate data records may be generated.\n) Cleaning of missing data: Most of them use approximate values ​​to replace missing values ​​to clean the data. \nMethods to obtain approximate values ​​include Bayesian networks, neural networks, KNN classification, rough set theory, etc. \nThe core of these methods is to judge missing records and Similarity between other complete records.\nDomestic research in this area is still in its initial stage. \nData cleaning is mainly concentrated in industries that require high accuracy of customer data, such as banking, insurance, and securities. These industries only do data cleaning for their customers, and only develop software for specific applications, without general products\nIt takes a long time to analyze and mine massive data. To make data mining more effective, data needs to be regulated. The main research content of the data reduction:\n) Dimensionality reduction processing of high-dimension data\nThe process mainly adopts the method of market redundant data attributes, and deleting redundant data attributes requires business knowledge in a certain field. Commonly used dimensionality reduction methods include stepwise forward selection method, stepwise backward deletion method, and decision tree induction method Wait\n) Reduce the amount of data\nWhen it takes a long time to process a large amount of data, it cannot meet the requirements of some applications with high real-time requirements. At this time, the amount of data needs to be reduced. The methods of comrades in this process, including histogram, clustering, etc., and then select smaller-scale data from the data set\n) Data Discretization Technology\nThis technology can convert continuous attributes into discrete attribute values, reducing the number of attribute values, thereby reducing the computing time for processing data\nIn this session we general introduced Data Preprocessing.\nthank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_2e65f8a0"}
