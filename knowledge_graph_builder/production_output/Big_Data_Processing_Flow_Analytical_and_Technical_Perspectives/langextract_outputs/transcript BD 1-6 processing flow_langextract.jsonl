{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "Big Data Processing Flow: Analytical and Technical Perspectives", "char_interval": {"start_pos": 193, "end_pos": 217}, "alignment_status": "match_lesser", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "The ultimate goal of big data processing is to extract value from data. This process can be viewed from two perspectives. From an analytical perspective, the flow involves identifying suitable structured, semi-structured, or unstructured data sources; collecting data using methods like ETL or crawlers; transforming and storing it, often in distributed systems. The data is then processed, which includes cleaning to remove errors and redundancies, and feature engineering to build key features for models. Finally, statistical or machine learning algorithms are applied to find patterns, which are then visualized through maps or reports to support business monitoring and decision-making. From a technical perspective, various data assets like transactions (OLTP, OLAP), documents, social media, and IoT data are integrated into a central cluster, such as Hadoop. In the cluster, data scientists discover, profile, and govern the data using metadata, while masking sensitive information to protect privacy. This prepared data is then used to generate business intelligence reports and visualizations. For more focused analysis, curated data may be moved to an Enterprise Data Warehouse to support specific business intelligence needs.", "char_interval": {"start_pos": 366, "end_pos": 395}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "Big Data Processing, Analytical Perspective, Technical Perspective, ETL, Feature Engineering, Hadoop, Business Intelligence, OLTP, OLAP, Data Governance", "char_interval": {"start_pos": 193, "end_pos": 2381}, "alignment_status": "match_fuzzy", "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "ETL (Extract Transform Load)", "char_interval": {"start_pos": 748, "end_pos": 775}, "alignment_status": "match_exact", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "A method used to extract data from different databases or data warehouses as part of the data collection process."}}, {"extraction_class": "CONCEPT", "extraction_text": "Feature Engineering", "char_interval": {"start_pos": 1402, "end_pos": 1421}, "alignment_status": "match_exact", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "The process of extracting or building representative key features from data which can enable the further use of a model or algorithm."}}, {"extraction_class": "CONCEPT", "extraction_text": "OLTP (Online Transaction Processing)", "char_interval": {"start_pos": 2239, "end_pos": 2243}, "alignment_status": "match_lesser", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "A type of data asset consisting of online transaction data."}}, {"extraction_class": "CONCEPT", "extraction_text": "OLAP (Online Analytical Processing)", "char_interval": {"start_pos": 2279, "end_pos": 2283}, "alignment_status": "match_lesser", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "A type of data asset consisting of online analysis processing data."}}, {"extraction_class": "CONCEPT", "extraction_text": "Hadoop", "char_interval": {"start_pos": 2601, "end_pos": 2607}, "alignment_status": "match_exact", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "A cluster technology used for big data storage and processing, where various data assets are offloaded and integrated for further analysis."}}, {"extraction_class": "CONCEPT", "extraction_text": "Enterprise Data Warehouse", "char_interval": {"start_pos": 3951, "end_pos": 3976}, "alignment_status": "match_exact", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "A data store that holds curated and relevant data moved from a larger cluster, specifically for generating business intelligence and supporting decision-making."}}], "text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering，\nSchool of Computer Science, Beijing Institute of Technology, \nin this session, we will discuss about Big Data Processing flow both from the analytical perspective and technical perspective. \nAll the big data analysis and processing is about to get value from data. \nThat is the ultimate goal of big data analysis. In order to achieve it, we need to do several steps, this diagram shows the bigdata process flow in analytical perspective.\nAccording to the analysis requirement to find the suitable data sources, which could be structured data, semi structured data or unstructured data.\nThen collect data from the selected data sources, which could use the ETL(Extract Transform load) to extract data from different database or data warehouse or use data crawlers to crawl the data. \nAfter get data from different data sources, sometime the data format is different, it need transform to facilitate the further data store or processing.\nwe need plan the suitable way to store the data, due to the nature of big data, data volume is big, which may need distributed data storage solution.\nThen the data need to be processed to support analysis. In data Processing, \ndata scientist maybe first clean the data, delete the wrong data, redundant data, and other dirty data. \nAfter that, data scientist need to do the feature engineering, to extract or build the representative key features which can enable the further model or algorithm.\nThen data scientist  could apply statistic, mathematic models or machine learning algorithms to find the pattern, correlation, classification and so on.\nthe pattern, correlations, classification and so on will be visualized, which can make the results easy to understand. For example, the pandemic data of the different provinces could explain the distribution better in the form of map than table.\nalso some reports could be generated to do the further analysis. Or the results could be used to monitor the business.\nThese are the Big data process flow in analytical perspective.\nNow  let’ s look at Big data process flow-technical perspective.\nAt the most left side,  are the data assets, including transactions, OLTP, online transaction processing and OLAP online analyzing processing, \ndifferent kinds of documents, social media content and the IoT data generated by machine devices. \nAll these data need to be integrated into cluster or big data storage to support the further analysis.\ntransaction data in database or data warehouse need to be offload & ETL processed to Hadoop, which is mostly the first step of journey. \nThe transaction data is mostly structured data.\nSome document data can also be used for data analysis, these data also need to be offloaded to Hadoop cluster for the further processing and analysis.\nSocial media data can help to understand customer’s opinion and preference, which enable better customized service .\nWith the help of the IoT device , we can collect IoT data from different kind of sensors, video cameras and so on , those data can help us \nlearn the status of the devices and the its content.\nAll these raw data can be integrated into cluster, and the metadata changed also need to update to cluster, \nbesides the real-time data need to be collected timely to support the further real-time analysis.\nAfter the data collected in cluster, the data scientist will try to discover and profile the data according to the analysis purpose, \nand use the defined meta data  to govern and enrich the data set. Then parse and prepare data for next analysis.\nWhen data scientist analysis the data, they should mask the sensitive data like name , id  etc. to protect the privacy.\nBased on the data in the cluster, data scientist can generate the business intelligence report or other visualization graphs. \nSometimes, in order to do the effective analysis, data scientist would like to build an Enterprise Data Warehouse and move the curated data to the  \nEnterprise Data warehouse, which just store the relevant data for the business intelligence. And on the basis of Enterprise Data Warehouse, \nanalytics can  generate the business intelligence report to support decision making.   \nLet’s make an example, JD big data analysis.\nJD has lots of transactions data from the online shopping history.\nAnd they also have lots of document data, like customer email, industry reports, user agreements and so on.\nAnd the customer’ s review about their feeling after shopping.\nAnd they also have lots of IoT data, like the logistic trucks GPS, product package ‘s RFID etc.\nWith all these data integrated into the Hadoop cluster, JD data scientist can discover and profile about the customer, product, express and \nwith all the predefined model, metadata, they can parse the original raw data, and generate the JD sales trend BI report, express efficiency BI report, product category BI report and etc.\nBut of cause, customer sensitive data should be masked to protect customer privacy.\nIn this session, we discussed the Big Data Processing flow both from the analytical perspective and technical perspective. We understand the general steps of big data analysis, including selecting the data sources, data collecting, cleaning the data before storing the data. \nBased on the data stored in the big data distributed storage, data scientist can process the data for further analysis, after all the data has been processed, data scientist can use algorithm or designed model to analyze the data, generate the business intelligence report or do the visualization analysis to dig the insight of the data and support decision making.\nThank you for your attention, if you have any question, feel free to connect me.", "document_id": "doc_a76dc628"}
