{"extractions": [{"extraction_class": "TOPIC", "extraction_text": "Data Transformation and Integration Techniques", "char_interval": {"start_pos": 52, "end_pos": 56}, "alignment_status": "match_lesser", "extraction_index": 1, "group_index": 0, "description": null, "attributes": null}, {"extraction_class": "SUMMARY", "extraction_text": "This session covers data transformation, which includes data integration and data transformation. Data integration involves combining data from multiple sources into a consistent storage by addressing three key tasks: pattern matching to map entities from different sources, handling data redundancy where attributes are duplicated or derivable, and resolving data value conflicts caused by differences in representation, scale, or coding. Following integration, data transformation changes data into a form suitable for efficient analysis. This is achieved through several methods: smoothing or discretization to reduce noise and data volume; aggregation using functions like sum() or avg() to summarize detailed data; data generalization to replace low-level data with higher-level concepts (e.g., street to city); data normalization to scale data into a specific range using methods like Min-Max or Z-score normalization; and attribute construction to create new, useful attributes from existing ones (e.g., creating 'area' from 'width' and 'height').", "char_interval": {"start_pos": 158, "end_pos": 170}, "alignment_status": "match_lesser", "extraction_index": 2, "group_index": 1, "description": null, "attributes": null}, {"extraction_class": "KEYWORDS", "extraction_text": "Data Integration, Data Transformation, Pattern Matching, Data Redundancy, Data Value Conflict, Aggregation, Data Generalization, Data Normalization, Attribute Construction", "char_interval": {"start_pos": 308, "end_pos": 3180}, "alignment_status": "match_fuzzy", "extraction_index": 3, "group_index": 2, "description": null, "attributes": null}, {"extraction_class": "CONCEPT", "extraction_text": "Data Integration", "char_interval": {"start_pos": 233, "end_pos": 249}, "alignment_status": "match_fuzzy", "extraction_index": 4, "group_index": 3, "description": null, "attributes": {"definition": "The process of integrating data from multiple data sources into a consistent storage, which involves tasks like pattern matching, data redundancy processing, and solving data value conflicts."}}, {"extraction_class": "CONCEPT", "extraction_text": "Pattern Matching", "char_interval": {"start_pos": 554, "end_pos": 570}, "alignment_status": "match_exact", "extraction_index": 5, "group_index": 4, "description": null, "attributes": {"definition": "A data integration task that involves integrating metadata from different sources to recognize and map real-world entities, such as matching 'A.cust-id' with 'B.customer_no'."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Redundancy", "char_interval": {"start_pos": 760, "end_pos": 775}, "alignment_status": "match_exact", "extraction_index": 6, "group_index": 5, "description": null, "attributes": {"definition": "An issue where the same attribute has different names in different databases, or one attribute can be derived from another. This can be detected through correlation analysis."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Value Conflict", "char_interval": {"start_pos": 1119, "end_pos": 1138}, "alignment_status": "match_exact", "extraction_index": 7, "group_index": 6, "description": null, "attributes": {"definition": "A problem where a real-world entity's attribute values differ across data sources due to differences in representation, scale (e.g., metric vs. imperial systems), or coding."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Transform", "char_interval": {"start_pos": 1608, "end_pos": 1622}, "alignment_status": "match_exact", "extraction_index": 8, "group_index": 7, "description": null, "attributes": {"definition": "The process of changing data from one form to another to facilitate efficient analysis."}}, {"extraction_class": "CONCEPT", "extraction_text": "Aggregation", "char_interval": {"start_pos": 1978, "end_pos": 1989}, "alignment_status": "match_exact", "extraction_index": 9, "group_index": 8, "description": null, "attributes": {"definition": "A data transformation method that uses calculations like avg(), count(), sum(), min(), and max() to represent detailed data with a summary result, such as aggregating daily sales into a monthly total."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Generalization", "char_interval": {"start_pos": 2295, "end_pos": 2314}, "alignment_status": "match_exact", "extraction_index": 10, "group_index": 9, "description": null, "attributes": {"definition": "A transformation that replaces low-level data objects with more abstract or higher-level concepts, such as generalizing a street to a city or a specific age to a category like 'young' or 'middle-aged'."}}, {"extraction_class": "CONCEPT", "extraction_text": "Data Normalization", "char_interval": {"start_pos": 2666, "end_pos": 2684}, "alignment_status": "match_exact", "extraction_index": 11, "group_index": 10, "description": null, "attributes": {"definition": "The process of scaling data proportionally to make it fall into a specific area (e.g., [-1.0, 1.0]) to eliminate deviations caused by different attribute scales. Methods include Min-Max and Zero-mean normalization."}}, {"extraction_class": "CONCEPT", "extraction_text": "Attribute Construction", "char_interval": {"start_pos": 3158, "end_pos": 3180}, "alignment_status": "match_exact", "extraction_index": 12, "group_index": 11, "description": null, "attributes": {"definition": "The process of using an existing set of attributes to construct new attributes, which are then added to the set to help discover deeper patterns and improve mining accuracy."}}], "text": "Hello everyone, I am Haiying Che, from Institute of Data Science and knowledge Engineering\nSchool of Computer Science, in Beijing Institute of Technology, in this session, we will discuss  Data transform .\nData transform can include Data integration and Data transform. Let’s look at Data integration first.\nData integration: \nIntegrate data from multiple data sources into a consistent storage, which could involve the task Pattern matching, Data redundancy processing and Data value conflict solving;\nLet’s look these 3 tasks one by one. \nFirst one is Pattern matching. Integrate metadata from different data sources.\nWe can recognize the real-world entity from different data sources and mapping them together, \nlike A.cust-id=B.customer_no.\nThen deal with Data redundancy issue,\nThe same attribute will have different field names in different databases.\nOne attribute can be derived from another attribute. For example, the average monthly income attribute in a customer data table can be calculated based on the monthly income attribute.\nAnd Some redundancy can be detected by correlation analysis.\nThen solve the Data value conflict problem.\nFor a real-world entity, its attribute values from different data sources may be different.\nSuch as Differences in representation, different scales, or differences in coding, etc. \nFor example: \nthe weight attribute uses the metric system, like kg, g in one system, but uses the imperial system like pound in another system. \nSame price attributes in different locations using different currency units, $, pound, RMB\nAfter data integration , then let’ come to data transform.\nData transform means, in order to facilitate efficient analysis , we need change the data from one form to another form.\nWe can use smooth methods like Binning Clustering Regression to eliminate the noise Or discretize continuous data, and increase granularity.\nBy doing this, we can reduce the data amount for further analysis.\nSecond transformation is Aggregation. \nBy doing avg(), count(), sum(), min(), max()...calculation, \nwe can use calculation result to represent the detail data. \nFor example: daily sales (data) can be aggregated to get the monthly or annual total.\nAnd these aggregation data are more convenient for big data analysis.\nThird  transformation is Data generalization. Which means Replace low-level data objects with more abstract (higher-level) concepts.\nFor example: \nstreet attributes can be generalized to higher-level concepts, such as: city, country. \nSimilarly, numeric attributes, such as age attributes, can be mapped to higher-level concepts, such as young, middle-aged, and old.\nThe fourth transformation is Data Normalization. \nWhich means The data is scaled proportionally to make it fall into a specific area, so as to eliminate the deviation of the mining results caused by the different sizes of the numerical attributes. \nSuch as mapping the salary income attribute value to the range of [-1.0,1.0].\nThe methods for Data Normalization could be \n(1) Min-Max normalization\n(2) Zero-mean normalization (z-score normalization)\n(3) Standardization of decimal calibration\nThe fifth transformation is Attribute construction. \nUse the existing attribute set to construct new attributes and add them to the existing attribute set to help dig deeper pattern knowledge and improve the accuracy of mining results.\nFor example: According to the width and height attributes, a new attribute can be constructed: area.\nIn this session we discussed Data integration and data transform.\nthank you for your attention, if you have any question, feel free to contact me.", "document_id": "doc_ad97073c"}
